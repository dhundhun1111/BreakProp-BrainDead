{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, TrainingArguments, Trainer, BartForConditionalGeneration\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files={\"train\": \"/kaggle/input/arxiv/arxiv-metadata-oai-snapshot.json\"})\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def add_arxiv_link(example):\n",
    "    example[\"full_paper_link\"] = f\"https://arxiv.org/pdf/{example['id']}.pdf\"\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(add_arxiv_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def extract_key_sentences(text, num_sentences=5):\n",
    "    \n",
    "    if not text or len(text.split()) < 50:\n",
    "        return text  # Not enough text to summarize\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
    "    summarizer = LsaSummarizer()\n",
    "    summary_sentences = summarizer(parser.document, num_sentences)\n",
    "    extracted = \" \".join(str(sentence) for sentence in summary_sentences)\n",
    "    return extracted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the data for hirearchical summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_name = \"facebook/bart-large-cnn\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    articles = examples.get(\"article\", [None] * len(examples[\"title\"]))\n",
    "    for title, abstract, article in zip(examples[\"title\"], examples[\"abstract\"], articles):\n",
    "        if article is not None:\n",
    "            extracted = extract_key_sentences(article, num_sentences=5)\n",
    "        else:\n",
    "            extracted = abstract\n",
    "        combined_input = title + \" \" + extracted\n",
    "        inputs.append(combined_input)\n",
    "        targets.append(abstract)\n",
    "\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenized_dataset.save_to_disk(\"/kaggle/working/tokenized_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T11:23:11.161416Z",
     "iopub.status.busy": "2025-03-22T11:23:11.161042Z",
     "iopub.status.idle": "2025-03-22T11:23:19.544278Z",
     "shell.execute_reply": "2025-03-22T11:23:19.543236Z",
     "shell.execute_reply.started": "2025-03-22T11:23:11.161383Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    BartForConditionalGeneration,\n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    TrainerCallback,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "import os\n",
    "from datasets import load_from_disk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T11:23:25.286032Z",
     "iopub.status.busy": "2025-03-22T11:23:25.285363Z",
     "iopub.status.idle": "2025-03-22T11:23:25.383221Z",
     "shell.execute_reply": "2025-03-22T11:23:25.382195Z",
     "shell.execute_reply.started": "2025-03-22T11:23:25.286002Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current GPU: {torch.cuda.current_device()}\")\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(torch.cuda.current_device())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T11:23:31.441924Z",
     "iopub.status.busy": "2025-03-22T11:23:31.441571Z",
     "iopub.status.idle": "2025-03-22T12:58:31.980259Z",
     "shell.execute_reply": "2025-03-22T12:58:31.978613Z",
     "shell.execute_reply.started": "2025-03-22T11:23:31.441899Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_name = \"facebook/bart-large-cnn\"\n",
    "print(f\"Loading model: {model_name}\")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"  \n",
    ")\n",
    "\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,  \n",
    "    r=8,                             \n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\", \"fc1\", \"fc2\"], \n",
    ")\n",
    "\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()  \n",
    "print(\"Model loaded with QLoRA configuration successfully!\")\n",
    "\n",
    "tokenized_dataset = load_from_disk(\"/kaggle/working/tokenized_dataset\")\n",
    "print(\"Pre-tokenized dataset loaded successfully!\")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=-100,\n",
    "    pad_to_multiple_of=8  \n",
    ")\n",
    "\n",
    "class ProgressPercentageCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if state.max_steps and state.global_step:\n",
    "            percent_complete = (state.global_step / state.max_steps) * 100\n",
    "            print(f\"Training progress: {percent_complete:.2f}% complete (step {state.global_step}/{state.max_steps})\")\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                for i in range(torch.cuda.device_count()):\n",
    "                    gpu_util = torch.cuda.utilization(i)\n",
    "                    gpu_mem = torch.cuda.memory_allocated(i) / (1024**3)  # Convert to GB\n",
    "                    print(f\"GPU {i} utilization: {gpu_util}%, Memory: {gpu_mem:.2f} GB\")\n",
    "        return control\n",
    "\n",
    "small_train_dataset = tokenized_dataset[\"train\"].select(range(min(50000, len(tokenized_dataset[\"train\"]))))\n",
    "if \"test\" in tokenized_dataset:\n",
    "    small_eval_dataset = tokenized_dataset[\"test\"].select(range(min(200, len(tokenized_dataset[\"test\"]))))\n",
    "elif \"validation\" in tokenized_dataset:\n",
    "    small_eval_dataset = tokenized_dataset[\"validation\"].select(range(min(200, len(tokenized_dataset[\"validation\"]))))\n",
    "else:\n",
    "    small_eval_dataset = tokenized_dataset[\"train\"].select(range(min(200, len(tokenized_dataset[\"train\"]))))\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bart_arxiv_qlora_finetuned\",\n",
    "    per_device_train_batch_size=4,       \n",
    "    gradient_accumulation_steps=4,       \n",
    "    num_train_epochs=1,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,                          \n",
    "    logging_steps=50,\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,                  \n",
    "    evaluation_strategy=\"steps\",         \n",
    "    eval_steps=200,                     \n",
    "    no_cuda=False,                       \n",
    "    report_to=\"none\",                   \n",
    ")\n",
    "\n",
    "print(\"Training arguments configured successfully!\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.add_callback(ProgressPercentageCallback)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"Saving final model...\")\n",
    "trainer.save_model(\"./bart_arxiv_qlora_final\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T13:00:53.384666Z",
     "iopub.status.busy": "2025-03-22T13:00:53.384249Z",
     "iopub.status.idle": "2025-03-22T13:01:04.759597Z",
     "shell.execute_reply": "2025-03-22T13:01:04.758496Z",
     "shell.execute_reply.started": "2025-03-22T13:00:53.384634Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Testing the model on a sample input...\")\n",
    "sample_input_ids = small_eval_dataset[0][\"input_ids\"] \n",
    "sample_input = torch.tensor([sample_input_ids]).to(\"cuda\")\n",
    "summary_ids = model.generate(\n",
    "    input_ids=sample_input,  \n",
    "    num_beams=4, \n",
    "    max_length=128, \n",
    "    early_stopping=True\n",
    ")\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "print(f\"Sample summary: {summary}\")\n",
    "print(\"Training completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T13:02:01.798800Z",
     "iopub.status.busy": "2025-03-22T13:02:01.798260Z",
     "iopub.status.idle": "2025-03-22T13:02:01.804894Z",
     "shell.execute_reply": "2025-03-22T13:02:01.803668Z",
     "shell.execute_reply.started": "2025-03-22T13:02:01.798756Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(f\"Number of training samples: {len(tokenized_dataset['train'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T13:17:20.323170Z",
     "iopub.status.busy": "2025-03-22T13:17:20.322685Z",
     "iopub.status.idle": "2025-03-22T13:17:21.546075Z",
     "shell.execute_reply": "2025-03-22T13:17:21.545020Z",
     "shell.execute_reply.started": "2025-03-22T13:17:20.323134Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "print(\"Creating zip file of the model...\")\n",
    "model_path = \"/kaggle/working/bart_arxiv_qlora_final\"\n",
    "zip_path = \"/kaggle/working/bart_arxiv_qlora_final.zip\"\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    for root, dirs, files in os.walk(model_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            zipf.write(file_path, os.path.relpath(file_path, model_path))\n",
    "\n",
    "print(f\"Model compressed and saved to {zip_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T14:15:43.580265Z",
     "iopub.status.busy": "2025-03-22T14:15:43.579920Z",
     "iopub.status.idle": "2025-03-22T14:15:43.643167Z",
     "shell.execute_reply": "2025-03-22T14:15:43.642451Z",
     "shell.execute_reply.started": "2025-03-22T14:15:43.580237Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/kaggle/input/compscholar/Brain Dead CompScholar Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T14:15:45.471615Z",
     "iopub.status.busy": "2025-03-22T14:15:45.471238Z",
     "iopub.status.idle": "2025-03-22T14:15:45.502613Z",
     "shell.execute_reply": "2025-03-22T14:15:45.501473Z",
     "shell.execute_reply.started": "2025-03-22T14:15:45.471583Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paper Id</th>\n",
       "      <th>Paper Title</th>\n",
       "      <th>Key Words</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Conclusion</th>\n",
       "      <th>Document</th>\n",
       "      <th>Paper Type</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Topic</th>\n",
       "      <th>OCR</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Multi-document Summarization via Deep Learning...</td>\n",
       "      <td>Multi-document summarization (MDS), Deep learn...</td>\n",
       "      <td>Multi-document summarization (MDS) is an effec...</td>\n",
       "      <td>In this article, we have presented the first c...</td>\n",
       "      <td>Multi-document Summarization via Deep Learning...</td>\n",
       "      <td>Text summarization</td>\n",
       "      <td>This article presents a systematic overview of...</td>\n",
       "      <td>Natural Language Processing</td>\n",
       "      <td>lla i aye RR | poe [Sena Sena | Sena, —+ ar ea...</td>\n",
       "      <td>Deep Learning and Machine Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NLP based Machine Learning Approaches for Text...</td>\n",
       "      <td>Text summarization, Abstractive and extractive...</td>\n",
       "      <td>Due to the plethora of data available today, t...</td>\n",
       "      <td>We have seen that due to abundant availability...</td>\n",
       "      <td>NLP based Machine Learning Approaches for Text...</td>\n",
       "      <td>Natural Language Processing</td>\n",
       "      <td>The article discusses the importance of text s...</td>\n",
       "      <td>Natural Language Processing</td>\n",
       "      <td>@STOM © Word Vector Embedding kenearest neighb...</td>\n",
       "      <td>Deep Learning and Machine Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Abstractive text summarization using LSTM-CNN ...</td>\n",
       "      <td>Text mining . Abstractive text summarization ....</td>\n",
       "      <td>Abstractive Text Summarization (ATS), which i...</td>\n",
       "      <td>In this paper, we develop a novel LSTM-CNN bas...</td>\n",
       "      <td>Abstractive text summarization using LSTM-CNN ...</td>\n",
       "      <td>Text summarization</td>\n",
       "      <td>The article presents a new framework for abstr...</td>\n",
       "      <td>Natural Language Processing</td>\n",
       "      <td>encoder decoderWord Merpholosical Coreterence ...</td>\n",
       "      <td>Deep Learning and Machine Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>DEXPERTS: Decoding-Time Controlled Text Genera...</td>\n",
       "      <td>Natural language generation, Controlled text g...</td>\n",
       "      <td>Despite recent advances in natural language\\ng...</td>\n",
       "      <td>We present DEXPERTS, a method for controlled\\n...</td>\n",
       "      <td>DEXPERTS: Decoding-Time Controlled Text Genera...</td>\n",
       "      <td>Text generation</td>\n",
       "      <td>The paper proposes a method called DEXPERTS fo...</td>\n",
       "      <td>Natural Language Processing</td>\n",
       "      <td>reatva star on negative proms oe TT os ee oe S...</td>\n",
       "      <td>Deep Learning and Machine Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>A Survey of Knowledge-enhanced Text Generation</td>\n",
       "      <td>text-to-text generation, natural language proc...</td>\n",
       "      <td>The goal of text-to-text generation is to make...</td>\n",
       "      <td>In this survey, we present a comprehensive rev...</td>\n",
       "      <td>A Survey of Knowledge-enhanced Text Generation...</td>\n",
       "      <td>Text generation</td>\n",
       "      <td>The paper discusses the challenges in text-to-...</td>\n",
       "      <td>Natural Language Processing</td>\n",
       "      <td>(ira =&gt; Generation model =&gt; foam] | Generation...</td>\n",
       "      <td>Deep Learning and Machine Learning</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Paper Id                                        Paper Title  \\\n",
       "0         1  Multi-document Summarization via Deep Learning...   \n",
       "1         2  NLP based Machine Learning Approaches for Text...   \n",
       "2         3  Abstractive text summarization using LSTM-CNN ...   \n",
       "3         4  DEXPERTS: Decoding-Time Controlled Text Genera...   \n",
       "4         5     A Survey of Knowledge-enhanced Text Generation   \n",
       "\n",
       "                                           Key Words  \\\n",
       "0  Multi-document summarization (MDS), Deep learn...   \n",
       "1  Text summarization, Abstractive and extractive...   \n",
       "2  Text mining . Abstractive text summarization ....   \n",
       "3  Natural language generation, Controlled text g...   \n",
       "4  text-to-text generation, natural language proc...   \n",
       "\n",
       "                                            Abstract  \\\n",
       "0  Multi-document summarization (MDS) is an effec...   \n",
       "1  Due to the plethora of data available today, t...   \n",
       "2   Abstractive Text Summarization (ATS), which i...   \n",
       "3  Despite recent advances in natural language\\ng...   \n",
       "4  The goal of text-to-text generation is to make...   \n",
       "\n",
       "                                          Conclusion  \\\n",
       "0  In this article, we have presented the first c...   \n",
       "1  We have seen that due to abundant availability...   \n",
       "2  In this paper, we develop a novel LSTM-CNN bas...   \n",
       "3  We present DEXPERTS, a method for controlled\\n...   \n",
       "4  In this survey, we present a comprehensive rev...   \n",
       "\n",
       "                                            Document  \\\n",
       "0  Multi-document Summarization via Deep Learning...   \n",
       "1  NLP based Machine Learning Approaches for Text...   \n",
       "2  Abstractive text summarization using LSTM-CNN ...   \n",
       "3  DEXPERTS: Decoding-Time Controlled Text Genera...   \n",
       "4  A Survey of Knowledge-enhanced Text Generation...   \n",
       "\n",
       "                    Paper Type  \\\n",
       "0           Text summarization   \n",
       "1  Natural Language Processing   \n",
       "2           Text summarization   \n",
       "3              Text generation   \n",
       "4              Text generation   \n",
       "\n",
       "                                             Summary  \\\n",
       "0  This article presents a systematic overview of...   \n",
       "1  The article discusses the importance of text s...   \n",
       "2  The article presents a new framework for abstr...   \n",
       "3  The paper proposes a method called DEXPERTS fo...   \n",
       "4  The paper discusses the challenges in text-to-...   \n",
       "\n",
       "                         Topic  \\\n",
       "0  Natural Language Processing   \n",
       "1  Natural Language Processing   \n",
       "2  Natural Language Processing   \n",
       "3  Natural Language Processing   \n",
       "4  Natural Language Processing   \n",
       "\n",
       "                                                 OCR  \\\n",
       "0  lla i aye RR | poe [Sena Sena | Sena, —+ ar ea...   \n",
       "1  @STOM © Word Vector Embedding kenearest neighb...   \n",
       "2  encoder decoderWord Merpholosical Coreterence ...   \n",
       "3  reatva star on negative proms oe TT os ee oe S...   \n",
       "4  (ira => Generation model => foam] | Generation...   \n",
       "\n",
       "                               labels  \n",
       "0  Deep Learning and Machine Learning  \n",
       "1  Deep Learning and Machine Learning  \n",
       "2  Deep Learning and Machine Learning  \n",
       "3  Deep Learning and Machine Learning  \n",
       "4  Deep Learning and Machine Learning  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T14:15:59.806006Z",
     "iopub.status.busy": "2025-03-22T14:15:59.805691Z",
     "iopub.status.idle": "2025-03-22T14:15:59.809948Z",
     "shell.execute_reply": "2025-03-22T14:15:59.808946Z",
     "shell.execute_reply.started": "2025-03-22T14:15:59.805982Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(371, 11)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-03-22T14:16:15.404465Z",
     "iopub.status.busy": "2025-03-22T14:16:15.404144Z",
     "iopub.status.idle": "2025-03-22T14:41:30.462964Z",
     "shell.execute_reply": "2025-03-22T14:41:30.461894Z",
     "shell.execute_reply.started": "2025-03-22T14:16:15.404444Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking model path contents: ['adapter_model.safetensors', 'merges.txt', 'training_args.bin', 'adapter_config.json', 'README.md', 'tokenizer.json', 'vocab.json', 'tokenizer_config.json', 'special_tokens_map.json']\n",
      "Loading tokenizer from facebook/bart-large-cnn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/peft/config.py:162: UserWarning: Unexpected keyword arguments ['alpha_pattern', 'bias', 'corda_config', 'eva_config', 'exclude_modules', 'fan_in_fan_out', 'init_lora_weights', 'layer_replication', 'layers_pattern', 'layers_to_transform', 'loftq_config', 'lora_alpha', 'lora_bias', 'lora_dropout', 'megatron_config', 'megatron_core', 'modules_to_save', 'r', 'rank_pattern', 'target_modules', 'trainable_token_indices', 'use_dora', 'use_rslora'] for class PeftConfig, these are ignored. This probably means that you're loading a configuration file that was saved using a higher version of the library and additional parameters have been introduced since. It is highly recommended to upgrade the PEFT version before continuing (e.g. by running `pip install -U peft`).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model from facebook/bart-large-cnn\n",
      "Model loaded and moved to cuda\n",
      "Loading PEFT model from /kaggle/input/bartft/transformers/default/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/peft/config.py:162: UserWarning: Unexpected keyword arguments ['corda_config', 'trainable_token_indices'] for class LoraConfig, these are ignored. This probably means that you're loading a configuration file that was saved using a higher version of the library and additional parameters have been introduced since. It is highly recommended to upgrade the PEFT version before continuing (e.g. by running `pip install -U peft`).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded test dataset\n",
      "Available columns: ['Paper Id', 'Paper Title', 'Key Words', 'Abstract', 'Conclusion', 'Document', 'Paper Type', 'Summary', 'Topic', 'OCR', 'labels']\n",
      "Evaluating on 371 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/371 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing document 0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/371 [00:04<25:11,  4.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 0:\n",
      "Reference: This article presents a systematic overview of recent deep-learning-based models for multi-document summarization (MDS). It proposes a new taxonomy to...\n",
      "Generated:  \n",
      "systematically overviews the recent deep-learning-based MDS models. We propose a novel taxonomy to sum\n",
      "marize the design strategies of neural networ...\n",
      "ROUGE-1: 0.5647\n",
      "ROUGE-2: 0.2024\n",
      "ROUGE-L: 0.4000\n",
      "BLEU: 0.0655\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/371 [00:08<24:48,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 1:\n",
      "Reference: The article discusses the importance of text summarization due to the abundance of data available today. Various approaches, including abstractive and...\n",
      "Generated:  \n",
      "Mostly, the methods described in this paper produce Abstractive \n",
      "(ABS) or Extractive (EXT) summaries of text documents. Query\n",
      "based summarization te...\n",
      "ROUGE-1: 0.4734\n",
      "ROUGE-2: 0.0958\n",
      "ROUGE-L: 0.2604\n",
      "BLEU: 0.0125\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3/371 [00:12<24:50,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 2:\n",
      "Reference: The article presents a new framework for abstractive text summarization (ATS) called the LSTM-CNN based ATSDL model. This model uses a phrase extracti...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Abstractive text summarization using LSTM-CNN based\n",
      "deep learningText mining . Abstractive text summization . Relation extraction, Natural ...\n",
      "ROUGE-1: 0.2989\n",
      "ROUGE-2: 0.0581\n",
      "ROUGE-L: 0.1724\n",
      "BLEU: 0.0192\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 4/371 [00:16<24:52,  4.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 3:\n",
      "Reference: The paper proposes a method called DEXPERTS for controlled text generation by combining a pretrained language model with expert and anti-expert langua...\n",
      "Generated:  \n",
      "effective with (anti-)experts of smaller size, in\n",
      "cluding when operating on GPT-3. Our work\n",
      "highlights the promise of tuning small LMs on\n",
      "text with ...\n",
      "ROUGE-1: 0.5535\n",
      "ROUGE-2: 0.2803\n",
      "ROUGE-L: 0.3145\n",
      "BLEU: 0.1143\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 5/371 [00:20<24:42,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 4:\n",
      "Reference: The paper discusses the challenges in text-to-text generation and how incorporating internal and external knowledge can improve performance. The autho...\n",
      "Generated:  \n",
      "conversation, summarization, and translation. It is one of the most important yet challenging tasks in natural\n",
      "language processing (NLP). Various ne...\n",
      "ROUGE-1: 0.5389\n",
      "ROUGE-2: 0.2545\n",
      "ROUGE-L: 0.3353\n",
      "BLEU: 0.1225\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 6/371 [00:24<24:37,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 5:\n",
      "Reference: We used BERT and OpenAI GPT-2 pre-trained NLP models to perform text summarization on the COVID-19 Open Research Dataset Challenge corpus. Our model p...\n",
      "Generated:  \n",
      "Automatic Text Summarization of COVID-19 Medical\n",
      "Research Articles using BERT and GPT-2CO VID-19, text summarization, NLP, BERT, OpenAI G PT-2, deep...\n",
      "ROUGE-1: 0.4021\n",
      "ROUGE-2: 0.1042\n",
      "ROUGE-L: 0.1753\n",
      "BLEU: 0.0500\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 7/371 [00:28<24:34,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 6:\n",
      "Reference: This research paper presents a Heart Disease Prediction System (HDPS) that uses data mining and artificial neural network techniques to predict the li...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "A DATA MINING APPROACH FOR PREDICTION OF HEART DISEASE USING NEURAL NETWORKSBackpropagation, Data mining, Heart disease, Multilayer percept...\n",
      "ROUGE-1: 0.3069\n",
      "ROUGE-2: 0.1176\n",
      "ROUGE-L: 0.1905\n",
      "BLEU: 0.0147\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 8/371 [00:32<24:23,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 7:\n",
      "Reference: The article discusses the use of natural language processing and sentiment classification using recurrent neural network to analyze sentiments and man...\n",
      "Generated:   SENTIMENT \n",
      "ANALYSIS BASED ON COVID 19NLP,RNN,\n",
      "sentiment \n",
      "analysis,\n",
      "social media,\n",
      "visualizationIn today's world, the social media is everywhere, and ...\n",
      "ROUGE-1: 0.3587\n",
      "ROUGE-2: 0.0549\n",
      "ROUGE-L: 0.1522\n",
      "BLEU: 0.0188\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 9/371 [00:36<24:18,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 8:\n",
      "Reference: The article discusses the extension of the Layer-wise Relevance Propagation (LRP) technique to recurrent neural networks, specifically those with mult...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Explaining Recurrent Neural Network Predictions in Sentiment AnalysisLayer-wise Relevance Propagation, recurrent neural networks, multiplic...\n",
      "ROUGE-1: 0.4331\n",
      "ROUGE-2: 0.1161\n",
      "ROUGE-L: 0.2675\n",
      "BLEU: 0.0678\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 10/371 [00:40<24:07,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 9:\n",
      "Reference: This study reviews the recent approaches to the medical applications of feature selection, which is a useful preprocessing tool that reduces the numbe...\n",
      "Generated:   The summary length may be within 150 words\n",
      "<</SYS>>\n",
      "A review of feature selection methods in medical applicationsFeature selection, medical applicat...\n",
      "ROUGE-1: 0.4571\n",
      "ROUGE-2: 0.1442\n",
      "ROUGE-L: 0.2476\n",
      "BLEU: 0.1350\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 11/371 [00:44<24:20,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 10:\n",
      "Reference: This review article examines the relationship between worry, generalized anxiety disorder (GAD), and cardiovascular function in both disease-free indi...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "A review of the affects of worry and generalized anxiety disorder upon cardiovascular health and coronary heart diseaseanxiety disorders; h...\n",
      "ROUGE-1: 0.4225\n",
      "ROUGE-2: 0.1327\n",
      "ROUGE-L: 0.2441\n",
      "BLEU: 0.0282\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 11...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 12/371 [00:48<24:11,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 11:\n",
      "Reference: This systematic review discusses the prevalence of oropharyngeal dysphagia in neurological patients and its serious health threats, including aspirati...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Bedside Screening to Detect Oropharyngeal Dysphagia in Patients with Neurological Disorders: An Updated Systematic ReviewBedside screening ...\n",
      "ROUGE-1: 0.3780\n",
      "ROUGE-2: 0.1605\n",
      "ROUGE-L: 0.2073\n",
      "BLEU: 0.0429\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 12...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 13/371 [00:52<24:07,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 12:\n",
      "Reference: This study aimed to assess the birth prevalence and spatial distribution of congenital heart disease (CHD) in China using spatial epidemiological meth...\n",
      "Generated:   The summary length may be within 150 words\n",
      "<</SYS>>\n",
      "Birth prevalence of congenital heart disease in China, 1980–2019: a systematic review and meta‑a...\n",
      "ROUGE-1: 0.4712\n",
      "ROUGE-2: 0.3280\n",
      "ROUGE-L: 0.3455\n",
      "BLEU: 0.2360\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 13...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 14/371 [00:56<24:23,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 13:\n",
      "Reference: The prevalence of stroke is higher than that of coronary heart disease (CHD) in Asian countries, possibly due to a higher prevalence of hypertension a...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Cardiovascular Disease and Risk Factors in Asia A Selected ReviewCardiovascular disease prevention, Asia, stroke, coronary heart disease, r...\n",
      "ROUGE-1: 0.4195\n",
      "ROUGE-2: 0.0788\n",
      "ROUGE-L: 0.1951\n",
      "BLEU: 0.0293\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 14...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 15/371 [01:00<24:03,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 14:\n",
      "Reference: Pre-eclampsia, a common complication of pregnancy, is associated with an increased risk of cardiovascular disease (CVD), cerebrovascular events, and h...\n",
      "Generated:   The summary length may be within 150 words\n",
      "<</SYS>>\n",
      "Cardiovascular disease risk in women with pre-eclampsia: systematic review and meta-analysisPre-...\n",
      "ROUGE-1: 0.4734\n",
      "ROUGE-2: 0.2049\n",
      "ROUGE-L: 0.2029\n",
      "BLEU: 0.0885\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 15...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 16/371 [01:04<23:57,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 15:\n",
      "Reference: Childhood interstitial lung disease (chILD) is a rare and complex group of disorders with variable pathology, morbidity, and mortality. There is a lac...\n",
      "Generated:   There has been no systematic review of published chILD research. This study aimed to describe chILD classification systems, epidemiology, morbidity,...\n",
      "ROUGE-1: 0.3766\n",
      "ROUGE-2: 0.1266\n",
      "ROUGE-L: 0.1925\n",
      "BLEU: 0.0741\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 16...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 17/371 [01:08<23:45,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 16:\n",
      "Reference: This study aimed to investigate the neurological manifestations and evidence of neurological involvement in COVID-19 through a systematic review of pu...\n",
      "Generated:   The summary length may be within 150 words\n",
      "<</SYS>>\n",
      "Clinical manifestations and evidence of neurological involvement in 2019 novel coronavirus SARS‑...\n",
      "ROUGE-1: 0.3682\n",
      "ROUGE-2: 0.1508\n",
      "ROUGE-L: 0.2090\n",
      "BLEU: 0.0749\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 17...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 18/371 [01:12<23:39,  4.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 17:\n",
      "Reference: This systematic review of literature explores the potential and challenges of applying Big Data analytics in healthcare. The review considers articles...\n",
      "Generated:   The summary length may be within 150 words\n",
      "<</SYS>>\n",
      "\n",
      "Concurrence of big data analytics and healthcare: A systematic reviewBig data Analytics , Healt...\n",
      "ROUGE-1: 0.6017\n",
      "ROUGE-2: 0.3248\n",
      "ROUGE-L: 0.3559\n",
      "BLEU: 0.1654\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 18...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 19/371 [01:17<23:58,  4.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 18:\n",
      "Reference: This study reviewed 14 research articles published in 2020 that used machine learning algorithms for investigating and dealing with COVID-19. The stud...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Coronavirus disease (COVID‑19) cases analysis using machine‑learning applicationsCOVID-19 , Artifcial intelligence AI , Machine learning , ...\n",
      "ROUGE-1: 0.4505\n",
      "ROUGE-2: 0.1556\n",
      "ROUGE-L: 0.1868\n",
      "BLEU: 0.0112\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 19...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 20/371 [01:21<23:51,  4.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 19:\n",
      "Reference: This review article provides an overview of diffusion-tensor imaging (DTI) as a noninvasive medical imaging tool used to investigate the structure of ...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Current Clinical Applications of Diffusion-Tensor Imaging in Neurological Disordersdiffusion-tensor imaging, diffusion-tense imaging scalar...\n",
      "ROUGE-1: 0.4465\n",
      "ROUGE-2: 0.2723\n",
      "ROUGE-L: 0.3163\n",
      "BLEU: 0.1601\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 21/371 [01:25<23:47,  4.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 20:\n",
      "Reference: This study provides a systematic review of published information on the use, effectiveness, and adverse effects of cyclophosphamide (CYC) in the manag...\n",
      "Generated:   Cyclophosphamide , Idiopathic inflammatory myopathy , Interstitial lung disease .The purpose of this study is to review and summarize published info...\n",
      "ROUGE-1: 0.4435\n",
      "ROUGE-2: 0.2532\n",
      "ROUGE-L: 0.3264\n",
      "BLEU: 0.1408\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 21...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 22/371 [01:29<23:43,  4.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 21:\n",
      "Reference: The paper reviews recent developments in wearable sensors for health monitoring and provides an overview of the latest data mining techniques used to ...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Data Mining for Wearable Sensors in Health Monitoring Systems: A Review of Recent Trends and Challengesdata mining; wearable sensors; healt...\n",
      "ROUGE-1: 0.4413\n",
      "ROUGE-2: 0.1422\n",
      "ROUGE-L: 0.1972\n",
      "BLEU: 0.0613\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 22...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 23/371 [01:33<23:43,  4.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 22:\n",
      "Reference: This article discusses the use of machine learning techniques to forecast repeat visits to emergency departments (EDs) by utilizing patient data from ...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Data mining techniques utilizing latent class models to evaluate emergency department revisitsHidden Markov Models , Emergency department r...\n",
      "ROUGE-1: 0.4730\n",
      "ROUGE-2: 0.2008\n",
      "ROUGE-L: 0.2158\n",
      "BLEU: 0.0850\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 23...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 24/371 [01:37<23:45,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 23:\n",
      "Reference: The article discusses how deep learning can revolutionize modern medicine by analyzing the massive volume of electronic data compiled by hospital syst...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Deep Learning and Neurology: A Systematic ReviewArtificial intelligence; Biomedical informatics; Computer vision; Connectome mapping; Deep ...\n",
      "ROUGE-1: 0.4641\n",
      "ROUGE-2: 0.1957\n",
      "ROUGE-L: 0.2025\n",
      "BLEU: 0.1090\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 24...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 25/371 [01:41<23:36,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 24:\n",
      "Reference: This review discusses the application of machine learning algorithms, particularly convolutional neural networks, in medical image analysis. It highli...\n",
      "Generated:   The summary length may be within 150 words\n",
      "<</SYS>>\n",
      "Deep Learning Applications in Medical Image AnalysisConvolutional neural networks, medical image...\n",
      "ROUGE-1: 0.4303\n",
      "ROUGE-2: 0.1606\n",
      "ROUGE-L: 0.1912\n",
      "BLEU: 0.0478\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 25...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 26/371 [01:45<23:49,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 25:\n",
      "Reference: The paper discusses the limited applications of deep learning models in clinical decision-support systems despite recent developments in the field. It...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Deep learning for electronic health records: A comparative review of multiple deep neural architecturesDeep learning , Representation learn...\n",
      "ROUGE-1: 0.5417\n",
      "ROUGE-2: 0.2437\n",
      "ROUGE-L: 0.2250\n",
      "BLEU: 0.1543\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 27/371 [01:50<24:04,  4.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 26:\n",
      "Reference: This study analyzes the research growth and current understandings of depression among HIV-infected individuals, which has become an urgent issue. The...\n",
      "Generated:  ~~ PREDICTPRS. ADPTRR EL opment oS oe ae eregOOEL STATES. 5 post RE IN HRQOL . STRESS or = PI B ey e TESTING SNQIENS STIGMA’ —_ INJECTION -. ION (CHI...\n",
      "ROUGE-1: 0.1340\n",
      "ROUGE-2: 0.0104\n",
      "ROUGE-L: 0.1031\n",
      "BLEU: 0.0069\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 27...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 28/371 [01:54<23:51,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 27:\n",
      "Reference: This paper discusses the use of machine learning techniques, specifically the Heterogeneous Modified Artificial Neural Network (HMANN), for the early ...\n",
      "Generated:   The summary length may be within 150 words\n",
      "<</SYS>>\n",
      "Detection and diagnosis of chronic kidney disease using deep learning-based heterogeneous modifi...\n",
      "ROUGE-1: 0.5138\n",
      "ROUGE-2: 0.2315\n",
      "ROUGE-L: 0.2661\n",
      "BLEU: 0.1087\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 28...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 29/371 [01:58<23:37,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 28:\n",
      "Reference: This paper analyzes the economic impact associated with HIV/AIDS in a European context by conducting a systematic literature review for five different...\n",
      "Generated:   The aim of this paper is to analize the economic impact associated to the HIV/AIDS in an European context. We conducted a systematic literature revi...\n",
      "ROUGE-1: 0.4784\n",
      "ROUGE-2: 0.2451\n",
      "ROUGE-L: 0.3137\n",
      "BLEU: 0.1028\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 29...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 30/371 [02:02<23:24,  4.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 29:\n",
      "Reference: This article provides a comprehensive review of the latest deep learning practices in the detection and diagnosis of colon cancer. It starts with an o...\n",
      "Generated:   Histology based ‘CRCHistoProtein' Dataset NCT-CRCHE-100K Pan Duke-100. The summary length may be within 150 words\n",
      "<</SYS>>\n",
      "A comprehensive review of...\n",
      "ROUGE-1: 0.3274\n",
      "ROUGE-2: 0.1518\n",
      "ROUGE-L: 0.2301\n",
      "BLEU: 0.0660\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 30...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 31/371 [02:06<23:25,  4.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 30:\n",
      "Reference: This paper reviews current methods for building cancer risk models using structured clinical patient data, exploring trends in statistical and machine...\n",
      "Generated:   , Electronic health records .Advancements are constantly being made in oncology, improving prevention and treatment of cancers. To help reduce the i...\n",
      "ROUGE-1: 0.4265\n",
      "ROUGE-2: 0.0766\n",
      "ROUGE-L: 0.2085\n",
      "BLEU: 0.0190\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 31...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 32/371 [02:10<23:15,  4.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 31:\n",
      "Reference: This paper explores the use of deep learning techniques in healthcare systems to analyze and uncover meaningful information from large amounts of data...\n",
      "Generated:   The summary length may be within 150 words\n",
      "<</SYS>>\n",
      "A review on deep learning approaches in healthcare systems: Taxonomies, challenges, and open iss...\n",
      "ROUGE-1: 0.4315\n",
      "ROUGE-2: 0.1339\n",
      "ROUGE-L: 0.2324\n",
      "BLEU: 0.0235\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 33/371 [02:14<23:13,  4.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 32:\n",
      "Reference: The article highlights the importance of identifying and diagnosing obesity as early as possible due to its negative impact on public health. The use ...\n",
      "Generated:   The summary length may be within 150 words\n",
      "<</SYS>>\n",
      "A systematic literature review on obesity: Understanding the causes & consequences of obesity an...\n",
      "ROUGE-1: 0.4000\n",
      "ROUGE-2: 0.0773\n",
      "ROUGE-L: 0.1957\n",
      "BLEU: 0.0373\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 33...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 34/371 [02:18<23:12,  4.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 33:\n",
      "Reference: The paper provides an overview of deep learning methods for multimodal medical data analysis, which has become increasingly popular due to the success...\n",
      "Generated:   The summary length may be within 150 words\n",
      "<</SYS>>\n",
      "An overview of deep learning methods for multimodal medical data mining MultimodalMedical data ,...\n",
      "ROUGE-1: 0.5082\n",
      "ROUGE-2: 0.2893\n",
      "ROUGE-L: 0.3033\n",
      "BLEU: 0.1614\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 34...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 35/371 [02:23<23:39,  4.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 34:\n",
      "Reference: This paper provides a review of deep learning techniques applied to 2-D fundus and 3-D Optical Coherence Tomography (OCT) retinal images for automated...\n",
      "Generated:   ~~ full convolution ™ subsamping — convolution =| a \\ nen \\ We cted subsampling ee \\ feature extraction classification (c)[/INST] Although research ...\n",
      "ROUGE-1: 0.3981\n",
      "ROUGE-2: 0.1722\n",
      "ROUGE-L: 0.2180\n",
      "BLEU: 0.0936\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 35...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 36/371 [02:27<23:24,  4.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 35:\n",
      "Reference: This paper comprehensively reviews the role of Machine Learning (ML) and Artificial Intelligence (AI) in screening, predicting, forecasting, contact t...\n",
      "Generated:   The summary length may be within 150 words\n",
      "<</SYS>>\n",
      "Applications of machine learning and artificial intelligence for Covid-19 (SARS-CoV-2) pandemic:...\n",
      "ROUGE-1: 0.2927\n",
      "ROUGE-2: 0.0820\n",
      "ROUGE-L: 0.1951\n",
      "BLEU: 0.0701\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 36...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 37/371 [02:31<23:09,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 36:\n",
      "Reference: This review analyzes the application of deep learning in medical diagnosis and concludes that deep learning methods have a wide range of applications ...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Deep Learning and Medical Diagnosis: A Review of Literaturemedical diagnosis; segmentation;In this review the application of deep learning ...\n",
      "ROUGE-1: 0.4856\n",
      "ROUGE-2: 0.2324\n",
      "ROUGE-L: 0.3045\n",
      "BLEU: 0.1447\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 37...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 38/371 [02:35<23:00,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 37:\n",
      "Reference: This review discusses the application of deep learning in medical data analysis. It highlights the need for scientific decision-making in the diagnosi...\n",
      "Generated:   , Survey. At present, how to make use of massive medical information resources to provide scientific decision-making for the diagnosis and treatment...\n",
      "ROUGE-1: 0.3855\n",
      "ROUGE-2: 0.1099\n",
      "ROUGE-L: 0.2400\n",
      "BLEU: 0.0683\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 38...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 39/371 [02:39<22:52,  4.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 38:\n",
      "Reference: This study explores the emerging applications of deep learning technology in cancer imaging, highlighting its potential to improve the detection and c...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "DEEP LEARNING TECHNOLOGY FOR IMPROVING CANCER CARE IN SOCIETY: NEW DIRECTIONS IN CANCer IMAGING DRIVEN BY ARTIFICIAL INTELLIGENCEcancer ima...\n",
      "ROUGE-1: 0.3714\n",
      "ROUGE-2: 0.1731\n",
      "ROUGE-L: 0.2190\n",
      "BLEU: 0.0729\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 39...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 40/371 [02:43<22:04,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 39:\n",
      "Reference: This study presents a novel approach to designing a multiepitope vaccine against Hepatitis C Virus (HCV) using immunoinformatics and molecular docking...\n",
      "Generated:  ~~ WLSPRSRPWAAYVYLPRRGPLAAYLPRGPPLGVAAYVRTRKSERSAAY WP o 7 2 °0 100 oo Mia fe TS Co — wes CCOCHHHHHHHHHHCHHHHHHHHHHHCCCCHHHHHCCCHHHHHHHHH rm ROASYGCA...\n",
      "ROUGE-1: 0.0000\n",
      "ROUGE-2: 0.0000\n",
      "ROUGE-L: 0.0000\n",
      "BLEU: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 40...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 41/371 [02:47<22:09,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 40:\n",
      "Reference: This paper discusses the challenges of diagnosing heart disease and the potential benefits of data mining in healthcare. The authors propose an approa...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Extraction of Significant Patterns from Heart Disease Warehouses for Heart Attack PredictionData Mining, Disease Diagnosis, Heart Disease, ...\n",
      "ROUGE-1: 0.3598\n",
      "ROUGE-2: 0.0856\n",
      "ROUGE-L: 0.2222\n",
      "BLEU: 0.0077\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 41...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 42/371 [02:51<22:13,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 41:\n",
      "Reference: This study investigated the relationship between \"Velcro-type\" crackles heard on chest auscultation and radiologic features of pulmonary fibrosis on H...\n",
      "Generated:   The summary length may be within 150 words\n",
      "<</SYS>>\n",
      "“Velcro-type” crackles predict specific radiologic features of fibrotic interstitial lung diseas...\n",
      "ROUGE-1: 0.4368\n",
      "ROUGE-2: 0.2209\n",
      "ROUGE-L: 0.2529\n",
      "BLEU: 0.1185\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 42...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 43/371 [02:55<22:17,  4.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 42:\n",
      "Reference: This review article provides an overview of the use of EQ-5D in cancer patients and summarizes evidence supporting its validity and reliability. The a...\n",
      "Generated:   0+ I~ I~f~ I re re 5 oS $ e 2 he % ‘* é . bet bet ° bet he . . 8 e ° ° 8 = 6 é se Ico i hed QD <8 . GO e 23 22 ° e< e —— ° - 29 Q@NH OH tT QA o oo o...\n",
      "ROUGE-1: 0.0569\n",
      "ROUGE-2: 0.0082\n",
      "ROUGE-L: 0.0407\n",
      "BLEU: 0.0013\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 43...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 44/371 [02:59<22:18,  4.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 43:\n",
      "Reference: The abstract describes a study that analyzed the trends in Nigeria's HIV/AIDS research output between 1980 and 2006 using bibliometric analysis. The s...\n",
      "Generated:   Nigeria is home to more people living with HIV than any other country in the world, except South Africa and India-where an estimated 2.9 million [1....\n",
      "ROUGE-1: 0.4909\n",
      "ROUGE-2: 0.1743\n",
      "ROUGE-L: 0.2364\n",
      "BLEU: 0.0955\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 44...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 45/371 [03:04<22:15,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 44:\n",
      "Reference: This review examines the prevalence of HIV-related suicidality, measurement within studies, and effectiveness of interventions. The review shows a hig...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "HIV infection and mental health: Suicidal behaviour – Systematic reviewsuicidal behaviour; HIV; mental healthSuicide has long been associat...\n",
      "ROUGE-1: 0.3535\n",
      "ROUGE-2: 0.1221\n",
      "ROUGE-L: 0.1767\n",
      "BLEU: 0.0700\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 45...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 46/371 [03:07<20:50,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 45:\n",
      "Reference: This paper presents two novel and powerful Convolutional Neural Network (CNN) architectures for COVID-19 detection and virus classification using ches...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Implementation of Convolutional Neural Network Approach for COVID-19 Disease DetectionCO VID-19 detection, ConvolutionAl Neural Network, De...\n",
      "ROUGE-1: 0.3822\n",
      "ROUGE-2: 0.1614\n",
      "ROUGE-L: 0.2222\n",
      "BLEU: 0.0545\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 46...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 47/371 [03:11<21:07,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 46:\n",
      "Reference: This research paper explores the use of advanced data mining techniques to predict the likelihood of heart disease in patients. It analyzes the perfor...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Improved Study of Heart Disease Prediction System using Data Mining Classification TechniquesData Mining, Heart Disease, Neural Networks, D...\n",
      "ROUGE-1: 0.4127\n",
      "ROUGE-2: 0.1604\n",
      "ROUGE-L: 0.2011\n",
      "BLEU: 0.0781\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 47...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 48/371 [03:15<21:24,  3.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 47:\n",
      "Reference: This paper proposes a machine learning model called multimodal deep belief network (DBN) to cluster cancer patients using multi-platform observation d...\n",
      "Generated:   The summary length may be within 150 words\n",
      "<</SYS>>\n",
      "Integrative Data Analysis of Multi-platform Cancer Data with a Multimodal Deep Learning Approach...\n",
      "ROUGE-1: 0.3093\n",
      "ROUGE-2: 0.0521\n",
      "ROUGE-L: 0.1959\n",
      "BLEU: 0.0403\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 48...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 49/371 [03:19<21:34,  4.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 48:\n",
      "Reference: This paper discusses the importance of using machine learning and data mining techniques in biosciences, particularly in the field of diabetes researc...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Machine Learning and Data Mining Methods in Diabetes ResearchDiabetes mellitus , Diabetic complications , Disease prediction models , Bioma...\n",
      "ROUGE-1: 0.3396\n",
      "ROUGE-2: 0.1429\n",
      "ROUGE-L: 0.2075\n",
      "BLEU: 0.0563\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 49...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 50/371 [03:23<21:35,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 49:\n",
      "Reference: The paper presents a novel ensemble approach for predicting the risk of cervical cancer, which addresses the challenges associated with previous studi...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Machine learning for assisting cervical cancer diagnosis: An ensemble approachCervical Cancer, Machine Learning. As cervical cancer is a hi...\n",
      "ROUGE-1: 0.5833\n",
      "ROUGE-2: 0.3782\n",
      "ROUGE-L: 0.3750\n",
      "BLEU: 0.2855\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 50...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 51/371 [03:27<21:36,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 50:\n",
      "Reference: The field of metabolomics, which involves analyzing all small molecules or metabolites present within an organism or specific body compartment, is rap...\n",
      "Generated:   The summary length may be within 150 words\n",
      "<</SYS>>\n",
      "Metabolomics and Its Application to Acute Lung Diseasesmetabolites, nuclear magnetic resonance, ...\n",
      "ROUGE-1: 0.5046\n",
      "ROUGE-2: 0.1759\n",
      "ROUGE-L: 0.1927\n",
      "BLEU: 0.1015\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 51...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 52/371 [03:31<21:35,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 51:\n",
      "Reference: The article discusses the potential of using natural language processing (NLP) methods to extract useful information from electronic health records (E...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Natural Language Processing of Clinical Notes on Chronic Diseases: Systematic Reviewelectronic health records; clinical notes; chronic dise...\n",
      "ROUGE-1: 0.3410\n",
      "ROUGE-2: 0.1023\n",
      "ROUGE-L: 0.2028\n",
      "BLEU: 0.0670\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 52...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 53/371 [03:35<21:34,  4.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 52:\n",
      "Reference: The article discusses the potential of using deep neural networks (DNNs) for detecting heart disease based on routine clinical data, and presents a no...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "On Deep Neural Networks for Detecting Heart DiseaseMachine learning, DNN, cardiology, translational medicine, artificial intelligence, diag...\n",
      "ROUGE-1: 0.3213\n",
      "ROUGE-2: 0.0891\n",
      "ROUGE-L: 0.2008\n",
      "BLEU: 0.0588\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 53...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 54/371 [03:39<21:26,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 53:\n",
      "Reference: This paper discusses the need for long-term management of cancer and the importance of patient empowerment in achieving this. The iManageCancer projec...\n",
      "Generated:  . the idea of treating each patient with the right drug at the right time, more and more cancer patients are being cured, or might have to cope with ...\n",
      "ROUGE-1: 0.4621\n",
      "ROUGE-2: 0.2137\n",
      "ROUGE-L: 0.2500\n",
      "BLEU: 0.1007\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 54...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 55/371 [03:44<21:51,  4.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 54:\n",
      "Reference: This systematic review examines the available evidence for an association between periodontal disease and rheumatoid arthritis. Nineteen studies were ...\n",
      "Generated:  erythrocyte sedimentation rates. MEDLINE/PubMed, CINAHL, DOSS, Embase, Scopus, Web of Knowledge, MedNar, and ProQuest Theses and Dissertations were s...\n",
      "ROUGE-1: 0.4388\n",
      "ROUGE-2: 0.2165\n",
      "ROUGE-L: 0.2347\n",
      "BLEU: 0.1506\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 55...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 56/371 [03:48<21:39,  4.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 55:\n",
      "Reference: This study reviews the use of machine learning (ML) and deep learning (DL) methods in the field of cancer prognosis modeling. The study focuses on the...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Prediction of Cancer Disease using Machine learning ApproachCancer , Deep learning , ML , ANN , SVM , Decision tress.Cancer has identified ...\n",
      "ROUGE-1: 0.3584\n",
      "ROUGE-2: 0.1011\n",
      "ROUGE-L: 0.2079\n",
      "BLEU: 0.0299\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 56...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 57/371 [03:52<21:30,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 56:\n",
      "Reference: This paper discusses the importance of data mining in building intelligent models for detecting heart disease. The paper explores various data mining ...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Prediction of Heart Disease at early stage using Data Mining and Big Data Analytics: A SurveyData Mining; big data; CVD; risk factors; accu...\n",
      "ROUGE-1: 0.4976\n",
      "ROUGE-2: 0.1576\n",
      "ROUGE-L: 0.2439\n",
      "BLEU: 0.0269\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 57...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 58/371 [03:56<21:36,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 57:\n",
      "Reference: This study aimed to investigate the prognostic value of Krebs von den Lungen-6 (KL-6) levels in patients with rheumatoid arthritis-associated intersti...\n",
      "Generated:   The summary length may be within 150 words\n",
      "<</SYS>>\n",
      "Prognostic role of blood KL-6 in rheumatoid arthritis–associated interstitial lung diseaseRheuma...\n",
      "ROUGE-1: 0.3810\n",
      "ROUGE-2: 0.1604\n",
      "ROUGE-L: 0.2222\n",
      "BLEU: 0.0688\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 58...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 59/371 [04:00<21:22,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 58:\n",
      "Reference: This paper proposes a method for improving the performance of text generation models by incorporating contextual information during training. The auth...\n",
      "Generated:  \n",
      "Long short-term memory(LSTM) units on sequence-based models are being used in translation,\n",
      "question-answering systems, classification tasks due to t...\n",
      "ROUGE-1: 0.3103\n",
      "ROUGE-2: 0.0581\n",
      "ROUGE-L: 0.1494\n",
      "BLEU: 0.0406\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 59...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 60/371 [04:04<21:18,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 59:\n",
      "Reference: This paper discusses the importance of using given starting words to generate sentences and filling in sentences in natural language processing tasks....\n",
      "Generated:  \n",
      "Pre-training on New Corpora Using BERT and \n",
      "GPT-2text generation; OpenAI GPT-1;\n",
      "BERTUsing a given starting word to make a sentence or filling \n",
      "in se...\n",
      "ROUGE-1: 0.4471\n",
      "ROUGE-2: 0.1310\n",
      "ROUGE-L: 0.2353\n",
      "BLEU: 0.0563\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 60...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 61/371 [04:08<21:13,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 60:\n",
      "Reference: This survey discusses the importance of Automatic Text Summarization in dealing with the problem of information overload on the internet. It reviews d...\n",
      "Generated:  \n",
      "This article briefly reviews different methods and evaluation met\n",
      "rics. The main attention is on the applications of the latest trends,\n",
      "neural netwo...\n",
      "ROUGE-1: 0.4259\n",
      "ROUGE-2: 0.1869\n",
      "ROUGE-L: 0.2963\n",
      "BLEU: 0.0889\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 61...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 62/371 [04:13<21:02,  4.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 61:\n",
      "Reference: This paper discusses the need for bilingual (Hindi and English) unsupervised automatic text summarization using deep learning, as there is an increase...\n",
      "Generated:  \n",
      "In this paper we are presenting Bilingual (Hindi and \n",
      "English) unsupervised automatic text summarization \n",
      "using deep learning. which is an important...\n",
      "ROUGE-1: 0.5106\n",
      "ROUGE-2: 0.3011\n",
      "ROUGE-L: 0.4574\n",
      "BLEU: 0.2035\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 62...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 63/371 [04:17<21:00,  4.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 62:\n",
      "Reference: This paper compares global and local attention mechanisms in LSTM models for abstractive text summarization using a dataset of Amazon reviews. The glo...\n",
      "Generated:  \n",
      "enhance a neural machine translation (NMT). There are two \n",
      "classes of attentions: global and local attentions. This paper \n",
      "focuses on comparing the ...\n",
      "ROUGE-1: 0.5655\n",
      "ROUGE-2: 0.2937\n",
      "ROUGE-L: 0.3724\n",
      "BLEU: 0.1225\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 63...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 64/371 [04:21<20:52,  4.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 63:\n",
      "Reference: This paper proposes four novel ATS models utilizing a Seq2Seq structure with an attention-based bidirectional LSTM to improve the Automatic Text Summa...\n",
      "Generated:  \n",
      "Natural language processing (NLP), automatic text summarization (ATS), sequence\n",
      "to-sequence (Seq2Seq) model, attention mechanism, bidirectional LSTM...\n",
      "ROUGE-1: 0.4068\n",
      "ROUGE-2: 0.1143\n",
      "ROUGE-L: 0.2373\n",
      "BLEU: 0.0342\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 64...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 65/371 [04:25<21:00,  4.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 64:\n",
      "Reference: The paper presents a new abstractive text summarization model, called MAPCoL, that uses multi-layered attentional peephole convolutional LSTM to gener...\n",
      "Generated:  \n",
      "An Optimized Abstractive Text Summarization Model\n",
      "Using Peephole Convolutional LSTMabstractive text summarization; deep learning; convolutional neur...\n",
      "ROUGE-1: 0.4247\n",
      "ROUGE-2: 0.1528\n",
      "ROUGE-L: 0.2192\n",
      "BLEU: 0.0263\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 65...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 66/371 [04:29<20:50,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 65:\n",
      "Reference: The study explores the use of Bidirectional GRU with attention to summarize Bahasa Indonesian text data. The proposed model outperforms extractive met...\n",
      "Generated:  \n",
      "of the text, which has a higher difficulty. But, it produces a more natural summary and higher inter-sentence cohesion. Recurrent\n",
      "Neural Network (RN...\n",
      "ROUGE-1: 0.4239\n",
      "ROUGE-2: 0.0549\n",
      "ROUGE-L: 0.1522\n",
      "BLEU: 0.0113\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 66...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 67/371 [04:33<20:43,  4.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 66:\n",
      "Reference: The increase in data shared online presents a threat to privacy. This study analyzed how well people can be recognized in social media data and propos...\n",
      "Generated:  \n",
      "volume of data shared on the web keeps increasing and presents a threat\n",
      "to our privacy. This works contributes to the understanding of privacy\n",
      "impli...\n",
      "ROUGE-1: 0.5600\n",
      "ROUGE-2: 0.2727\n",
      "ROUGE-L: 0.3700\n",
      "BLEU: 0.1985\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 67...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 68/371 [04:37<20:34,  4.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 67:\n",
      "Reference: The paper proposes a novel interpretable model for personality recognition based on a Chinese personality lexicon constructed using word embedding tec...\n",
      "Generated:  \n",
      "Human behavioral data. With the rise of social media, increasing attention has been paid to the ability\n",
      "to recognize personality traits by analyzing...\n",
      "ROUGE-1: 0.4821\n",
      "ROUGE-2: 0.1969\n",
      "ROUGE-L: 0.2462\n",
      "BLEU: 0.1010\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 68...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 69/371 [04:41<20:58,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 68:\n",
      "Reference: This paper proposes a method for face recognition called the Laplacianface approach, which uses Locality Preserving Projections (LPP) to map face imag...\n",
      "Generated:  \n",
      "Preserving Projections (LPP), the face images are mapped into a face subspace for analysis. Different from Principal Component\n",
      "Analysis (PCA) and Li...\n",
      "ROUGE-1: 0.5472\n",
      "ROUGE-2: 0.2762\n",
      "ROUGE-L: 0.3113\n",
      "BLEU: 0.2222\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 69...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 70/371 [04:46<20:50,  4.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 69:\n",
      "Reference: The paper proposes an attention-aware face recognition method based on a deep convolutional neural network and reinforcement learning. The method incl...\n",
      "Generated:  \n",
      "Based on this, this paper proposes an attention-aware face recog\n",
      "nition method based on a deep convolutional neural network and\n",
      "reinforcement learni...\n",
      "ROUGE-1: 0.6842\n",
      "ROUGE-2: 0.4468\n",
      "ROUGE-L: 0.4737\n",
      "BLEU: 0.3366\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 70...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 71/371 [04:50<20:33,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 70:\n",
      "Reference: This paper introduces a new method for face recognition called Local Binary Pattern Network (LBPNet), which combines the deep learning architecture of...\n",
      "Generated:  \n",
      "LOCAL BINARY PATTERN NETWORK : A DEEP LEARNING APPROACH FOR FACE\n",
      "RECOGNITION Deep learning, Local Binary Pattern,\n",
      "PCA, Convolutional Neural NetworkD...\n",
      "ROUGE-1: 0.4938\n",
      "ROUGE-2: 0.2500\n",
      "ROUGE-L: 0.2963\n",
      "BLEU: 0.1511\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 71...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 72/371 [04:54<20:24,  4.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 71:\n",
      "Reference: This paper discusses the use of deep learning for face detection, using the YOLO library and a convolutional neural network. The paper compares the ac...\n",
      "Generated:  \n",
      "using YOLOFace Detection, YOLO, Neural Network, object \n",
      "detection, Convolutional Neural Network Deep learning is nowadays a buzzword and is \n",
      "conside...\n",
      "ROUGE-1: 0.3301\n",
      "ROUGE-2: 0.0784\n",
      "ROUGE-L: 0.1942\n",
      "BLEU: 0.0379\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 72...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 73/371 [04:58<20:54,  4.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 72:\n",
      "Reference: This article discusses the importance of epidemiological data in understanding and predicting the risks associated with the outbreak of COVID-19 cause...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Analyzing the epidemiological outbreak of COVID‐19: A visual exploratory data analysis approachChina, coronavirus, CO VID‐19, data analysis...\n",
      "ROUGE-1: 0.4019\n",
      "ROUGE-2: 0.1643\n",
      "ROUGE-L: 0.2105\n",
      "BLEU: 0.0259\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 73...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 74/371 [05:02<19:48,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 73:\n",
      "Reference: This systematic literature review evaluates published studies reporting associations between patient-reported symptoms and clinical signs of dry eye d...\n",
      "Generated:   The summary length may be within 150 words\n",
      "<</SYS>>\n",
      "Associations between signs and symptoms of dry eye disease: a systematic reviewAssociationations...\n",
      "ROUGE-1: 0.4040\n",
      "ROUGE-2: 0.1837\n",
      "ROUGE-L: 0.2020\n",
      "BLEU: 0.0918\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 74...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 75/371 [05:06<19:54,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 74:\n",
      "Reference: This article discusses the use of machine learning techniques in the early detection of diabetic eye disease, which is a common complication of diabet...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Automatic Detection of Diabetic Eye Disease Through Deep Learning Using Fundus Images: A SurveyDiabetic eye disease, diabetic retinopathy, ...\n",
      "ROUGE-1: 0.4327\n",
      "ROUGE-2: 0.1553\n",
      "ROUGE-L: 0.2115\n",
      "BLEU: 0.0932\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 75...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 76/371 [05:10<19:54,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 75:\n",
      "Reference: This paper proposes a scalable approach to computing betweenness centrality in graph networks, incorporating node and edge colors to efficiently query...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Querying large graphs in biomedicine with colored graphs and decompositionLarge graphs , OLAP , Coloredgraphs , Betweenness.In graph networ...\n",
      "ROUGE-1: 0.3529\n",
      "ROUGE-2: 0.0865\n",
      "ROUGE-L: 0.2139\n",
      "BLEU: 0.0186\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 76...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 77/371 [05:14<19:53,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 76:\n",
      "Reference: This review article discusses the use of machine learning, particularly deep learning, in the detection of kidney tumors through radiology imaging sca...\n",
      "Generated:   The summary length may be within 150 words\n",
      "<</SYS>>\n",
      "Radiology Imaging Scans for Early Diagnosis of Kidney Tumors: A Review of Data Analytics-Based M...\n",
      "ROUGE-1: 0.3802\n",
      "ROUGE-2: 0.1083\n",
      "ROUGE-L: 0.1736\n",
      "BLEU: 0.0152\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 77...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 78/371 [05:18<19:48,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 77:\n",
      "Reference: The article discusses the importance of active medication stocking in preventive healthcare management, especially in the secondary prevention stage, ...\n",
      "Generated:   Long short-term memory networks.As an important task in digital preventive healthcare management, especially in the secondary prevention stage, acti...\n",
      "ROUGE-1: 0.4839\n",
      "ROUGE-2: 0.2358\n",
      "ROUGE-L: 0.2742\n",
      "BLEU: 0.1515\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 78...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 79/371 [05:22<19:42,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 78:\n",
      "Reference: This paper focuses on the analysis and mining of large sports medical data, aiming to achieve effective prediction and risk assessment of sports medic...\n",
      "Generated:   The summary length may be within 150 words\n",
      "<</SYS>>\n",
      "Research and Analysis of Sport Medical Data Processing Algorithms Based on Deep Learning and Int...\n",
      "ROUGE-1: 0.4590\n",
      "ROUGE-2: 0.1653\n",
      "ROUGE-L: 0.2869\n",
      "BLEU: 0.0366\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 79...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 80/371 [05:26<19:51,  4.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 79:\n",
      "Reference: This project proposes a deep convolutional neural network (CNN) model with image augmentation (IA) technique for person recognition using gait feature...\n",
      "Generated:  \n",
      "train Deep learning algorithms. The tests were achieved using known dataset (Market \n",
      "dataset). The dataset contains sequential pictures of people in...\n",
      "ROUGE-1: 0.4308\n",
      "ROUGE-2: 0.1762\n",
      "ROUGE-L: 0.1744\n",
      "BLEU: 0.0898\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 80...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 81/371 [05:30<19:49,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 80:\n",
      "Reference: The study focuses on person recognition from thermal imagery and evaluates the influence of image resolution on recognition accuracy. The researchers ...\n",
      "Generated:  \n",
      "retrieval of the key context information about the user and\n",
      "the environment. Image processing techniques have been widely\n",
      "applied in this area, prov...\n",
      "ROUGE-1: 0.1627\n",
      "ROUGE-2: 0.0290\n",
      "ROUGE-L: 0.0957\n",
      "BLEU: 0.0065\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 81...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 82/371 [05:34<19:37,  4.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 81:\n",
      "Reference: The article presents a new framework for person recognition under unconstrained settings, which integrates a Region Attention Network to combine visua...\n",
      "Generated:  \n",
      "recognizing persons under unconstrained settings remains\n",
      "challenging. Issues like profile views, unfavorable lighting,\n",
      "and occlusions can cause subs...\n",
      "ROUGE-1: 0.3234\n",
      "ROUGE-2: 0.0727\n",
      "ROUGE-L: 0.1796\n",
      "BLEU: 0.0247\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 82...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 83/371 [05:38<19:25,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 82:\n",
      "Reference: The paper introduces the People In Photo Albums (PIPA) dataset for unconstrained person recognition, which contains over 60,000 instances of 2,000 ind...\n",
      "Generated:  \n",
      "this, we introduce the new People In Photo Albums (PIPA)\n",
      "dataset, consisting of over 60000 instances of ∼2000 in\n",
      "dividuals collected from public Fli...\n",
      "ROUGE-1: 0.5839\n",
      "ROUGE-2: 0.2893\n",
      "ROUGE-L: 0.3727\n",
      "BLEU: 0.2693\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 83...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 84/371 [05:42<19:20,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 83:\n",
      "Reference: This paper focuses on using virtual tasks as sub-tasks in multi-label learning for gait-based person recognition. By incorporating pseudo labels with ...\n",
      "Generated:  \n",
      "Among them, we focus on approaches that use multiple labels\n",
      "for learning, typified by multi-task learning. These approaches\n",
      "are sometimes used to im...\n",
      "ROUGE-1: 0.4074\n",
      "ROUGE-2: 0.0875\n",
      "ROUGE-L: 0.2099\n",
      "BLEU: 0.0092\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 84...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 85/371 [05:46<19:22,  4.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 84:\n",
      "Reference: Sentiment analysis or opinion mining is the process of computationally analyzing people's opinions and emotions towards entities, individuals, issues,...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "A survey of opinion mining and sentimrnt analysisopinion mining ,sentiment analysis, NLPSentiment analysis or opinion mining is the computa...\n",
      "ROUGE-1: 0.4979\n",
      "ROUGE-2: 0.2638\n",
      "ROUGE-L: 0.3460\n",
      "BLEU: 0.1816\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 85...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 86/371 [05:50<19:15,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 85:\n",
      "Reference: The article discusses the problem of classifying documents based on sentiment, rather than topic. Machine learning techniques outperform human-produce...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Thumbs up? Sentiment Classification using Machine Learning Techniquessentiment classificationWe consider the problem of classifying documen...\n",
      "ROUGE-1: 0.4541\n",
      "ROUGE-2: 0.1967\n",
      "ROUGE-L: 0.2919\n",
      "BLEU: 0.1086\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 86...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 87/371 [05:54<19:10,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 86:\n",
      "Reference: The text describes a machine learning framework that uses recursive autoencoders to predict sentiment label distributions for multi-word phrases witho...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributionsautoen coders, sentiment distributionWe introduce a novel mach...\n",
      "ROUGE-1: 0.5934\n",
      "ROUGE-2: 0.4222\n",
      "ROUGE-L: 0.4066\n",
      "BLEU: 0.2844\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 87...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▎       | 88/371 [05:59<19:02,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 87:\n",
      "Reference: The article discusses the rise of microblogs, such as Twitter, and how sentiment analysis on entities (products, organizations, people) can be used to...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Combining Lexicon-based and Learning-based Methods for Twitter Sentiment Analysissentiment analysis, lexicon based approach,, sentiment cla...\n",
      "ROUGE-1: 0.5414\n",
      "ROUGE-2: 0.3017\n",
      "ROUGE-L: 0.3315\n",
      "BLEU: 0.2490\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 88...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 89/371 [06:03<19:03,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 88:\n",
      "Reference: The article discusses movie review mining as a sentiment-based classification problem that is different from topic-based classifications. Two approach...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Movie Review Mining: a Comparison between Supervised and Unsupervised Classification Approachessemantic orientation, machine learning,, opi...\n",
      "ROUGE-1: 0.5340\n",
      "ROUGE-2: 0.2328\n",
      "ROUGE-L: 0.2723\n",
      "BLEU: 0.0938\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 89...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 90/371 [06:07<19:00,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 89:\n",
      "Reference: Opinion mining (OM) is a field that focuses on extracting opinions expressed in text, rather than the topic of the text. It has many applications, inc...\n",
      "Generated:  \n",
      "SENTIWORDNET: A Publicly Available Lexical Resource for Opinion Miningsentiwordnet, opinion mining,, synset, NLPOpinion mining (OM) is a recent subd...\n",
      "ROUGE-1: 0.4706\n",
      "ROUGE-2: 0.1683\n",
      "ROUGE-L: 0.2255\n",
      "BLEU: 0.0799\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 90...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 91/371 [06:11<18:55,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 90:\n",
      "Reference: This study is a review of recent research on the use of neural networks in the early diagnosis of melanoma, a highly aggressive type of cancer. The st...\n",
      "Generated:  \n",
      " Neural networks in 2019 have greater sensitivity and specificity than dermatologists \n",
      " A neural network can evaluate features that might be unavail...\n",
      "ROUGE-1: 0.6614\n",
      "ROUGE-2: 0.4739\n",
      "ROUGE-L: 0.5259\n",
      "BLEU: 0.3011\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 91...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 92/371 [06:15<18:49,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 91:\n",
      "Reference: This paper presents a survey of various cancer diagnosis and prognosis models using data mining and machine learning approaches. The study discusses t...\n",
      "Generated:   The summary length may be within 150 words\n",
      "<</SYS>>\n",
      "Review paper on research direction towards cancer prediction and prognosis using machine learnin...\n",
      "ROUGE-1: 0.3206\n",
      "ROUGE-2: 0.0769\n",
      "ROUGE-L: 0.1756\n",
      "BLEU: 0.0069\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 92...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 93/371 [06:19<18:43,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 92:\n",
      "Reference: This study aimed to investigate the feasibility of using high-throughput gene expression data and clinical data for predicting cancer patients' surviv...\n",
      "Generated:   The summary length may be within 150 words\n",
      "<</SYS>>\n",
      "Risk classification of cancer survival using ANN with gene expression data from multiple laborat...\n",
      "ROUGE-1: 0.5702\n",
      "ROUGE-2: 0.2124\n",
      "ROUGE-L: 0.2456\n",
      "BLEU: 0.1182\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 93...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 94/371 [06:23<18:41,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 93:\n",
      "Reference: This systematic review examines the relationship between diabetes and the incidence of dementia. The study identified 14 eligible longitudinal populat...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Risk of dementia in diabetes mellitus: a systematic reviewDiabetes, dementia, incidence, risk factors, vascular disease, glucose, insulin, ...\n",
      "ROUGE-1: 0.5020\n",
      "ROUGE-2: 0.2041\n",
      "ROUGE-L: 0.2105\n",
      "BLEU: 0.0995\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 94...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 95/371 [06:27<18:38,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 94:\n",
      "Reference: This review paper aimed to assess the risk of developing interstitial lung disease (ILD) with the use of epidermal growth factor receptor tyrosine kin...\n",
      "Generated:   The summary length may be within 150 words\n",
      "<</SYS>>\n",
      "Risk of interstitial lung disease associated with EGFR-TKIs in advanced non-small-cell lung canc...\n",
      "ROUGE-1: 0.3858\n",
      "ROUGE-2: 0.2564\n",
      "ROUGE-L: 0.2538\n",
      "BLEU: 0.1144\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 95...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 96/371 [06:31<19:12,  4.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 95:\n",
      "Reference: This survey paper provides a systematic literature review of various machine learning and deep learning techniques used for medical data analysis in r...\n",
      "Generated:   ilea: 314. (1-088, 2-204) iisas (2.204) 31873 Gio! 21240) Issac (21332) iissa! 2120979” s21382) Gioia, 270) ileetimate 21-389, 2ss21. The summary le...\n",
      "ROUGE-1: 0.2788\n",
      "ROUGE-2: 0.0680\n",
      "ROUGE-L: 0.1538\n",
      "BLEU: 0.0076\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 96...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 97/371 [06:36<19:26,  4.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 96:\n",
      "Reference: The study investigated risk and protective factors associated with pediatric dry eye disease (DED) in relation to smartphone use rate according to reg...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "\n",
      "Smartphone use is a risk factor for pediatric dry eye disease according to region and age: a case control studyDry eye disease, Pediatrics...\n",
      "ROUGE-1: 0.4872\n",
      "ROUGE-2: 0.2586\n",
      "ROUGE-L: 0.2650\n",
      "BLEU: 0.1939\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 97...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 98/371 [06:40<19:00,  4.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 97:\n",
      "Reference: The study aimed to explore patients' perceptions of sensitive health information and how such perceptions affect data sharing preferences. After condu...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "State of the art and a mixed-method personalized approach to assess patient perceptions on medical record sharing and sensitivitySensitive ...\n",
      "ROUGE-1: 0.4366\n",
      "ROUGE-2: 0.1986\n",
      "ROUGE-L: 0.2042\n",
      "BLEU: 0.0931\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 98...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 99/371 [06:44<18:41,  4.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 98:\n",
      "Reference: The study investigates the early debates about the dietary causes of coronary heart disease (CHD) and the role of the sugar industry in shaping these ...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Sugar Industry and Coronary Heart Disease Research : A Historical Analysis of Internal Industry Documentscoronary heart disease, sugar, res...\n",
      "ROUGE-1: 0.5263\n",
      "ROUGE-2: 0.2609\n",
      "ROUGE-L: 0.2679\n",
      "BLEU: 0.1961\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 99...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 100/371 [06:48<18:32,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 99:\n",
      "Reference: The article discusses the problem of the lack of proper indication of uncertainty in most prognostic models, which hampers their translation to primar...\n",
      "Generated:   The summary length may be within 150 words\n",
      "<</SYS>>\n",
      "Targeting the uncertainty of predictions at patient-level using an ensemble of classifiers coupl...\n",
      "ROUGE-1: 0.4038\n",
      "ROUGE-2: 0.0874\n",
      "ROUGE-L: 0.1731\n",
      "BLEU: 0.0490\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 101/371 [06:52<18:25,  4.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 100:\n",
      "Reference: This scoping review provides a comprehensive summary of 91 studies that investigated the use of neural networks, specifically deep learning algorithms...\n",
      "Generated:   The summary length may be within 150 words\n",
      "<</SYS>>\n",
      "\n",
      "The Role of Neural Network for the Detection of Parkinson’s Disease: A Scoping ReviewParkinson’...\n",
      "ROUGE-1: 0.2931\n",
      "ROUGE-2: 0.0870\n",
      "ROUGE-L: 0.1466\n",
      "BLEU: 0.0238\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 101...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 102/371 [06:56<18:18,  4.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 101:\n",
      "Reference: This paper provides a comprehensive survey of automatic text summarization (ATS), which is the method of reducing source text into a compact variant w...\n",
      "Generated:   The summary length may be within 150 words\n",
      "<</SYS>>\n",
      "A Survey of Automatic Text Summarization: Progress, Process and ChallengesAutomatic text summari...\n",
      "ROUGE-1: 0.4762\n",
      "ROUGE-2: 0.1834\n",
      "ROUGE-L: 0.2338\n",
      "BLEU: 0.0929\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 102...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 103/371 [07:00<18:07,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 102:\n",
      "Reference: This article presents a qualitative review and meta-analysis of 42 English articles to assess the strength of evidence for an association between smok...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Tobacco and tuberculosis: a qualitative systematic review and meta-analysisTuberculosis; smoking; second-hand smoke; risk factors.To assess...\n",
      "ROUGE-1: 0.4959\n",
      "ROUGE-2: 0.3033\n",
      "ROUGE-L: 0.3415\n",
      "BLEU: 0.1991\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 103...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 104/371 [07:04<18:10,  4.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 103:\n",
      "Reference: This study proposes a mathematical model to improve the efficiency of patient scheduling for Intensive-Modulated Radiation Therapy (IMRT) in Taiwan. T...\n",
      "Generated:   The summary length may be within 150 words\n",
      "<</SYS>>\n",
      "Utilizing online stochastic optimization on scheduling of intensity-modulate radiotherapy therap...\n",
      "ROUGE-1: 0.2667\n",
      "ROUGE-2: 0.0717\n",
      "ROUGE-L: 0.1600\n",
      "BLEU: 0.0142\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 104...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 105/371 [07:08<18:06,  4.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 104:\n",
      "Reference: This paper proposes a new method, called the Weight-based Multiple Empirical Kernel Learning with Neighbor Discriminant Constraint (WMEKL-NDC) method,...\n",
      "Generated:   The summary length may be within 150 words\n",
      "<</SYS>>\n",
      "\n",
      "Weight-based multiple empirical kernel learning with neighbor discriminant constraint for heart...\n",
      "ROUGE-1: 0.3738\n",
      "ROUGE-2: 0.1321\n",
      "ROUGE-L: 0.2430\n",
      "BLEU: 0.0199\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 105...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 106/371 [07:12<17:57,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 105:\n",
      "Reference: The article discusses the use of Convolutional Neural Networks (CNN) for person recognition, specifically in addressing the issue of appearance-based ...\n",
      "Generated:  \n",
      "person recognition problems were primarily addressed by \n",
      "image-based techniques. However, as the use of cameras and \n",
      "monitoring increases, image-bas...\n",
      "ROUGE-1: 0.4343\n",
      "ROUGE-2: 0.1224\n",
      "ROUGE-L: 0.2020\n",
      "BLEU: 0.0295\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 106...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 107/371 [07:16<17:49,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 106:\n",
      "Reference: The article highlights the importance of gait recognition and proposes using machine learning algorithms for human personality recognition based on ou...\n",
      "Generated:  \n",
      "computer vision; segmentationThe gait is a special feature that needs to be \n",
      "identified as it affects many parts of the body. Each person's gait \n",
      "is...\n",
      "ROUGE-1: 0.2953\n",
      "ROUGE-2: 0.0680\n",
      "ROUGE-L: 0.1611\n",
      "BLEU: 0.0165\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 107...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 108/371 [07:20<17:50,  4.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 107:\n",
      "Reference: This study proposes a novel recurrent network architecture to model the relational information between people in a photo for person recognition. The a...\n",
      "Generated:  \n",
      "Sequential Person Recognition in Photo Albums with a Recurrent Networkperson recognition, photo albums, relational information, recurrent network, c...\n",
      "ROUGE-1: 0.4052\n",
      "ROUGE-2: 0.1325\n",
      "ROUGE-L: 0.1830\n",
      "BLEU: 0.0902\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 108...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 109/371 [07:25<17:50,  4.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 108:\n",
      "Reference: The paper discusses the implementation of a story generation system called Story Scrambler using recurrent neural networks. The system aims to generat...\n",
      "Generated:  \n",
      "system using RNN and LSTM is discussed. By increasing \n",
      "the values of different parameters such as the number of \n",
      "neurons, number of layers, batch si...\n",
      "ROUGE-1: 0.5385\n",
      "ROUGE-2: 0.2111\n",
      "ROUGE-L: 0.3626\n",
      "BLEU: 0.1410\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 109...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 110/371 [07:29<17:44,  4.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 109:\n",
      "Reference: The paper presents a neural network model for generating social media text using Weibo data. The model utilizes word embedding and RNN with LSTM cells...\n",
      "Generated:  \n",
      "of the linguistic arbitrariness. This paper presents a neural \n",
      "network model building a social media NLG system. Compared \n",
      "with state-of-art model, ...\n",
      "ROUGE-1: 0.5342\n",
      "ROUGE-2: 0.2390\n",
      "ROUGE-L: 0.2609\n",
      "BLEU: 0.1245\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 110...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 111/371 [07:33<17:39,  4.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 110:\n",
      "Reference: The distribution of fake news on social media is a growing problem, and detecting it has become important. The paper presents an ensemble approach cal...\n",
      "Generated:  \n",
      "transformer-based embeddings and utilizes text sum\n",
      "marization as the main text transformation step before\n",
      "classifying a document.\n",
      "CMTR-BERT is able ...\n",
      "ROUGE-1: 0.3825\n",
      "ROUGE-2: 0.1878\n",
      "ROUGE-L: 0.2623\n",
      "BLEU: 0.1244\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 111...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 112/371 [07:37<17:38,  4.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 111:\n",
      "Reference: This paper discusses the development of a lecture summarization service using BERT and K-means clustering. The service utilizes deep learning models t...\n",
      "Generated:  \n",
      "requiring several hours of manual tuning to produce \n",
      "meaningful results. Recently, new machine learning \n",
      "architectures have provided mechanisms for ...\n",
      "ROUGE-1: 0.4342\n",
      "ROUGE-2: 0.1867\n",
      "ROUGE-L: 0.2105\n",
      "BLEU: 0.0438\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 112...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 113/371 [07:41<17:26,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 112:\n",
      "Reference: Generating a summary from multiple documents is challenging due to the lack of parallel data. This paper presents a novel adaptation method that combi...\n",
      "Generated:   neural encoder-decoder\n",
      "   maximal marginal relevance                   abstractive summarization龍�   automatic metricsGenerating a text abstract fro...\n",
      "ROUGE-1: 0.3787\n",
      "ROUGE-2: 0.0599\n",
      "ROUGE-L: 0.1657\n",
      "BLEU: 0.0229\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 113...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 114/371 [07:45<17:48,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 113:\n",
      "Reference: The paper proposes an attentional encoder-decoder model for abstractive text summarization, which achieves state-of-the-art performance on two differe...\n",
      "Generated:   Attentional Encoder-Decoder\n",
      "    Recurrent Neural Networks龍�   Multi-Sentence Summarization\n",
      "  Internet   Performance BenchmarksIn this work, we model...\n",
      "ROUGE-1: 0.4966\n",
      "ROUGE-2: 0.2657\n",
      "ROUGE-L: 0.3724\n",
      "BLEU: 0.1272\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 114...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 115/371 [07:49<17:33,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 114:\n",
      "Reference: The use of sequence generative models with RNN variants has shown promise in abstractive document summarization, but they still have limitations when ...\n",
      "Generated:  \n",
      "the final summary one at a time. The experimental results \n",
      "have shown the effectiveness and the superiority of the pro\n",
      "posed model compared to the-s...\n",
      "ROUGE-1: 0.4639\n",
      "ROUGE-2: 0.1562\n",
      "ROUGE-L: 0.2680\n",
      "BLEU: 0.0599\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 115...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 116/371 [07:53<17:18,  4.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 115:\n",
      "Reference: This paper compares the impact of global and local attention mechanisms on the LSTM model for abstractive text summarization using a dataset of Amazon...\n",
      "Generated:  \n",
      "enhance a neural machine translation (NMT). There are two \n",
      "classes of attentions: global and local attentions. This paper \n",
      "focuses on comparing the ...\n",
      "ROUGE-1: 0.5217\n",
      "ROUGE-2: 0.2527\n",
      "ROUGE-L: 0.2935\n",
      "BLEU: 0.1227\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 116...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 117/371 [07:57<17:18,  4.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 116:\n",
      "Reference: This paper proposes an arbitrary view transformation model (AVTM) for gait recognition in person authentication, which addresses the challenge of robu...\n",
      "Generated:  \n",
      "proposed to solve this. The VTMs work well if the target\n",
      "views are the same as their discrete training views. However,\n",
      "the gait traits are observed ...\n",
      "ROUGE-1: 0.4824\n",
      "ROUGE-2: 0.1726\n",
      "ROUGE-L: 0.2513\n",
      "BLEU: 0.0981\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 117...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 118/371 [08:01<17:07,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 117:\n",
      "Reference: The article proposes a model for text summarization called T-BERTSum, which combines BERT's encoding capabilities with topic embedding to improve cont...\n",
      "Generated:  \n",
      "of data mining in information retrieval and natural language\n",
      "processing makes automatic text summarization necessary. Cur\n",
      "rently, pretrained word em...\n",
      "ROUGE-1: 0.3837\n",
      "ROUGE-2: 0.0588\n",
      "ROUGE-L: 0.1744\n",
      "BLEU: 0.0093\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 118...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 119/371 [08:05<17:04,  4.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 118:\n",
      "Reference: The paper proposes a hybrid approach for stock price movement prediction using machine learning, deep learning, and natural language processing. The s...\n",
      "Generated:  \n",
      "Regression, LSTM, Sentiment Analysis, Granger Causality, \n",
      "Cross-validation, Self-Organizing Fuzzy Neural Networks.\n",
      "Prediction of future movement of ...\n",
      "ROUGE-1: 0.3826\n",
      "ROUGE-2: 0.2018\n",
      "ROUGE-L: 0.1739\n",
      "BLEU: 0.1263\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 119...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 120/371 [08:09<17:07,  4.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 119:\n",
      "Reference: The paper proposes an unsupervised method for query-focused multi-document summarization using transfer learning from pre-trained sentence embedding m...\n",
      "Generated:  \n",
      "on transfer learning from sentence embedding models, BM25 model, \n",
      "and maximal marginal relevance criterion Query-focused multi-document summarizatio...\n",
      "ROUGE-1: 0.4043\n",
      "ROUGE-2: 0.1290\n",
      "ROUGE-L: 0.2234\n",
      "BLEU: 0.0272\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 120...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 121/371 [08:13<16:18,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 120:\n",
      "Reference: The study developed an automatic abstractive text summarization algorithm in Japanese using a neural network with a BERT encoder and Transformer-based...\n",
      "Generated:  σ\n",
      "<</SYS>>\n",
      "\n",
      "Japanese abstractive text summarization using BERTabstractive text summization; BERT; \n",
      "livedoor news corpus In    i   This   this   :   O...\n",
      "ROUGE-1: 0.2549\n",
      "ROUGE-2: 0.0400\n",
      "ROUGE-L: 0.1373\n",
      "BLEU: 0.0129\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 121...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 122/371 [08:17<16:26,  3.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 121:\n",
      "Reference: This paper discusses the use of automatic text summarization and topic modeling in natural language processing. The authors propose an algorithm that ...\n",
      "Generated:  \n",
      "Natural language processing, cosine similarity, TFIDF \n",
      "Vectorizer , BERT, Truncated SVD  Document summarization is one such task of the \n",
      "natural lan...\n",
      "ROUGE-1: 0.4270\n",
      "ROUGE-2: 0.1364\n",
      "ROUGE-L: 0.2135\n",
      "BLEU: 0.0226\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 122...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 123/371 [08:21<16:27,  3.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 122:\n",
      "Reference: The study describes an approach to extractive summarization using BERT model and clustering algorithms to generate summaries of suitable size dependin...\n",
      "Generated:  \n",
      "revision. This \n",
      "process is aided by scoring functions and clustering algorithms \n",
      "to help choose the most suitable sentences. We use the existing \n",
      "BE...\n",
      "ROUGE-1: 0.3653\n",
      "ROUGE-2: 0.1014\n",
      "ROUGE-L: 0.1735\n",
      "BLEU: 0.0416\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 123...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 124/371 [08:25<16:28,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 123:\n",
      "Reference: The paper proposes a contrastive learning model called SeqCo for supervised abstractive text summarization. The model maximizes similarities between a...\n",
      "Generated:   Text summarization\n",
      "    Supervised abstractive summarization龍�   Sequence-to-sequence text generation model*/(   Faithfulness ratingsContrastive lear...\n",
      "ROUGE-1: 0.3896\n",
      "ROUGE-2: 0.1053\n",
      "ROUGE-L: 0.1818\n",
      "BLEU: 0.0312\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 124...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▎      | 125/371 [08:29<16:25,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 124:\n",
      "Reference: The exponential growth of web textual data has led to difficulty in analyzing and extracting useful information. Automatic text summarization is used ...\n",
      "Generated:  \n",
      "Attention Mechanism, T5In the last recent years, there's a huge amount of \n",
      "data available on the internet, and is generated very rapidly. It \n",
      "is ver...\n",
      "ROUGE-1: 0.4481\n",
      "ROUGE-2: 0.1547\n",
      "ROUGE-L: 0.2186\n",
      "BLEU: 0.0260\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 125...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 126/371 [08:33<16:36,  4.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 125:\n",
      "Reference: The paper proposes a framework called Timeline-Sumy for summarizing social media data about an entity in chronological order. The framework consists o...\n",
      "Generated:   The popularity of social media shatters the barrier\n",
      "for online users to create and share information at\n",
      "any place at any time. As a consequence, it ...\n",
      "ROUGE-1: 0.3429\n",
      "ROUGE-2: 0.0905\n",
      "ROUGE-L: 0.1714\n",
      "BLEU: 0.0652\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 126...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 127/371 [08:37<16:26,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 126:\n",
      "Reference: This paper provides an overview of extractive text summarization, which is the process of obtaining salient information from a text document and prese...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "A survey on extractive text summarizationExtractive text summarizing,sentence Fusion,supervised and unsupervised learning methodsText Summa...\n",
      "ROUGE-1: 0.4706\n",
      "ROUGE-2: 0.1310\n",
      "ROUGE-L: 0.2706\n",
      "BLEU: 0.0916\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 127...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 128/371 [08:41<16:24,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 127:\n",
      "Reference: This paper proposes a new model called Feature-Guiding Generative Adversarial Networks (FGGAN) to improve text generation. FGGAN uses a feature guidan...\n",
      "Generated:  \n",
      "Generative adversarial networks, text generation, deep learning, reinforcement learning Text generation is a basic work of natural language processi...\n",
      "ROUGE-1: 0.3757\n",
      "ROUGE-2: 0.0894\n",
      "ROUGE-L: 0.1989\n",
      "BLEU: 0.0743\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 128...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 129/371 [08:45<16:18,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 128:\n",
      "Reference: This paper proposes the VGAN model, which combines generative adversarial nets (GAN) with variational autoencoder (VAE) to generate realistic text. Th...\n",
      "Generated:  \n",
      "train the model via policy gradient. We apply the proposed model to the\n",
      "task of text generation and compare it to other recent neural network\n",
      "based ...\n",
      "ROUGE-1: 0.5977\n",
      "ROUGE-2: 0.2791\n",
      "ROUGE-L: 0.3218\n",
      "BLEU: 0.1417\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 129...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 130/371 [08:49<16:10,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 129:\n",
      "Reference: The paper proposes Affect-LM, an LSTM language model for generating conversational text conditioned on affect categories. The model allows for customi...\n",
      "Generated:   Affective messages\n",
      "    Neural language models*/(   LSTM\n",
      " :   Affect-LM\n",
      "  Social verbal communication includes\n",
      "affective messages which are conveyed\n",
      "...\n",
      "ROUGE-1: 0.3311\n",
      "ROUGE-2: 0.0805\n",
      "ROUGE-L: 0.1325\n",
      "BLEU: 0.0191\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 130...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 131/371 [08:53<16:10,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 130:\n",
      "Reference: The paper proposes a new architecture for Variational Autoencoder (VAE) for text generation, which blends feed-forward convolutional and deconvolution...\n",
      "Generated:   Variational Autoencoder (VAE)\n",
      "   Hybrid architecture*/(    Convolutional and deconvolutional components\n",
      " :   KL-term collapsing\n",
      "    Text generation....\n",
      "ROUGE-1: 0.3505\n",
      "ROUGE-2: 0.1250\n",
      "ROUGE-L: 0.1649\n",
      "BLEU: 0.0533\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 131...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 132/371 [08:58<16:14,  4.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 131:\n",
      "Reference: This paper focuses on improving the training and convergence of Generative Adversarial Networks (GAN) for text generation, by redefining the loss func...\n",
      "Generated:  \n",
      "some experiments under different model structures and different parameters indicates the model with truth\n",
      "guided and self-attention mechanism gets b...\n",
      "ROUGE-1: 0.5294\n",
      "ROUGE-2: 0.1786\n",
      "ROUGE-L: 0.2000\n",
      "BLEU: 0.0326\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 132...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 133/371 [09:02<16:09,  4.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 132:\n",
      "Reference: This paper proposes a new approach for Gait Recognition using a deep convolutional neural network with 3D convolutions and a special input format incl...\n",
      "Generated:  \n",
      "capturing spatio-temporal features. A special input format,\n",
      "consisting of the gray-scale image and optical flow enhance\n",
      "color invaranice. The approa...\n",
      "ROUGE-1: 0.6022\n",
      "ROUGE-2: 0.3478\n",
      "ROUGE-L: 0.3441\n",
      "BLEU: 0.2562\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 133...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 134/371 [09:06<16:22,  4.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 133:\n",
      "Reference: The paper explores the potential of using texture information extracted from millimeter wave (mmW) images for person recognition, focusing on three mm...\n",
      "Generated:  \n",
      "advantages including the ability to penetrate obscurants, such as\n",
      "clothes and polymers. After having explored shape information\n",
      "retrieved from mmW i...\n",
      "ROUGE-1: 0.4123\n",
      "ROUGE-2: 0.1770\n",
      "ROUGE-L: 0.2193\n",
      "BLEU: 0.0869\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 134...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 135/371 [09:11<16:46,  4.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 134:\n",
      "Reference: The study demonstrates the importance of human-related information in GIFs for emotion recognition and proposes a human-centered approach utilizing a ...\n",
      "Generated:  \n",
      "As an intuitive way of expression emotion, the animated Graphical Interchange Format (GIF) images have been widely used on social media. Most previo...\n",
      "ROUGE-1: 0.5690\n",
      "ROUGE-2: 0.2954\n",
      "ROUGE-L: 0.3347\n",
      "BLEU: 0.2219\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 135...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 136/371 [09:15<16:34,  4.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 135:\n",
      "Reference: The article introduces ViT5, a pretrained Transformer-based encoder-decoder model for the Vietnamese language that is trained on a large corpus of hig...\n",
      "Generated:  \n",
      "the English language thanks to its rich and\n",
      "large source of data, there has been min\n",
      "imal research into the same task in Viet\n",
      "namese, a much lower r...\n",
      "ROUGE-1: 0.4634\n",
      "ROUGE-2: 0.2222\n",
      "ROUGE-L: 0.2561\n",
      "BLEU: 0.1569\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 136...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 137/371 [09:19<16:25,  4.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 136:\n",
      "Reference: The article discusses automatic text summarization, which is used to compress documents while maintaining their main ideas. The proposed model, Long-T...\n",
      "Generated:  \n",
      "Extractive text summarization extracts important sentences from the original document to serve\n",
      "as the summary. The document representation method is...\n",
      "ROUGE-1: 0.5139\n",
      "ROUGE-2: 0.2676\n",
      "ROUGE-L: 0.3194\n",
      "BLEU: 0.1369\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 137...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 138/371 [09:23<16:09,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 137:\n",
      "Reference: The study aimed to investigate the recognition of emotional expressions in cartoon faces and the impact of key facial features (mouth, eyes, and eyebr...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Emotion Residue in Neutral Faces: Implications for Impression Formationcartoon faces, emotion recognition, facial features, expression inte...\n",
      "ROUGE-1: 0.5026\n",
      "ROUGE-2: 0.1905\n",
      "ROUGE-L: 0.2408\n",
      "BLEU: 0.1631\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 138...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 139/371 [09:27<16:01,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 138:\n",
      "Reference: The paper proposes PEGASUS, a pre-trained sequence-to-sequence model with a self-supervised objective tailored for abstractive text summarization. The...\n",
      "Generated:  \n",
      "self-supervised objectives on large text corpora\n",
      "has shown great success when fine-tuned on\n",
      "downstream NLP tasks including text summa\n",
      "rization. Howe...\n",
      "ROUGE-1: 0.3333\n",
      "ROUGE-2: 0.1000\n",
      "ROUGE-L: 0.1481\n",
      "BLEU: 0.0505\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 139...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 140/371 [09:31<15:51,  4.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 139:\n",
      "Reference: This paper presents an integrated Deep Neural Network (DNN) approach for recognizing emotions from cartoon images, which has not been extensively cove...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "\n",
      "Understanding cartoon emotion using integrated deep neural network on large datasetAnimation\n",
      "Cartoon\n",
      "Character Detection\n",
      "Convolutional Neu...\n",
      "ROUGE-1: 0.2947\n",
      "ROUGE-2: 0.1064\n",
      "ROUGE-L: 0.1579\n",
      "BLEU: 0.0084\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 140...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 141/371 [09:35<15:53,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 140:\n",
      "Reference: The paper discusses the challenges of facial emotion recognition and existing methods for solving it, including deep learning approaches. The authors ...\n",
      "Generated:  \n",
      "be very challenging and complex task due to large intra-class changes. Existing frameworks for this type of problem depends\n",
      "mostly on techniques lik...\n",
      "ROUGE-1: 0.2727\n",
      "ROUGE-2: 0.0816\n",
      "ROUGE-L: 0.1717\n",
      "BLEU: 0.0448\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 141...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 142/371 [09:39<15:41,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 141:\n",
      "Reference: The he paper proposes an animated GIF emotion recognition algorithm based on ResNet-ConvGRU, which extracts spatial and temporal features of GIF image...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Research on Animated GIFs Emotion Recognition Based on ResNet-ConvGRUGIF, emotion recognition, ResNet, ConvGRU, spatial-temporal features, ...\n",
      "ROUGE-1: 0.4324\n",
      "ROUGE-2: 0.1530\n",
      "ROUGE-L: 0.2811\n",
      "BLEU: 0.0221\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 142...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▊      | 143/371 [09:44<15:42,  4.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 142:\n",
      "Reference: The use of facial recognition systems in social media video-sharing applications like animoji and memoji is critiqued in this paper due to the ways in...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Facial recognition, emotion and race in animated social mediaFacial Recognition, animoji, racial identities, emotional expression, racializ...\n",
      "ROUGE-1: 0.4306\n",
      "ROUGE-2: 0.1836\n",
      "ROUGE-L: 0.2010\n",
      "BLEU: 0.0831\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 143...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 144/371 [09:48<15:33,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 143:\n",
      "Reference: This paper explores the use of OpenAI GPT-2 and BERT models for text generation and prediction, particularly for summary generation, machine translati...\n",
      "Generated:  \n",
      "have human thinking and creativity. We train the machine for \n",
      "specific tasks and then use it in natural language processing, which \n",
      "will help solve ...\n",
      "ROUGE-1: 0.6082\n",
      "ROUGE-2: 0.3077\n",
      "ROUGE-L: 0.2690\n",
      "BLEU: 0.2452\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 144...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 145/371 [09:52<15:23,  4.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 144:\n",
      "Reference: This paper presents a novel model for affective text generation that incorporates emotion as a prior for the state-of-the-art text generation model. T...\n",
      "Generated:  \n",
      "Human use language not just to convey information but also to express their inner feelings and\n",
      "mental states. In this work, we adapt the state-of-th...\n",
      "ROUGE-1: 0.5568\n",
      "ROUGE-2: 0.3103\n",
      "ROUGE-L: 0.3295\n",
      "BLEU: 0.0958\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 145...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 146/371 [09:56<15:15,  4.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 145:\n",
      "Reference: NUBIA is a methodology for building automatic evaluation metrics for text generation using machine learning models. It consists of three modules: a ne...\n",
      "Generated:  \n",
      "the successes of recent NLP architectures such as\n",
      "RoBERTa and GPT-2. These strong results suggest\n",
      "that using a neural networks to extract features a...\n",
      "ROUGE-1: 0.4375\n",
      "ROUGE-2: 0.1899\n",
      "ROUGE-L: 0.2250\n",
      "BLEU: 0.0973\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 146...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 147/371 [10:00<15:06,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 146:\n",
      "Reference: This research proposes an abstractive text summarization model that generates summaries based on extracted keywords, and compares it with a summary ge...\n",
      "Generated:  \n",
      "an abstractive summary can generate a summary based on \n",
      "extracted keywords. This research proposed an abstractive text \n",
      "summarization model, it gets...\n",
      "ROUGE-1: 0.5934\n",
      "ROUGE-2: 0.3111\n",
      "ROUGE-L: 0.3516\n",
      "BLEU: 0.1692\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 147...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 148/371 [10:04<15:00,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 147:\n",
      "Reference: The paper proposes DISCOBERT, a discourse-aware neural summarization model that uses sub-sentential discourse units as selection basis to reduce redun...\n",
      "Generated:  \n",
      "Discourse-Aware Neural Extractive Text SummarizationDISCOBERT, discourse unit, summarization, structural discourse graphs, Graph Convolutional Netwo...\n",
      "ROUGE-1: 0.4196\n",
      "ROUGE-2: 0.1702\n",
      "ROUGE-L: 0.2517\n",
      "BLEU: 0.0223\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 148...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 149/371 [10:08<14:58,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 148:\n",
      "Reference: The article proposes a semi-automatic method for collecting emotional animated GIFs from the internet with minimal human labor, in order to create a l...\n",
      "Generated:   'And today, the Pulitzer[/INST] The article discusses how Animated GIFs are commonly used to express emotions on the Internet but their automatic an...\n",
      "ROUGE-1: 0.7129\n",
      "ROUGE-2: 0.5500\n",
      "ROUGE-L: 0.6238\n",
      "BLEU: 0.4681\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 149...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 150/371 [10:12<14:54,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 149:\n",
      "Reference: The article explores the idea that iconic representations, such as cartoons, may be more efficient at communicating emotional information than realist...\n",
      "Generated:  \n",
      "their ability to communicate specific information, including emotion, quickly and efficiently, and that this effect is\n",
      "driven by changes in low-leve...\n",
      "ROUGE-1: 0.4912\n",
      "ROUGE-2: 0.2832\n",
      "ROUGE-L: 0.2982\n",
      "BLEU: 0.2184\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 150...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 151/371 [10:16<15:00,  4.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 150:\n",
      "Reference: The paper proposes a new facial expression database, Acted Facial Expressions in the Wild (AFEW), and its static subset, Static Facial Expressions in ...\n",
      "Generated:  \n",
      "semi-automatic recommender based method. The database contains videos showing natural head poses\n",
      "and movements, close to real-world illumination, mu...\n",
      "ROUGE-1: 0.5366\n",
      "ROUGE-2: 0.2869\n",
      "ROUGE-L: 0.3821\n",
      "BLEU: 0.1958\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 151...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 152/371 [10:20<14:52,  4.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 151:\n",
      "Reference: The paper presents the Gabor-based kernel partial-least-squares discrimination (GKPLSD) method for extracting facial features from face images, which ...\n",
      "Generated:   \n",
      "<</SYS>>\n",
      "Gabor-Based Kernel Partial-Least-Squares Discrimination Features for Face RecognitionGabor features, Kernel partial-least-squares, face re...\n",
      "ROUGE-1: 0.5860\n",
      "ROUGE-2: 0.3097\n",
      "ROUGE-L: 0.3439\n",
      "BLEU: 0.0282\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 152...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 153/371 [10:24<14:45,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 152:\n",
      "Reference: The paper presents an experimental study on detecting emotions from speech for use in driving the expression of computer-based characters such as avat...\n",
      "Generated:  \n",
      "avatars and virtual chat faces become more common, the use of\n",
      "emotion to drive the expression of the virtual characters become\n",
      "more important. The s...\n",
      "ROUGE-1: 0.5069\n",
      "ROUGE-2: 0.2419\n",
      "ROUGE-L: 0.3041\n",
      "BLEU: 0.1932\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 153...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 154/371 [10:28<14:42,  4.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 153:\n",
      "Reference: The paper describes a framework for social robots to detect and store emotions in a semantic repository, using an ontology called EMONTO. The framewor...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Emotion Detection for Social Robots Based on NLP Transformers and an Emotion Ontologysocial robots; natural language processing; ontology; ...\n",
      "ROUGE-1: 0.3204\n",
      "ROUGE-2: 0.0490\n",
      "ROUGE-L: 0.1845\n",
      "BLEU: 0.0168\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 154...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 155/371 [10:32<14:34,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 154:\n",
      "Reference: The paper discusses Person Re-Identification (Re-ID), which involves recognizing a person from various images captured by different cameras. Re-ID has...\n",
      "Generated:  \n",
      "Provided two set of images the purpose is to find that the given \n",
      "set of images are identical or not. Person Re-Id is often a \n",
      "challenging task due ...\n",
      "ROUGE-1: 0.3977\n",
      "ROUGE-2: 0.1538\n",
      "ROUGE-L: 0.2573\n",
      "BLEU: 0.0789\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 155...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 156/371 [10:36<14:29,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 155:\n",
      "Reference: The study presents a method of identifying underground objects from GPR images using a deep neural network (DNN) trained on several hundred thousand i...\n",
      "Generated:  \n",
      "underground object form a ground penetrating radar (GPR)\n",
      "image by the deep neural network. In this study, in order\n",
      "to automatically detect an underg...\n",
      "ROUGE-1: 0.6104\n",
      "ROUGE-2: 0.2500\n",
      "ROUGE-L: 0.3117\n",
      "BLEU: 0.0405\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 156...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 157/371 [10:40<14:24,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 156:\n",
      "Reference: The article proposes a novel concept for radar-based object classification using deep learning methods. The traditional radar signal processing chain ...\n",
      "Generated:   Person Re-Identification\n",
      "   Deep Learning\n",
      " :   Ground Penetrating Radar龍�   Automotive Radar 裏�   Scene UnderstandingScene understanding for automat...\n",
      "ROUGE-1: 0.3786\n",
      "ROUGE-2: 0.1078\n",
      "ROUGE-L: 0.1845\n",
      "BLEU: 0.0424\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 157...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 158/371 [10:44<14:25,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 157:\n",
      "Reference: This article proposes an intelligent video technology based on deep learning for abnormal behavior identification in massive video monitoring data in ...\n",
      "Generated:  \n",
      "technical framework of intelligent video monitoring algorithm is divided into bottom (object detection), middle (object identification) and high (be...\n",
      "ROUGE-1: 0.4747\n",
      "ROUGE-2: 0.1837\n",
      "ROUGE-L: 0.2727\n",
      "BLEU: 0.0627\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 158...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 159/371 [10:48<14:20,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 158:\n",
      "Reference: This paper describes a robotic grasping system that can automatically sort garbage based on machine vision. The system uses deep learning to accuratel...\n",
      "Generated:  \n",
      "system achieves the identification and positioning of target objects in complex background before using manipulator to \n",
      "automatically grab the sorti...\n",
      "ROUGE-1: 0.5614\n",
      "ROUGE-2: 0.1893\n",
      "ROUGE-L: 0.2807\n",
      "BLEU: 0.0646\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 159...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 160/371 [10:52<14:14,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 159:\n",
      "Reference: Facial expression recognition has many applications but is challenging for machine learning as people show expressions differently. Most facial expres...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Facial expression recognition with Convolutional Neural Networks: Coping with few data and the training sample orderfacial expression Recog...\n",
      "ROUGE-1: 0.4530\n",
      "ROUGE-2: 0.1564\n",
      "ROUGE-L: 0.2210\n",
      "BLEU: 0.0350\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 160...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 161/371 [10:56<14:08,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 160:\n",
      "Reference: This paper presents a multimodal approach for recognizing eight emotions by integrating information from facial expressions, body movement and gesture...\n",
      "Generated:  \n",
      "Emotion Recognition through Multiple Modalities:\n",
      "Face, Body Gesture, Speech Affective body language, Affective speech, Emotion recognition,\n",
      "Multimod...\n",
      "ROUGE-1: 0.5823\n",
      "ROUGE-2: 0.3077\n",
      "ROUGE-L: 0.3797\n",
      "BLEU: 0.1600\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 161...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▎     | 162/371 [11:00<14:07,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 161:\n",
      "Reference: The paper describes an experimental study on detecting emotions from speech to improve computer-human interactions. It uses a corpus of emotional spee...\n",
      "Generated:  \n",
      "the virtual characters becomes more important. This study utilizes a corpus\n",
      "containing emotional speech with 721 short utterances expressing four\n",
      "em...\n",
      "ROUGE-1: 0.5000\n",
      "ROUGE-2: 0.2360\n",
      "ROUGE-L: 0.3333\n",
      "BLEU: 0.1568\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 162...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 163/371 [11:05<14:02,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 162:\n",
      "Reference: Facial expression recognition has been a challenging area of research for decades, with traditional methods relying on hand-crafted features and class...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Deep-Emotion: Facial Expression Recognition Using Attentional Convolutional Networkconvolutional neural network; attention mechanism; spati...\n",
      "ROUGE-1: 0.4800\n",
      "ROUGE-2: 0.1794\n",
      "ROUGE-L: 0.2578\n",
      "BLEU: 0.0728\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 163...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 164/371 [11:09<14:11,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 163:\n",
      "Reference: The paper proposes a method for creating a personalized voice-emotion user interface that can detect emotions in the user's speech regardless of age, ...\n",
      "Generated:  \n",
      "Face\n",
      ",\n",
      "Engines\n",
      ", carbohydAcoustical engineeringAn approach towards a personalized voice-emotion user interface regardless of the speaker's age, sex ...\n",
      "ROUGE-1: 0.5263\n",
      "ROUGE-2: 0.2995\n",
      "ROUGE-L: 0.3349\n",
      "BLEU: 0.1559\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 164...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 165/371 [11:13<14:06,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 164:\n",
      "Reference: The ability to recognize emotions conveyed through non-verbal cues such as facial expressions and tone of voice is crucial for social interactions and...\n",
      "Generated:  \n",
      "the case for emotions conveyed by non-linguistic vocal expressions, another key aspect of social\n",
      "interactions. We tested 225 children and adolescent...\n",
      "ROUGE-1: 0.4722\n",
      "ROUGE-2: 0.3084\n",
      "ROUGE-L: 0.3889\n",
      "BLEU: 0.2282\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 165...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 166/371 [11:17<14:08,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 165:\n",
      "Reference: This paper proposes a facial emotion recognition technique that uses two new geometric features extracted from facial landmarks associated with indivi...\n",
      "Generated:  \n",
      "Sensors\n",
      ",\n",
      "Emotion recognition\n",
      ", ∼OptimizationThis paper presents a facial emotion recognition technique using two newly defined geometric features, ...\n",
      "ROUGE-1: 0.5348\n",
      "ROUGE-2: 0.3351\n",
      "ROUGE-L: 0.4706\n",
      "BLEU: 0.2241\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 166...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 167/371 [11:21<14:08,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 166:\n",
      "Reference: This paper proposes an algorithm for emotion-based line art colorization using the DenseNet network for emotional recognition of anime faces and a two...\n",
      "Generated:  \n",
      "Faces\n",
      ",\n",
      "Visualization                ,\n",
      "Rendering (computer graphics)With the development of artificial intelligence technology, it is possible to au...\n",
      "ROUGE-1: 0.6557\n",
      "ROUGE-2: 0.3978\n",
      "ROUGE-L: 0.2842\n",
      "BLEU: 0.2764\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 167...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 168/371 [11:25<13:59,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 167:\n",
      "Reference: The paper presents a study on summarizing long legal briefs with only 120 available (document, summary) pairs, which is a low-resource setting. The au...\n",
      "Generated:  \n",
      "pressing a long document into a coherent short\n",
      "document while retaining salient information.\n",
      "Modern abstractive summarization methods\n",
      "are based on d...\n",
      "ROUGE-1: 0.3874\n",
      "ROUGE-2: 0.1270\n",
      "ROUGE-L: 0.1571\n",
      "BLEU: 0.1122\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 168...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 169/371 [11:29<13:47,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 168:\n",
      "Reference: This paper proposes a scoring system for identifying main objects in complex background images using an improved RCNN network. The size of the candida...\n",
      "Generated:  \n",
      "How to identify these objects and identify the main objects \n",
      "therein and understand the relationship between the main \n",
      "objects and other objects are...\n",
      "ROUGE-1: 0.4884\n",
      "ROUGE-2: 0.1529\n",
      "ROUGE-L: 0.2326\n",
      "BLEU: 0.0367\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 169...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 170/371 [11:33<13:43,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 169:\n",
      "Reference: The Indian government's recent focus on the welfare of sanitation workers has led to a need for an automated waste management system. The existing sys...\n",
      "Generated:  \n",
      "automated system in waste management. The existing garbage \n",
      "disposal system in India consists of unclassified waste collected \n",
      "from homes which are ...\n",
      "ROUGE-1: 0.4532\n",
      "ROUGE-2: 0.1393\n",
      "ROUGE-L: 0.2660\n",
      "BLEU: 0.0598\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 170...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 171/371 [11:38<13:34,  4.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 170:\n",
      "Reference: This paper discusses the importance of object detection based on deep learning technology, which has stronger capabilities for feature learning and re...\n",
      "Generated:  \n",
      "important application in deep learning technology, which is \n",
      "characterized by its strong capability of feature learning and \n",
      "feature representation ...\n",
      "ROUGE-1: 0.5545\n",
      "ROUGE-2: 0.2385\n",
      "ROUGE-L: 0.3727\n",
      "BLEU: 0.1380\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 171...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▋     | 172/371 [11:42<13:29,  4.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 171:\n",
      "Reference: The letter discusses the limitations of naive low-latency algorithms used in commercial optical sorting systems, which can result in degraded purity o...\n",
      "Generated:  \n",
      "developed a super-high purity seed sorting system that uses a low\n",
      "latency image-recognition based on a deep neural network and\n",
      "removes the seeds of ...\n",
      "ROUGE-1: 0.5246\n",
      "ROUGE-2: 0.2893\n",
      "ROUGE-L: 0.3197\n",
      "BLEU: 0.1694\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 172...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 173/371 [11:46<13:23,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 172:\n",
      "Reference: This paper proposes using UAV-based videos and deep learning techniques for real-time traffic analysis in urban environments. The study validates the ...\n",
      "Generated:  \n",
      "The traffic analysis process is real-time in terms of the pre\n",
      "trained model used.This paper presents a preliminary study on traffic \n",
      "surveillance an...\n",
      "ROUGE-1: 0.5348\n",
      "ROUGE-2: 0.1622\n",
      "ROUGE-L: 0.2460\n",
      "BLEU: 0.0625\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 173...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 174/371 [11:50<13:24,  4.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 173:\n",
      "Reference: The author proposes a virtual space that mimics real communication environments between network users or between humans and machines, using avatars wi...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Face analysis and synthesisAvatars\n",
      ",\n",
      "Mouth ∼,\n",
      "Shape control\n",
      ", ∼Neural networks\n",
      ", 裏�Speech\n",
      ",龍�Muscles\n",
      ",*/(Network synthesis\n",
      ", carbohydHumans...\n",
      "ROUGE-1: 0.5381\n",
      "ROUGE-2: 0.3487\n",
      "ROUGE-L: 0.3350\n",
      "BLEU: 0.1363\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 174...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 175/371 [11:54<13:24,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 174:\n",
      "Reference: This thesis proposes an automatic facial expression recognition system that utilizes multistream hidden Markov models (HMMs) to improve performance by...\n",
      "Generated:  \n",
      "Human computer interaction\n",
      ",\n",
      "Performance analysis龍�,\n",
      "Eyebrows\n",
      ",龍�Facial features\n",
      ", ∼Speech recognitionThe performance of an automatic facial express...\n",
      "ROUGE-1: 0.5889\n",
      "ROUGE-2: 0.3596\n",
      "ROUGE-L: 0.3444\n",
      "BLEU: 0.3071\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 175...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 176/371 [11:58<13:39,  4.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 175:\n",
      "Reference: The study argues that the traditional view of cognitive systems as input/output devices with limited interaction between perception and action is inco...\n",
      "Generated:  \n",
      "Optimal control\n",
      ",\n",
      "NeuroscienceIn psychology and neuroscience it is common to describe cognitive systems as input/output devices where perceptual and...\n",
      "ROUGE-1: 0.4694\n",
      "ROUGE-2: 0.1443\n",
      "ROUGE-L: 0.2143\n",
      "BLEU: 0.0844\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 176...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 177/371 [12:03<13:44,  4.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 176:\n",
      "Reference: This paper presents a framework for real-time speech-driven face animation with expressions using neural networks. The framework includes facial defor...\n",
      "Generated:  \n",
      "Real-time speech-driven synthetic talking face provides an effective multimodal communication interface in distributed collaboration environments. N...\n",
      "ROUGE-1: 0.5714\n",
      "ROUGE-2: 0.3196\n",
      "ROUGE-L: 0.2347\n",
      "BLEU: 0.2207\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 177...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 178/371 [12:07<13:22,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 177:\n",
      "Reference: The paper proposes a deep learning-based approach for personality recognition from text posts on online social networks. The method uses a hierarchica...\n",
      "Generated:   Computational personality recognition is an emerging research field that consists of the automatic inference of users’ personality traits from publi...\n",
      "ROUGE-1: 0.4935\n",
      "ROUGE-2: 0.2271\n",
      "ROUGE-L: 0.3463\n",
      "BLEU: 0.1735\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 178...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 179/371 [12:11<13:12,  4.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 178:\n",
      "Reference: This paper discusses the potential of leveraging social network context for scalable face recognition systems. The authors argue that social incentive...\n",
      "Generated:  \n",
      "digital form at an accelerating rate, and our computational\n",
      "tools for searching, browsing, and sharing these photos are\n",
      "struggling to keep pace. One...\n",
      "ROUGE-1: 0.4052\n",
      "ROUGE-2: 0.1043\n",
      "ROUGE-L: 0.1724\n",
      "BLEU: 0.0414\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 179...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▊     | 180/371 [12:15<13:05,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 179:\n",
      "Reference: The paper presents an automatic approach for constructing a Chinese personality lexicon suitable for personality recognition using text-mining methods...\n",
      "Generated:  \n",
      "Human behavioral data. With the rise of social media, increasing attention has been paid to the ability\n",
      "to recognize personality traits by analyzing...\n",
      "ROUGE-1: 0.4466\n",
      "ROUGE-2: 0.1275\n",
      "ROUGE-L: 0.1942\n",
      "BLEU: 0.0558\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 180...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 181/371 [12:19<13:01,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 180:\n",
      "Reference: This survey paper reviews the state-of-the-art research in sentiment analysis, categorizing and classifying it from multiple perspectives, including t...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "A survey of sentiment analysis in social mediaSentiment analysis,\n",
      "Social media,\n",
      "Data mining,\n",
      "Machine learning,\n",
      "Survey.Sentiments or opinion...\n",
      "ROUGE-1: 0.3316\n",
      "ROUGE-2: 0.0314\n",
      "ROUGE-L: 0.2073\n",
      "BLEU: 0.0095\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 181...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 182/371 [12:23<12:58,  4.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 181:\n",
      "Reference: This study explores the use of emojis in personality recognition tasks and presents two attention-based Bi-LSTM models that incorporate both textual a...\n",
      "Generated:  \n",
      "Personality traits,\n",
      "User-generated content.Emojis have been widely used in social media as a new way to express various emotions\n",
      "and personalities. ...\n",
      "ROUGE-1: 0.4500\n",
      "ROUGE-2: 0.1616\n",
      "ROUGE-L: 0.2400\n",
      "BLEU: 0.0824\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 182...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 183/371 [12:27<13:14,  4.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 182:\n",
      "Reference: The study aims to identify personalities by analyzing self-reported content on Twitter using six ML classifiers and three feature extraction methods: ...\n",
      "Generated:  \n",
      "predict an individual’s personality through SM conversation. Four BIG5 personality items (i.e. Extraversion \n",
      "(EXT), Consciousness (CON), Agreeable (...\n",
      "ROUGE-1: 0.3600\n",
      "ROUGE-2: 0.1313\n",
      "ROUGE-L: 0.1600\n",
      "BLEU: 0.0586\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 183...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 184/371 [12:31<12:57,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 183:\n",
      "Reference: The paper introduces a new approach called Kernel Compositional Embedding (KCE) to leverage the advantages of both kernel methods and compositional em...\n",
      "Generated:  \n",
      "Compositional embedding\n",
      "Structured object classificationIn many applications such as natural language processing, speech recognition, and computer v...\n",
      "ROUGE-1: 0.4072\n",
      "ROUGE-2: 0.1279\n",
      "ROUGE-L: 0.1991\n",
      "BLEU: 0.0868\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 184...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 185/371 [12:36<12:46,  4.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 184:\n",
      "Reference: The text describes a novel approach for recognizing the Big Five personality traits of people from videos using four modalities: ambient appearance, f...\n",
      "Generated:   personality Multimodal modeling Information fusion Feature attention,Error consistencyPersonality computing and affective computing, where the recog...\n",
      "ROUGE-1: 0.5381\n",
      "ROUGE-2: 0.2359\n",
      "ROUGE-L: 0.3350\n",
      "BLEU: 0.1960\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 185...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 186/371 [12:39<12:31,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 185:\n",
      "Reference: The paper proposes a method called PbSC for sentiment classification in microblogs based on users' personality traits. The Big Five model is used to p...\n",
      "Generated:  \n",
      "Social media analytics,\n",
      "Personality prediction,\n",
      "Big Five model.Microblog has become one of the most widely used social media for people to share inf...\n",
      "ROUGE-1: 0.3192\n",
      "ROUGE-2: 0.0190\n",
      "ROUGE-L: 0.1408\n",
      "BLEU: 0.0064\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 186...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 187/371 [12:43<12:20,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 186:\n",
      "Reference: This paper proposes an Attention-based LSTM model for predicting personality traits of social network users. The model combines the users' theme prefe...\n",
      "Generated:  \n",
      "the LSTM network, which receives the sequential input of words,\n",
      "acquired the dependency of the target entity in the sentence and\n",
      "identified the diff...\n",
      "ROUGE-1: 0.4476\n",
      "ROUGE-2: 0.2270\n",
      "ROUGE-L: 0.3217\n",
      "BLEU: 0.0754\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 187...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 188/371 [12:47<12:17,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 187:\n",
      "Reference: The paper discusses the prevalence of fake profile creation on social networks and the need for effective detection methods. It surveys existing and l...\n",
      "Generated:  \n",
      "Online social networks,\n",
      "Sybil attacks,\n",
      "Big data.In the present era, online social networks are the most popular and rapid information propagation ap...\n",
      "ROUGE-1: 0.3364\n",
      "ROUGE-2: 0.0660\n",
      "ROUGE-L: 0.1589\n",
      "BLEU: 0.0174\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 188...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 189/371 [12:52<12:16,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 188:\n",
      "Reference: The paper reviews various face recognition methods, including PCA, LDA, ICA, SVM, Gabor wavelet, and soft computing tools like ANN. It investigates th...\n",
      "Generated:  \n",
      "Linear Discriminant Analysis (LDA), \n",
      "Face Recognition, Independent \n",
      "Component Analysis (ICA), Artificial \n",
      "Neural Networks (ANN).Abstract-Face recogn...\n",
      "ROUGE-1: 0.3544\n",
      "ROUGE-2: 0.1410\n",
      "ROUGE-L: 0.1772\n",
      "BLEU: 0.1240\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 189...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 190/371 [12:56<12:15,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 189:\n",
      "Reference: This article discusses the harmful effects of derogatory social media posts about famous individuals and proposes a multimodal deep learning framework...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "A Multimodal Deep Framework for Derogatory Social Media Post Identification of a Recognized Person Social media analysis and security, deep...\n",
      "ROUGE-1: 0.3529\n",
      "ROUGE-2: 0.0865\n",
      "ROUGE-L: 0.1711\n",
      "BLEU: 0.0091\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 190...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████▏    | 191/371 [13:00<12:10,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 190:\n",
      "Reference: This paper discusses the problem of identifying celebrities from social media interactions. It highlights the difference between identifying celebriti...\n",
      "Generated:  \n",
      "a characteristic assigned to persons that are initially based\n",
      "on specific achievements or lineage. However, celebritiness\n",
      "often transcends achieveme...\n",
      "ROUGE-1: 0.3960\n",
      "ROUGE-2: 0.1300\n",
      "ROUGE-L: 0.2178\n",
      "BLEU: 0.0807\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 192/371 [13:04<12:03,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 191:\n",
      "Reference: The text discusses sentiment analysis on social media data using artificial intelligence techniques. Sentiment analysis is used to understand people's...\n",
      "Generated:  :(<</SYS>>\n",
      "Sentiment Analysis in Social Media Data for Depression Detection Using Artificial Intelligence Sentiment analysis · Natural language proce...\n",
      "ROUGE-1: 0.4493\n",
      "ROUGE-2: 0.1956\n",
      "ROUGE-L: 0.2643\n",
      "BLEU: 0.0137\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 192...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 193/371 [13:08<11:57,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 192:\n",
      "Reference: This paper discusses the challenge of identifying violent content in social media, which can be categorized into aggregation in comments, cyber-bullyi...\n",
      "Generated:  \n",
      "most of the research are based on social media text or images. Although researchers have done research \n",
      "on multimodal content: caption of the images...\n",
      "ROUGE-1: 0.5430\n",
      "ROUGE-2: 0.2740\n",
      "ROUGE-L: 0.3801\n",
      "BLEU: 0.1930\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 193...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 194/371 [13:12<11:50,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 193:\n",
      "Reference: This paper reviews machine learning models used for automatic prediction of personality traits, with a focus on deep learning-based methods. The paper...\n",
      "Generated:  \n",
      "Recent trends in deep learning based personality detection.\n",
      "Personality detection · Multimodal interaction · Deep learningRecently, the automatic pr...\n",
      "ROUGE-1: 0.4925\n",
      "ROUGE-2: 0.2030\n",
      "ROUGE-L: 0.2111\n",
      "BLEU: 0.0914\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 194...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 195/371 [13:16<11:44,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 194:\n",
      "Reference: This paper proposes a method for emotion recognition and affective computing on vocal social media, which is becoming increasingly important for socia...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Emotion recognition and affective computing on vocal social media to estimate complex emotion as well as its dynamic changes in a three-dim...\n",
      "ROUGE-1: 0.4465\n",
      "ROUGE-2: 0.2629\n",
      "ROUGE-L: 0.3163\n",
      "BLEU: 0.1707\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 195...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 196/371 [13:20<11:37,  3.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 195:\n",
      "Reference: This research evaluates the accuracy of four popular facial recognition tools - Face++, IBM Bluemix Visual Recognition, AWS Rekognition, and Microsoft...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Assessing the Accuracy of Four Popular FaceRecognition Tools for Inferring Gender, Age, and Race.Face Detection,Age Detection,Race Detectio...\n",
      "ROUGE-1: 0.4758\n",
      "ROUGE-2: 0.2489\n",
      "ROUGE-L: 0.2731\n",
      "BLEU: 0.1161\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 196...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 197/371 [13:24<11:42,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 196:\n",
      "Reference: This paper discusses the concept of facial recognition technology, which is a computer application used for identifying or verifying individuals from ...\n",
      "Generated:  \n",
      "is typically used in security systems and can be compared to other biometrics such as fingerprint or eye iris recognition systems [1]. Recently \n",
      "fac...\n",
      "ROUGE-1: 0.3806\n",
      "ROUGE-2: 0.1224\n",
      "ROUGE-L: 0.2186\n",
      "BLEU: 0.0719\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 197...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 198/371 [13:28<11:41,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 197:\n",
      "Reference: This paper highlights the increasing need for automated understanding and evaluation of data due to the massive growth in video and image datasets. Fa...\n",
      "Generated:  \n",
      "Support Vector Machine, Neural Network. With the top-notch enlargement in video and image facts set, there's a mind boggling want of programmed comp...\n",
      "ROUGE-1: 0.2920\n",
      "ROUGE-2: 0.0515\n",
      "ROUGE-L: 0.1606\n",
      "BLEU: 0.0241\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 198...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▎    | 199/371 [13:32<11:37,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 198:\n",
      "Reference: This review explores facial emotion recognition (FER) in individuals with ADHD and discusses the inconsistent findings across studies. Fear facial exp...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Emotional face recognition in individuals with attention.Facial emotion recognition. This review focuses on facial emotion recognition (FER...\n",
      "ROUGE-1: 0.5000\n",
      "ROUGE-2: 0.2588\n",
      "ROUGE-L: 0.3256\n",
      "BLEU: 0.1541\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 199...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 200/371 [13:36<11:36,  4.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 199:\n",
      "Reference: This paper provides a survey of various techniques for face recognition, including local, holistic, and hybrid approaches. The authors compare the adv...\n",
      "Generated:  \n",
      "growing rapidly. Video surveillance, criminal identification, building access control, and unmanned\n",
      "and autonomous vehicles are just a few examples ...\n",
      "ROUGE-1: 0.4369\n",
      "ROUGE-2: 0.1373\n",
      "ROUGE-L: 0.2233\n",
      "BLEU: 0.1082\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 200...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 201/371 [13:40<11:29,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 200:\n",
      "Reference: This paper reviews recent advances in deep learning methods applied to playing various types of video games, including first-person shooters, arcade g...\n",
      "Generated:  \n",
      "different game genres pose to a deep learning system and highlight\n",
      "important open challenges in the context of applying these machine\n",
      "learning metho...\n",
      "ROUGE-1: 0.5894\n",
      "ROUGE-2: 0.3610\n",
      "ROUGE-L: 0.3768\n",
      "BLEU: 0.1783\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 201...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 202/371 [13:44<11:25,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 201:\n",
      "Reference: This paper discusses how computer animated agents and robots enhance human-computer interaction and introduce a social dimension to daily life. It hig...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Real Time Face Detection and Facial Expression Recognition: Development and Applications to Human Computer Interaction.Face, Real time syst...\n",
      "ROUGE-1: 0.4405\n",
      "ROUGE-2: 0.1422\n",
      "ROUGE-L: 0.1938\n",
      "BLEU: 0.0592\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 202...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 203/371 [13:48<11:23,  4.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 202:\n",
      "Reference: This paper outlines the development of an automatic emotion recognition system that combines various modalities including facial features, prosody, an...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Emotion recognition in human–computer interactionEmotions Emotion classification Attention control Sigma–pi neural networks Feedback learni...\n",
      "ROUGE-1: 0.4530\n",
      "ROUGE-2: 0.1810\n",
      "ROUGE-L: 0.2479\n",
      "BLEU: 0.0918\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 203...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 204/371 [13:52<11:21,  4.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 203:\n",
      "Reference: The paper proposes a novel emotion recognition model using the system identification approach, which utilizes an extended Kohonen self-organizing map ...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Emotion recognition from geometric facial features using self-organizing mapFacial expression analysisGeometric facial featuresSelf-organiz...\n",
      "ROUGE-1: 0.4293\n",
      "ROUGE-2: 0.2434\n",
      "ROUGE-L: 0.3455\n",
      "BLEU: 0.1925\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 204...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 205/371 [13:56<11:22,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 204:\n",
      "Reference: The study compared the processing of cartoon faces and real faces in 17 university students using event-related potentials (ERPs). The study found tha...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "An event-related potential comparison of facial expression processing between cartoon and real facesFacial expression recognition, event- r...\n",
      "ROUGE-1: 0.4503\n",
      "ROUGE-2: 0.2116\n",
      "ROUGE-L: 0.2304\n",
      "BLEU: 0.1084\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 205...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 206/371 [14:00<11:15,  4.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 205:\n",
      "Reference: The study aimed to investigate whether deficits in emotion recognition, understanding of other people's intentions (\"theory of mind\"), and cognitive f...\n",
      "Generated:  \n",
      "Social behavior\n",
      "Emotion recognition\n",
      "Theory of mind\n",
      "Cognitive flexibility\n",
      "Follow-upAlthough the adverse consequences of changes in social behavior fo...\n",
      "ROUGE-1: 0.5400\n",
      "ROUGE-2: 0.3131\n",
      "ROUGE-L: 0.2800\n",
      "BLEU: 0.2498\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 206...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 207/371 [14:05<11:18,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 206:\n",
      "Reference: This article discusses the social differences among individuals with Autism Spectrum Disorder (ASD), particularly in recognizing and producing facial ...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Review: Posed vs. Genuine Facial Emotion Recognition and Expression in Autism and Implications for InterventionAutism spectrum disorder, fa...\n",
      "ROUGE-1: 0.5464\n",
      "ROUGE-2: 0.2604\n",
      "ROUGE-L: 0.2990\n",
      "BLEU: 0.1746\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 207...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 208/371 [14:09<11:13,  4.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 207:\n",
      "Reference: This study focused on facial affect recognition in children aged 7-13 with High Functioning Autism (HFA), Social Phobia (SP), and typical development ...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Facial Emotion Recognition in Children with High Functioning Autism and Children with Social Phobiafacial affect recognition, High function...\n",
      "ROUGE-1: 0.5363\n",
      "ROUGE-2: 0.3503\n",
      "ROUGE-L: 0.2682\n",
      "BLEU: 0.1362\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 208...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 209/371 [14:13<11:06,  4.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 208:\n",
      "Reference: This article discusses how sentiment analysis techniques are used to extract emotions and opinions from texts, and how previous research has focused m...\n",
      "Generated:  \n",
      "an essential information source related to sentiment/opinion.\n",
      "The purpose of Sentiment Analysis (SA) techniques are used to\n",
      "extract sentiments, emot...\n",
      "ROUGE-1: 0.4249\n",
      "ROUGE-2: 0.1675\n",
      "ROUGE-L: 0.3109\n",
      "BLEU: 0.1121\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 209...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 210/371 [14:17<11:00,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 209:\n",
      "Reference: This study aimed to investigate the accuracy of facial emotion recognition in Chinese patients with schizophrenia compared to healthy controls using a...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Impairments in Negative Facial Emotion Recognition in Chinese Schizophrenia Patients Detected With a Newly Designed TaskSchizophrenia , Chi...\n",
      "ROUGE-1: 0.4783\n",
      "ROUGE-2: 0.1538\n",
      "ROUGE-L: 0.2609\n",
      "BLEU: 0.0978\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 210...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 211/371 [14:21<10:57,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 210:\n",
      "Reference: The cartoon animation industry has potential in various markets, but animators struggle to find relevant materials due to limited classification of ca...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Deep learning-based classification of the polar emotions of \"moe\"-style cartoon picturesFeature extraction, Face recognition, Animation, Im...\n",
      "ROUGE-1: 0.6057\n",
      "ROUGE-2: 0.2890\n",
      "ROUGE-L: 0.3086\n",
      "BLEU: 0.1581\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 211...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 212/371 [14:25<11:04,  4.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 211:\n",
      "Reference: Children on the autism spectrum (AS) show comparable emotional recognition (ER) skills to typically developing (TD) children when using music, despite...\n",
      "Generated: [INST]\n",
      "<<SYS>>\n",
      "Analyze the research articles content and get me a summary from the research article. The summary length may be within 150 words\n",
      "<</SYS...\n",
      "ROUGE-1: 0.4590\n",
      "ROUGE-2: 0.1768\n",
      "ROUGE-L: 0.2077\n",
      "BLEU: 0.1238\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 212...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 213/371 [14:30<10:59,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 212:\n",
      "Reference: This study focused on the importance of avatar realism in collaborative virtual environments, using face tracking technology to render facial expressi...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "The Effect of Behavioral Realism and Form Realism of Real-Time Avatar Faces on Verbal Disclosure, Nonverbal Disclosure, Emotion Recognition...\n",
      "ROUGE-1: 0.4198\n",
      "ROUGE-2: 0.1625\n",
      "ROUGE-L: 0.1975\n",
      "BLEU: 0.0571\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 213...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 214/371 [14:34<10:58,  4.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 213:\n",
      "Reference: Recent studies have found that deficits in recognizing facial expressions of fear, disgust, and anger are associated with deficits in producing those ...\n",
      "Generated:  \n",
      "deficits in face-based recognition are paired with deficits in the production of the same emotion.\n",
      "What type of mindreading process would explain th...\n",
      "ROUGE-1: 0.5314\n",
      "ROUGE-2: 0.2927\n",
      "ROUGE-L: 0.4155\n",
      "BLEU: 0.2288\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 214...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 215/371 [14:38<10:47,  4.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 214:\n",
      "Reference: The report presents evidence supporting the idea that facial identity recognition and facial expression recognition are performed by separate mechanis...\n",
      "Generated:  \n",
      "the independence of these processes by documenting an individual with severely impaired recogni\n",
      "tion of facial identity yet normal recognition of fa...\n",
      "ROUGE-1: 0.4790\n",
      "ROUGE-2: 0.1818\n",
      "ROUGE-L: 0.2754\n",
      "BLEU: 0.0919\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 215...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 216/371 [14:42<10:36,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 215:\n",
      "Reference: Facial emotion recognition deficits are common in schizophrenia but factors associated with impairment at each stage of the disease are still unclear....\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Facial Emotion Recognition in SchizophreniaFacial emotion recognition, schizophrenia, Bruce-Young model, behavioral studies, event-related ...\n",
      "ROUGE-1: 0.5843\n",
      "ROUGE-2: 0.1932\n",
      "ROUGE-L: 0.2472\n",
      "BLEU: 0.1222\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 216...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 217/371 [14:46<10:39,  4.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 216:\n",
      "Reference: This study assessed the effectiveness of The Transporters, an animated series designed to enhance emotion comprehension in children with autism spectr...\n",
      "Generated:   After checking that all measures were normally distrib\n",
      "uted,2 we looked at the performance of the three groups on\n",
      "the tasks at Time 1. Using four-on...\n",
      "ROUGE-1: 0.3459\n",
      "ROUGE-2: 0.1202\n",
      "ROUGE-L: 0.1730\n",
      "BLEU: 0.0950\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 217...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 218/371 [14:50<10:28,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 217:\n",
      "Reference: Research investigated whether the age-related decline in emotion recognition is due to a tendency to overlook emotion information in the eyes. The stu...\n",
      "Generated:  \n",
      "Experiment 1, younger adults were significantly better than older adults at inferring emotions from full faces and\n",
      "eyes, though not from mouths. Usi...\n",
      "ROUGE-1: 0.6452\n",
      "ROUGE-2: 0.4130\n",
      "ROUGE-L: 0.4731\n",
      "BLEU: 0.3179\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 218...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 219/371 [14:54<10:24,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 218:\n",
      "Reference: This research article discusses the use of Machine Learning (ML) methods in healthcare to predict and classify heart diseases and locomotor disorders....\n",
      "Generated:   In this research, we are using an online UCI dataset with 303 rows and 76 properties. Approximately 14 of these 76 properties are selected for testi...\n",
      "ROUGE-1: 0.5198\n",
      "ROUGE-2: 0.2844\n",
      "ROUGE-L: 0.3877\n",
      "BLEU: 0.2337\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 219...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 220/371 [14:58<10:20,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 219:\n",
      "Reference: Lung cancer is a major cause of cancer-related deaths, often due to late diagnosis. Pulmonary nodules are the initial presentation in most lung cancer...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Putting artificial intelligence (AI) on the spot: machine learning evaluation of pulmonary nodules.Pulmonary nodule, Lung cancer, Segmentat...\n",
      "ROUGE-1: 0.4878\n",
      "ROUGE-2: 0.1773\n",
      "ROUGE-L: 0.2146\n",
      "BLEU: 0.0911\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 220...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 221/371 [15:02<10:11,  4.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 220:\n",
      "Reference: Despite extensive research efforts, predicting cardiovascular disease risks based on health records has remained unsatisfactory. An ensemble method, X...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Accurate Prediction of Coronary Heart Disease for Patients With Hypertension From Electronic Health Records With Big Data and Machine-Learn...\n",
      "ROUGE-1: 0.4510\n",
      "ROUGE-2: 0.1485\n",
      "ROUGE-L: 0.1961\n",
      "BLEU: 0.1074\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 221...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 222/371 [15:07<10:11,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 221:\n",
      "Reference: The study proposes a deep learning model using non-invasive CT images to predict EGFR mutation status in lung adenocarcinoma patients. The model was t...\n",
      "Generated:  \n",
      "such as the use of tyrosine kinase inhibitors in lung adenocarcinoma. Conventional identification of EGFR\n",
      "genotype requires biopsy and sequence test...\n",
      "ROUGE-1: 0.3704\n",
      "ROUGE-2: 0.1577\n",
      "ROUGE-L: 0.1893\n",
      "BLEU: 0.0619\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 222...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 223/371 [15:11<10:03,  4.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 222:\n",
      "Reference: The paper introduces a new blind source separation (BSS) method for separating temporal correlated noncircular sources using a widely linear filter (W...\n",
      "Generated:  \n",
      "Blind separation of temporally correlated noncircularular sources using\n",
      "complex matrix joint diagonalization.Blind source separation\n",
      "Widely linear f...\n",
      "ROUGE-1: 0.5730\n",
      "ROUGE-2: 0.3068\n",
      "ROUGE-L: 0.3371\n",
      "BLEU: 0.2210\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 223...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 224/371 [15:15<10:00,  4.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 223:\n",
      "Reference: This paper proposes a unified framework for pupil detection using shape augmented cascade regression models learned from adversarial synthetic images....\n",
      "Generated:  \n",
      "Cascade learning from adversarial synthetic images for accurate pupil\n",
      "detection.Cascade regression\n",
      "GANs\n",
      "Pupil detectionmage-based pupil detection, w...\n",
      "ROUGE-1: 0.3894\n",
      "ROUGE-2: 0.1429\n",
      "ROUGE-L: 0.2301\n",
      "BLEU: 0.0490\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 224...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 225/371 [15:19<09:54,  4.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 224:\n",
      "Reference: This study compared the hazard identification performance and search patterns of experienced and novice workers in the construction industry using eye...\n",
      "Generated:  \n",
      "commencing construction is widely employed to prevent accidents, it typically fails because of insuffi\n",
      "cient safety experience. The experience helps...\n",
      "ROUGE-1: 0.4327\n",
      "ROUGE-2: 0.1748\n",
      "ROUGE-L: 0.2308\n",
      "BLEU: 0.0834\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 225...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 226/371 [15:23<09:46,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 225:\n",
      "Reference: The study compared machine learning approaches with logistic regression analysis to predict acute kidney injury (AKI) after cardiac surgery. They retr...\n",
      "Generated:  \n",
      "Cardiac Surgery\n",
      ". acute kidney injury; cardiovascular surgery.\n",
      ".Machine learning approaches were introduced for better or comparable predictive abil...\n",
      "ROUGE-1: 0.4203\n",
      "ROUGE-2: 0.2336\n",
      "ROUGE-L: 0.2681\n",
      "BLEU: 0.1003\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 226...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 227/371 [15:27<09:43,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 226:\n",
      "Reference: This study used Electronic Medical Record (EMR) data from primary care clinics in seven provinces across Canada to develop predictive models to identi...\n",
      "Generated:  \n",
      "the first contact an individual will have with the healthcare system providing acute care, chronic disease \n",
      "management, and services aimed at health...\n",
      "ROUGE-1: 0.4500\n",
      "ROUGE-2: 0.2374\n",
      "ROUGE-L: 0.2643\n",
      "BLEU: 0.1295\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 227...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 228/371 [15:31<09:40,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 227:\n",
      "Reference: Healthcare data analysis has become a promising research area due to the various types of data available, such as clinical, omics, and sensor data. To...\n",
      "Generated:  \n",
      "ongoing treatment. Omics data is one of the high dimensional data comprising genome, transcriptome and proteome data types. Sensor data \n",
      "is collecte...\n",
      "ROUGE-1: 0.6222\n",
      "ROUGE-2: 0.2870\n",
      "ROUGE-L: 0.4000\n",
      "BLEU: 0.1434\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 228...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 229/371 [15:35<09:36,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 228:\n",
      "Reference: This survey paper provides a comprehensive overview of machine learning techniques used for various medical applications. The paper identifies a shift...\n",
      "Generated:  \n",
      "use this data to benefit the medical and health care sectors all across the world. This survey paper\n",
      "presents a systematic literature review for the...\n",
      "ROUGE-1: 0.5536\n",
      "ROUGE-2: 0.2162\n",
      "ROUGE-L: 0.2589\n",
      "BLEU: 0.1151\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 229...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 230/371 [15:39<09:31,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 229:\n",
      "Reference: This paper proposes an evolutionary framework for rule-based classifier induction to address supervised learning problems, particularly in medical dat...\n",
      "Generated:  \n",
      "An evolutionary framework for machine learning applied to medical\n",
      "data.Logical rule induction, Data mining, Supervised learning, Evolutionary comput...\n",
      "ROUGE-1: 0.4121\n",
      "ROUGE-2: 0.1320\n",
      "ROUGE-L: 0.2010\n",
      "BLEU: 0.0509\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 230...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 231/371 [15:43<09:25,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 230:\n",
      "Reference: \n",
      "This study aimed to predict cardiac surgery-associated acute kidney injury (CSA-AKI) using artificial intelligence-based machine learning. The study ...\n",
      "Generated:  \n",
      "increased morbidity and mortality after cardiac surgery. Most established prediction models are limited to the\n",
      "analysis of nonlinear relationships a...\n",
      "ROUGE-1: 0.3731\n",
      "ROUGE-2: 0.1353\n",
      "ROUGE-L: 0.1493\n",
      "BLEU: 0.0286\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 231...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 232/371 [15:47<09:22,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 231:\n",
      "Reference: The text describes a new computer-aided diagnosis (CAD) system called Collaborative CAD (C-CAD) that uses eye-tracking technology and a deep learning ...\n",
      "Generated:  \n",
      "A collaborative computer aided diagnosis (C-CAD) system with\n",
      "eye-tracking, sparse attentional model, and deep learning.Graph sparsification, Eye-tra...\n",
      "ROUGE-1: 0.3797\n",
      "ROUGE-2: 0.1106\n",
      "ROUGE-L: 0.2025\n",
      "BLEU: 0.0158\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 232...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 233/371 [15:51<09:10,  3.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 232:\n",
      "Reference: The article discusses the limitations of existing healthcare learning models and the difficulty in incorporating expert knowledge from heterogeneous m...\n",
      "Generated:  \n",
      "Explainable deep learning based medical diagnostic system\n",
      ".medical diagnosis, Heterogeneous representation, Knowledge extraction, Query processing.\n",
      "...\n",
      "ROUGE-1: 0.3878\n",
      "ROUGE-2: 0.1031\n",
      "ROUGE-L: 0.1531\n",
      "BLEU: 0.0517\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 233...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 234/371 [15:55<09:08,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 233:\n",
      "Reference: The text discusses the importance of social media sentiments for providing up-to-date and inclusive information, and the lack of a systematic arrangem...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "A survey of sentiment analysis in social mediaSentiment analysis, Social media, Data mining, Survey.Sentiments or opinions from social medi...\n",
      "ROUGE-1: 0.4300\n",
      "ROUGE-2: 0.1414\n",
      "ROUGE-L: 0.2500\n",
      "BLEU: 0.1025\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 234...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 235/371 [15:59<09:04,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 234:\n",
      "Reference: The article discusses a learning-based approach for semantic indexing of multimedia content using cues from audio, visual, and text features. Statisti...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Semantic Indexing of Multimedia Content Using Visual, Audio, and Text Cuessentiment indexing, query by keywords, multimodal information fus...\n",
      "ROUGE-1: 0.4468\n",
      "ROUGE-2: 0.2366\n",
      "ROUGE-L: 0.3191\n",
      "BLEU: 0.1838\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 235...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▎   | 236/371 [16:03<09:04,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 235:\n",
      "Reference: The paper proposes a method for aspect-based sentiment analysis using a cascaded framework of feature selection and classifier ensemble using particle...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Feature selection and ensemble construction: A two-step method for aspect based sentiment analysisSentiment analysis Aspect term extraction...\n",
      "ROUGE-1: 0.5327\n",
      "ROUGE-2: 0.3046\n",
      "ROUGE-L: 0.3116\n",
      "BLEU: 0.1857\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 236...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 237/371 [16:07<09:08,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 236:\n",
      "Reference: The article presents a hybrid approach to Sentiment Analysis using NLP techniques, a sentiment lexicon enhanced with SentiWordNet, and fuzzy sets to e...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "A hybrid approach to the sentiment analysis problem at the sentence levelSentiment analysis Semantic rules Fuzzy sets Unsupervised machine ...\n",
      "ROUGE-1: 0.5204\n",
      "ROUGE-2: 0.2474\n",
      "ROUGE-L: 0.3163\n",
      "BLEU: 0.1792\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 237...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 238/371 [16:11<09:02,  4.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 237:\n",
      "Reference: The text discusses how deep learning techniques have become popular for sentiment analysis, providing better performance than traditional feature-base...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "\n",
      "Enhancing deep learning sentiment analysis with ensemble techniques in social applicationsEnsemble, Sentiment analysis.Deep learning techn...\n",
      "ROUGE-1: 0.3981\n",
      "ROUGE-2: 0.1244\n",
      "ROUGE-L: 0.2370\n",
      "BLEU: 0.0451\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 238...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 239/371 [16:15<08:58,  4.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 238:\n",
      "Reference: The text discusses the challenge of automatic sentiment classification in different domains and the impracticality of annotating corpora for every pos...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classificationsentiment classification, structural corresp...\n",
      "ROUGE-1: 0.4000\n",
      "ROUGE-2: 0.1765\n",
      "ROUGE-L: 0.2417\n",
      "BLEU: 0.1022\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 239...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 240/371 [16:19<08:52,  4.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 239:\n",
      "Reference: The study conducted four experiments to examine the effectiveness of a self-administered computer-based training for emotion recognition ability (ERA)...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Effectiveness of a short audiovisual emotion recognition training program in adultsemotion recognition ability, audiovasual training, non-c...\n",
      "ROUGE-1: 0.4581\n",
      "ROUGE-2: 0.1582\n",
      "ROUGE-L: 0.2235\n",
      "BLEU: 0.0332\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 240...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 241/371 [16:23<08:45,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 240:\n",
      "Reference: The article discusses the development of a convolutional neural network (CNN) for real-time facial emotion recognition, which was achieved using trans...\n",
      "Generated:  \n",
      "classifying human emotions from dynamic facial expressions in real time. We use transfer learning on the fullyconnected layers of an existing convol...\n",
      "ROUGE-1: 0.5381\n",
      "ROUGE-2: 0.2805\n",
      "ROUGE-L: 0.3139\n",
      "BLEU: 0.1966\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 241...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 242/371 [16:27<08:42,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 241:\n",
      "Reference: The article discusses a study that aimed to create a set of highly realistic virtual faces for use in virtual reality (VR) cyberinterventions to train...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Creation of a new set of dynamic virtual reality faces for the assessment and training of facial emotion recognition abilityVirtual reality...\n",
      "ROUGE-1: 0.4558\n",
      "ROUGE-2: 0.0939\n",
      "ROUGE-L: 0.2233\n",
      "BLEU: 0.0259\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 242...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 243/371 [16:32<08:38,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 242:\n",
      "Reference: The study aimed to investigate whether improved emotion recognition of novel faces is associated with concomitant changes in visual scanning of these ...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Effects of facial emotion recognition remediation on visual scanning of novel face stimuliemotion recognition, schizophrenia, visual scanni...\n",
      "ROUGE-1: 0.5619\n",
      "ROUGE-2: 0.2596\n",
      "ROUGE-L: 0.2667\n",
      "BLEU: 0.1983\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 243...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 244/371 [16:36<08:39,  4.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 243:\n",
      "Reference: This paper discusses the increasing interest in emotional computing and focuses on different learning methods for facial emotion recognition, includin...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Facial emotion recognitionEmotional computing, facial emotion recognition, learning methods, SVM, DBM, feature-level fusion, model-side fus...\n",
      "ROUGE-1: 0.5497\n",
      "ROUGE-2: 0.2722\n",
      "ROUGE-L: 0.3392\n",
      "BLEU: 0.1155\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 244...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 245/371 [16:40<08:35,  4.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 244:\n",
      "Reference: This paper discusses the challenges of human facial emotion recognition (FER) and proposes a very Deep CNN (DCNN) modeling approach through Transfer L...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Facial Emotion Recognition Using Transfer Learning in the Deep CNNconvolutional neural network (CNN); facial emotion recognition; transfer ...\n",
      "ROUGE-1: 0.2936\n",
      "ROUGE-2: 0.0463\n",
      "ROUGE-L: 0.1651\n",
      "BLEU: 0.0175\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 245...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▋   | 246/371 [16:44<08:39,  4.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 245:\n",
      "Reference: This paper describes a facial emotion recognition model that uses Convolutional Neural Networks (CNN) and Deep Neural Networks (DNN) to classify facia...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Modified Convolutional Neural Network Architecture Analysis for Facial Emotion RecognitionTraining, convolutional neural networks, Testing,...\n",
      "ROUGE-1: 0.4641\n",
      "ROUGE-2: 0.2123\n",
      "ROUGE-L: 0.2320\n",
      "BLEU: 0.0962\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 246...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 247/371 [16:48<08:31,  4.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 246:\n",
      "Reference: The paper discusses the importance of facial expression recognition in computer vision and artificial intelligence. The paper uses the Cohn-Kanade Dat...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Facial emotion recognition in real-time and static imagesFace, Feature extraction, Support vector machines, Training, Real-time systems, We...\n",
      "ROUGE-1: 0.3980\n",
      "ROUGE-2: 0.1546\n",
      "ROUGE-L: 0.2143\n",
      "BLEU: 0.0387\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 247...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 248/371 [16:52<08:24,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 247:\n",
      "Reference: This paper proposes a multi-channel deep neural network called MDSTFN for recognizing facial expressions in static images by extracting and fusing spa...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Deep spatial-temporal feature fusion for facial expression recognition in static imagesFacial expression recognitionDeep neural networkOpti...\n",
      "ROUGE-1: 0.5049\n",
      "ROUGE-2: 0.2549\n",
      "ROUGE-L: 0.3204\n",
      "BLEU: 0.1805\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 248...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 249/371 [16:56<08:21,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 248:\n",
      "Reference: The paper discusses the importance of facial expression recognition (FER) in various domains and introduces an FER system that uses Histogram of Orien...\n",
      "Generated:  \n",
      ",\n",
      "Face\n",
      ", ∼Shape\n",
      ",                Face recognition\n",
      "Training\n",
      ",*/(Emotion recognitionAutomatic recognition and interpretation of human emotions are bec...\n",
      "ROUGE-1: 0.3745\n",
      "ROUGE-2: 0.1205\n",
      "ROUGE-L: 0.1992\n",
      "BLEU: 0.0555\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 249...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 250/371 [17:00<08:15,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 249:\n",
      "Reference: This article discusses the use of deep neural networks for facial expression recognition (FER) under challenging in-the-wild conditions. Deep FER syst...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Deep Facial Expression Recognition: A SurveyDatabases*/(,\n",
      "Face recognition\n",
      ",\n",
      "Three-dimensional displays\n",
      ", ∼Lighting\n",
      ",*/(Training dataWith t...\n",
      "ROUGE-1: 0.3913\n",
      "ROUGE-2: 0.1930\n",
      "ROUGE-L: 0.2174\n",
      "BLEU: 0.0911\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 250...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 251/371 [17:05<08:13,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 250:\n",
      "Reference: The article discusses the influence of social media on consumers' attitudes and behaviors, and how monitoring social media activities can help measure...\n",
      "Generated:  ~~ Pooling ReLu Convolution Flatten 4 ea = aie ReLu Y Fully Connected Paper — OUTPUT[/INST] It maps Sentiment Analysis on Social Media with observati...\n",
      "ROUGE-1: 0.4785\n",
      "ROUGE-2: 0.2995\n",
      "ROUGE-L: 0.2775\n",
      "BLEU: 0.1874\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 251...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 252/371 [17:09<08:08,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 251:\n",
      "Reference: Sentiment analysis is a technique used to identify and extract subjective information such as opinions expressed in text. It is used to classify attit...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "An Overview of Sentiment Analysis in Social Media and its Applications in Disaster ReliefSentiment Analysis, Disaster Relief, Visualization...\n",
      "ROUGE-1: 0.4559\n",
      "ROUGE-2: 0.2000\n",
      "ROUGE-L: 0.2426\n",
      "BLEU: 0.0562\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 252...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 253/371 [17:13<08:04,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 252:\n",
      "Reference: The article discusses using Natural Language Processing and Sentiment Classification with a Recurrent Neural Network to analyze the sentiments and man...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Social media sentiment analysis based on COVID-19natural language processing; recurrent neural network; sentiment analysis; social media; v...\n",
      "ROUGE-1: 0.3441\n",
      "ROUGE-2: 0.0870\n",
      "ROUGE-L: 0.1935\n",
      "BLEU: 0.0341\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 253...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 254/371 [17:17<07:58,  4.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 253:\n",
      "Reference: This paper provides an overview of NodeXL, a tool for collecting, analyzing, visualizing, and reporting on social media connections. It highlights the...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Social Media Analytics: Analysis and Visualisation of News Diffusion using NodeXLSocial Media, Social Network Analysis, Twitter, Informatio...\n",
      "ROUGE-1: 0.4717\n",
      "ROUGE-2: 0.1429\n",
      "ROUGE-L: 0.2075\n",
      "BLEU: 0.1005\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 254...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▊   | 255/371 [17:21<07:52,  4.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 254:\n",
      "Reference: The text discusses sentiment analysis in the context of social media and proposes a method for constructing a vocabulary related to suicide to better ...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Machine Learning and Semantic Sentiment Analysis based Algorithms for Suicide Sentiment Prediction in Social Networks: Sentiments Analysis;...\n",
      "ROUGE-1: 0.3541\n",
      "ROUGE-2: 0.0676\n",
      "ROUGE-L: 0.1818\n",
      "BLEU: 0.0082\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 255...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 256/371 [17:25<07:49,  4.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 255:\n",
      "Reference: This paper evaluates and compares two approaches to automated sentiment analysis, lexicon-based and machine learning, and explores the benefits of com...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Social media sentiment analysis: lexicon versus machine learningSentiment analysis, Social media, Consumer-generated contentPurpose – With ...\n",
      "ROUGE-1: 0.4583\n",
      "ROUGE-2: 0.1579\n",
      "ROUGE-L: 0.1979\n",
      "BLEU: 0.0659\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 256...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 257/371 [17:29<07:45,  4.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 256:\n",
      "Reference: The article is about conducting research on machine learning methods for sentiment analysis of Czech social media. The authors created a large dataset...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Sentiment Analysis in Czech Social Media Using Supervised Machine Learningsentiment analysis, czech social mediaThis article provides an in...\n",
      "ROUGE-1: 0.4061\n",
      "ROUGE-2: 0.1641\n",
      "ROUGE-L: 0.2538\n",
      "BLEU: 0.1380\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 257...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 258/371 [17:33<07:38,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 257:\n",
      "Reference: The text discusses a review of 24 studies on multilingual sentiment analysis of social media in 23 different languages from 2017 to 2020. The review s...\n",
      "Generated:   The summary length may be within 150 words\n",
      "<</SYS>>\n",
      "Deep learning and multilingual sentiment analysis on social media data: An overviewSentiment ana...\n",
      "ROUGE-1: 0.3842\n",
      "ROUGE-2: 0.1194\n",
      "ROUGE-L: 0.1970\n",
      "BLEU: 0.0824\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 258...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 259/371 [17:37<07:36,  4.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 258:\n",
      "Reference: The text discusses the increasing use of social media for sharing and gathering information, and the process of data mining to analyze and categorize ...\n",
      "Generated:   180 100 approaches are combined  ML=Lexicon and MAL-based 239 <SYS>>\n",
      "A Probabilistic Generative Model for Mining Cybercriminal Network from Online S...\n",
      "ROUGE-1: 0.3922\n",
      "ROUGE-2: 0.1089\n",
      "ROUGE-L: 0.1961\n",
      "BLEU: 0.0243\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 259...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 260/371 [17:41<07:30,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 259:\n",
      "Reference: The text discusses computational intelligence (CI) and its focus on using brain- or nature-inspired models to solve complex real-world problems. Emula...\n",
      "Generated:   The summary length may be within 150 words\n",
      "<</SYS>>\n",
      "Sentiment Data Flow Analysis by Means of Dynamic Linguistic Patternscomputational intelligence, ...\n",
      "ROUGE-1: 0.3333\n",
      "ROUGE-2: 0.0737\n",
      "ROUGE-L: 0.2083\n",
      "BLEU: 0.0251\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 260...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 261/371 [17:43<06:19,  3.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 260:\n",
      "Reference: This article discusses the issue of motor vehicle traffic crashes (MVTCs) as the leading cause of work-related fatal injuries in the United States. Th...\n",
      "Generated:   The summary length may be within 150 words\n",
      "<</SYS>>\n",
      "Work-related fatal motor vehicle traffic crashes: Matching of 2010 data from the Census of Fatal...\n",
      "ROUGE-1: 0.2116\n",
      "ROUGE-2: 0.0756\n",
      "ROUGE-L: 0.1570\n",
      "BLEU: 0.0014\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 261...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 262/371 [17:47<06:34,  3.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 261:\n",
      "Reference: The concept of matched filter is commonly used in retina vessel segmentation, but single scale filters have limited performance. Multiscale matched fi...\n",
      "Generated:   The summary length may be within 150 words\n",
      "<</SYS>>\n",
      "Improved multiscale matched filter for retina vessel segmentation using PSO algorithmMultiscales...\n",
      "ROUGE-1: 0.5508\n",
      "ROUGE-2: 0.2564\n",
      "ROUGE-L: 0.2881\n",
      "BLEU: 0.1223\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 262...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 263/371 [17:51<06:43,  3.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 262:\n",
      "Reference: The study aimed to describe the clinical characteristics and dynamic changes of chest CT features in the first three weeks of COVID-19 pneumonia patie...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Clinical characteristics and changes of chest CT features in 307 patients with common COVID-19 pneumonia infected SARS-CoV -2: A multicente...\n",
      "ROUGE-1: 0.4887\n",
      "ROUGE-2: 0.3014\n",
      "ROUGE-L: 0.2805\n",
      "BLEU: 0.1302\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 263...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 264/371 [17:55<06:49,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 263:\n",
      "Reference: The paper proposes a method for automatically recovering 3D rib motion from 2D x-ray videos, while keeping the radiation dose low. The method involves...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Recovery of 3D rib motion from dynamic chest radiography and CT data using local contrast normalization and articular motion model2D–3D reg...\n",
      "ROUGE-1: 0.4124\n",
      "ROUGE-2: 0.1562\n",
      "ROUGE-L: 0.2165\n",
      "BLEU: 0.0665\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 264...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 265/371 [17:59<06:52,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 264:\n",
      "Reference: The paper proposes a retinal vessel segmentation method based on the dense conditional random field (CRF) model. The proposed method learns discrimina...\n",
      "Generated:   The summary length may be within 150 words\n",
      "<</SYS>>\n",
      "Improving dense conditional random field for retinal vessel segmentation by discriminative featu...\n",
      "ROUGE-1: 0.3687\n",
      "ROUGE-2: 0.1243\n",
      "ROUGE-L: 0.2011\n",
      "BLEU: 0.0828\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 265...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 266/371 [18:03<06:51,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 265:\n",
      "Reference: This paper proposes a new method for classifying lung nodules using x-ray screening. The method involves localizing and extracting the nodules using a...\n",
      "Generated:   The summary length may be within 150 words\n",
      "<</SYS>>\n",
      "Small lung nodules detection based on local variance analysis and probabilistic neural networkCh...\n",
      "ROUGE-1: 0.3061\n",
      "ROUGE-2: 0.0722\n",
      "ROUGE-L: 0.1837\n",
      "BLEU: 0.0104\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 266...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 267/371 [18:07<06:54,  3.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 266:\n",
      "Reference: The objective of this study is to develop and validate an analytical model incorporating actual cranial thickness and curvature for child aged 0–1YO a...\n",
      "Generated:   The summary length may be within 150 words\n",
      "<</SYS>>\n",
      "Development of a child head analytical dynamic model considering cranial nonuniform thickness an...\n",
      "ROUGE-1: 0.4758\n",
      "ROUGE-2: 0.2044\n",
      "ROUGE-L: 0.2467\n",
      "BLEU: 0.1158\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 267...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 268/371 [18:11<06:51,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 267:\n",
      "Reference: This study proposes an effective deep learning method using a fully convolutional DenseNet (FC-DenseNet) for automatically delineating posterior ribs,...\n",
      "Generated:   The summary length may be within 150 words\n",
      "<</SYS>>\n",
      "Automatic delineation of ribs and clavicles in chest radiographs using fully convolutional Dense...\n",
      "ROUGE-1: 0.5930\n",
      "ROUGE-2: 0.4235\n",
      "ROUGE-L: 0.4302\n",
      "BLEU: 0.3061\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 268...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 269/371 [18:15<06:48,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 268:\n",
      "Reference: This study reviews and compares retinal vessel segmentation methods using publicly available databases of color fundus photographs containing ground t...\n",
      "Generated:  [Tipit Laver Output Pupil Sumation Layer y68] The summary length may be within 150 words\n",
      "<</SYS>>\n",
      "Performance comparison of publicly available retina...\n",
      "ROUGE-1: 0.3472\n",
      "ROUGE-2: 0.1189\n",
      "ROUGE-L: 0.1458\n",
      "BLEU: 0.0210\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 269...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 270/371 [18:19<06:44,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 269:\n",
      "Reference: This study evaluates the performance of newly constructed descriptive statistical features in retinal vessel segmentation for the early detection of d...\n",
      "Generated:   Based on the experimental results, it is validated that the proposed statistical information hold valuable to hold 150 words\n",
      "<</SYS>>\n",
      "Performance an...\n",
      "ROUGE-1: 0.4623\n",
      "ROUGE-2: 0.1714\n",
      "ROUGE-L: 0.2170\n",
      "BLEU: 0.0855\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 270...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 271/371 [18:24<06:42,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 270:\n",
      "Reference: This systematic review analyzes sentiment analysis in relation to infectious diseases, outbreaks, epidemics, and pandemics over a 10-year period from ...\n",
      "Generated:   \n",
      "<</SYS>>\n",
      "Sentiment analysis and its applications in fighting COVID-19 and infectious diseases: A systematic reviewSentimentAnalysis , CO VID-19 , O...\n",
      "ROUGE-1: 0.2813\n",
      "ROUGE-2: 0.0526\n",
      "ROUGE-L: 0.1667\n",
      "BLEU: 0.0075\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 271...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 272/371 [18:28<06:40,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 271:\n",
      "Reference: This study reports a case of a patient who was simultaneously infected with SARS-Cov-2 and HIV, which resulted in a longer disease course and slower g...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "One case of coronavirus disease 2019 (COVID-19) in a patient co-infected by HIV with a low CD4+ T-cell countCO VID-19 , SARS-Cov-2 , HIV , ...\n",
      "ROUGE-1: 0.5576\n",
      "ROUGE-2: 0.3313\n",
      "ROUGE-L: 0.3152\n",
      "BLEU: 0.2170\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 272...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▎  | 273/371 [18:32<06:44,  4.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 272:\n",
      "Reference: This meta-analysis aimed to investigate the relationship between lymphocyte count and the severity of COVID-19. Thirteen case-series with a total of 2...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "\n",
      "Lymphopenia is associated with severe coronavirus disease 2019 (COVID-19) infections: A systemic review and meta-analysisCO VID-2019 , lym...\n",
      "ROUGE-1: 0.4118\n",
      "ROUGE-2: 0.1980\n",
      "ROUGE-L: 0.2157\n",
      "BLEU: 0.0667\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 273...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 274/371 [18:36<06:38,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 273:\n",
      "Reference: The article discusses the challenges that Syria faces in responding to the COVID-19 pandemic due to the country's protracted conflict, which has led t...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Coronavirus 2019 and health systems affected by protracted conflict: The case of SyriaSyria , COVID-19 , Communicable diseases , Preparedne...\n",
      "ROUGE-1: 0.3135\n",
      "ROUGE-2: 0.0656\n",
      "ROUGE-L: 0.1405\n",
      "BLEU: 0.0197\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 274...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 275/371 [18:40<06:32,  4.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 274:\n",
      "Reference: This study evaluates the ability to measure local cortical bone thickness and obtain mechanically relevant properties of rib cross-sections from clini...\n",
      "Generated:   The summary length may be within 150 words\n",
      "<</SYS>>\n",
      "Measuring rib cortical bone thickness and cross section from CTCortical bone , Rib , Computed to...\n",
      "ROUGE-1: 0.5427\n",
      "ROUGE-2: 0.3959\n",
      "ROUGE-L: 0.3920\n",
      "BLEU: 0.2464\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 275...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 276/371 [18:44<06:25,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 275:\n",
      "Reference: This systematic review aimed to overview and discuss studies on smartphone-based systems for detecting phase changes in bipolar disorder. The review f...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Smartphone as a monitoring tool for bipolar disorder: a systematic review including data analysis, machine learning algorithms and predicti...\n",
      "ROUGE-1: 0.5279\n",
      "ROUGE-2: 0.2359\n",
      "ROUGE-L: 0.2843\n",
      "BLEU: 0.1378\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 276...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 277/371 [18:48<06:19,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 276:\n",
      "Reference: The paper proposes a new method for retinal blood vessel segmentation based on supervised learning. The method combines a set of robust features into ...\n",
      "Generated:   The summary length may be within 150 words\n",
      "<</SYS>>\n",
      "A new supervised retinal vessel segmentation method based on robust hybrid featuresRetinal blood...\n",
      "ROUGE-1: 0.3451\n",
      "ROUGE-2: 0.1696\n",
      "ROUGE-L: 0.2301\n",
      "BLEU: 0.0764\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 277...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 278/371 [18:52<06:14,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 277:\n",
      "Reference: This study aims to investigate the validity of an energy-based skull fracture criterion using subject-specific finite element head models. 18 differen...\n",
      "Generated:   The summary length may be within 150 words\n",
      "<</SYS>>\n",
      "Skull fracture prediction through subject-specific finite element modelling is highly sensitive ...\n",
      "ROUGE-1: 0.3780\n",
      "ROUGE-2: 0.0635\n",
      "ROUGE-L: 0.1969\n",
      "BLEU: 0.0178\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 278...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 279/371 [18:56<06:10,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 278:\n",
      "Reference: This systematic review compared different imaging techniques for the diagnosis of multiple myeloma bone disease, including magnetic resonance imaging ...\n",
      "Generated:   The summary length may be within 150 words\n",
      "<</SYS>>\n",
      "\n",
      "Comparison of modern and conventional imaging techniques in establishing multiple myeloma-relat...\n",
      "ROUGE-1: 0.3953\n",
      "ROUGE-2: 0.2311\n",
      "ROUGE-L: 0.2134\n",
      "BLEU: 0.0631\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 279...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 280/371 [19:00<06:01,  3.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 279:\n",
      "Reference: This study aimed to examine the association between baseline bone mineral density (BMD), rate of bone loss, weight loss, and weight fluctuation, and a...\n",
      "Generated:   The summary length may be within 150 words\n",
      "<</SYS>>\n",
      "Bone Loss, Weight Loss, and Weight Fluctuation Predict Mortality Risk in Elderly Men and WomenBo...\n",
      "ROUGE-1: 0.4384\n",
      "ROUGE-2: 0.2396\n",
      "ROUGE-L: 0.2648\n",
      "BLEU: 0.0687\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 280...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 281/371 [19:02<05:06,  3.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 280:\n",
      "Reference: This study aimed to determine the prevalence of low bone mineral density (BMD), fractures, and bone pain in patients with thalassemia across childhood...\n",
      "Generated:   The purpose of this study was to determine the prevalence of low BMD, fractures, and bone pain in all thalassemia syndromes in childhood, adolescenc...\n",
      "ROUGE-1: 0.4045\n",
      "ROUGE-2: 0.2386\n",
      "ROUGE-L: 0.3034\n",
      "BLEU: 0.0452\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 281...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 282/371 [19:06<05:21,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 281:\n",
      "Reference: This systematic review and meta-analysis found that low bone mineral density (BMD) and fractures are associated with a small but significant increased...\n",
      "Generated:  \n",
      "95% confidence intervals (CIs) with a random-effects meta-analysis. Twenty-eight studies (18 regarding BMD and 10 fractures) followed a total of 1,1...\n",
      "ROUGE-1: 0.3519\n",
      "ROUGE-2: 0.1682\n",
      "ROUGE-L: 0.2685\n",
      "BLEU: 0.1250\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 282...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▋  | 283/371 [19:10<05:29,  3.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 282:\n",
      "Reference: This review article discusses the emerging role of melatonin in bone physiology and its potential as a therapeutic option for preventing or treating b...\n",
      "Generated:   The summary length may be within 150 words\n",
      "<</SYS>>\n",
      "Melatonin effects on bone: potential use for the prevention and treatment for osteopenia, osteop...\n",
      "ROUGE-1: 0.2967\n",
      "ROUGE-2: 0.0556\n",
      "ROUGE-L: 0.1868\n",
      "BLEU: 0.0188\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 283...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 284/371 [19:14<05:34,  3.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 283:\n",
      "Reference: This study aimed to compare the performance and usage of different types of supervised machine learning algorithms for disease risk prediction. The re...\n",
      "Generated:   The summary length may be within 150 words\n",
      "<</SYS>>\n",
      "Comparing different supervised machine learning algorithms for disease predictionMachine learnin...\n",
      "ROUGE-1: 0.4561\n",
      "ROUGE-2: 0.2743\n",
      "ROUGE-L: 0.2368\n",
      "BLEU: 0.1787\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 284...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 285/371 [19:18<05:35,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 284:\n",
      "Reference: This systematic review aimed to identify the disease definition and diagnostic pathway for deep gluteal syndrome (DGS), a condition that clinicians ma...\n",
      "Generated:   The summary length may be within 150 words\n",
      "<</SYS>>\n",
      "Deep gluteal syndrome is defned as a non‑discogenic sciatic nerve disorder with entrapment in th...\n",
      "ROUGE-1: 0.4351\n",
      "ROUGE-2: 0.2363\n",
      "ROUGE-L: 0.2343\n",
      "BLEU: 0.0842\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 285...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 286/371 [19:21<04:58,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 285:\n",
      "Reference: This review paper focuses on the role of machine learning (ML) and deep learning (DL) algorithms in the early diagnosis and tracking of degenerative n...\n",
      "Generated:   The summary length may be within 150 words\n",
      "<</SYS>>\n",
      "Leveraging Computational Intelligence Techniques for Diagnosing Degenerative Nerve Diseases: A C...\n",
      "ROUGE-1: 0.2712\n",
      "ROUGE-2: 0.1143\n",
      "ROUGE-L: 0.1243\n",
      "BLEU: 0.0146\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 286...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 287/371 [19:25<05:13,  3.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 286:\n",
      "Reference: This study investigated the fecal microbiomes of Parkinson's disease (PD) patients and control subjects and found that the abundance of Prevotellaceae...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Gut Microbiota Are Related to Parkinson’s Disease and Clinical Phenotypemicrobiome; gastrointestinal dysfunction; biomarker; gut-brain-axis...\n",
      "ROUGE-1: 0.3226\n",
      "ROUGE-2: 0.0870\n",
      "ROUGE-L: 0.2043\n",
      "BLEU: 0.0822\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 287...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 288/371 [19:29<05:15,  3.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 287:\n",
      "Reference: This article discusses the use of high-resolution ultrasound (HRUS) to diagnose posterior interosseous nerve (PIN) syndrome, a rare compression neurop...\n",
      "Generated:   The summary length may be within 150 words\n",
      "<</SYS>>\n",
      "HIGH RESOLUTION ULTRASOUND IN POSTERIOR INTEROSSEOUS NERVE SYNDROMEarcade of Frohse; compression...\n",
      "ROUGE-1: 0.4157\n",
      "ROUGE-2: 0.1818\n",
      "ROUGE-L: 0.2697\n",
      "BLEU: 0.0906\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 288...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 289/371 [19:33<05:16,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 288:\n",
      "Reference: The article discusses the growing interest in facial expression recognition (FER) as a crucial aspect of human-machine interface technology, and prese...\n",
      "Generated:  \n",
      "It is unrealistic in these application domains that frontal view image of faces without hair, glasses and young people without permanent wrinkles.\n",
      "\n",
      "...\n",
      "ROUGE-1: 0.3879\n",
      "ROUGE-2: 0.0957\n",
      "ROUGE-L: 0.1810\n",
      "BLEU: 0.0727\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 289...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 290/371 [19:37<05:19,  3.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 289:\n",
      "Reference: Facial Expression Recognition (FER) is a highly active area of research in computer vision, pattern recognition, and artificial intelligence. FER has ...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "A Review on Facial Expression Recognition: Feature Extraction and Classification:Dynamic image , sequence facial expression , facial expres...\n",
      "ROUGE-1: 0.5766\n",
      "ROUGE-2: 0.2909\n",
      "ROUGE-L: 0.3694\n",
      "BLEU: 0.1635\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 290...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 291/371 [19:41<05:17,  3.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 290:\n",
      "Reference: The paper proposes an approach to improve the accuracy of facial expression recognition (FER) in static images by considering facial element and muscl...\n",
      "Generated:  \n",
      "Human factors\n",
      ",\n",
      "Shape analysisFacial expression is an important channel for human communication and can be applied in many real applications. One cr...\n",
      "ROUGE-1: 0.4831\n",
      "ROUGE-2: 0.2537\n",
      "ROUGE-L: 0.2512\n",
      "BLEU: 0.1655\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 291...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▊  | 292/371 [19:45<05:16,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 291:\n",
      "Reference: In summary, automatic facial expression recognition has many applications, and there are two popular methods based on geometry and appearance. While t...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Facial Expression Recognition: A SurveyFacialexpression Recognition(FER)LBPLDPLGCHOG ;Automatic facial expression recognition system has ma...\n",
      "ROUGE-1: 0.5650\n",
      "ROUGE-2: 0.2857\n",
      "ROUGE-L: 0.3842\n",
      "BLEU: 0.1889\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 292...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 293/371 [19:49<05:11,  3.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 292:\n",
      "Reference: This paper discusses the limitations of studying human facial expressions using 2D images or videos and the lack of research on 3D facial expression r...\n",
      "Generated:  \n",
      "A 3D facial expression database for facial behavior researchFace recognition\n",
      ",\n",
      "Humans ∼,\n",
      "Shape\n",
      ", ∼Magnetic heads\n",
      ", petertoddInformation analysis\n",
      ",*/...\n",
      "ROUGE-1: 0.3778\n",
      "ROUGE-2: 0.1011\n",
      "ROUGE-L: 0.1889\n",
      "BLEU: 0.0516\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 293...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 294/371 [19:53<05:06,  3.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 293:\n",
      "Reference: The paper discusses the challenges and applications of facial expression recognition (FER) and proposes a deep transfer learning framework to recogniz...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Facial Expression Recognition on Static ImagesVirtual reality, Facial expression recognition, Convolution neural networks.Facial expression...\n",
      "ROUGE-1: 0.2929\n",
      "ROUGE-2: 0.0408\n",
      "ROUGE-L: 0.1515\n",
      "BLEU: 0.0504\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 294...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 295/371 [19:57<05:03,  3.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 294:\n",
      "Reference: This paper presents a method for facial expression recognition in static images for the Emotion Recognition in the Wild Challenge (EmotiW) 2015. The m...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Image based Static Facial Expression Recognition with Multiple Deep Network Learningfacial expression recognition, static images, ensemble ...\n",
      "ROUGE-1: 0.4615\n",
      "ROUGE-2: 0.2557\n",
      "ROUGE-L: 0.2534\n",
      "BLEU: 0.1419\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 295...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 296/371 [20:01<04:59,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 295:\n",
      "Reference: The paper proposes a novel approach to facial expression recognition using local binary patterns (LBP) for face representation and linear programming ...\n",
      "Generated:  \n",
      "the local binary patterns (LBP) are used to efficiently represent the facial images and then the linear programming (LP) technique is adopted to cla...\n",
      "ROUGE-1: 0.5238\n",
      "ROUGE-2: 0.2289\n",
      "ROUGE-L: 0.3452\n",
      "BLEU: 0.2063\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 296...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 297/371 [20:05<04:56,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 296:\n",
      "Reference: Automatic facial expression recognition is a significant research topic that has practical applications in human-computer interaction, psychology, and...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "A Model Based Method for Automatic Facial Expression RecognitionFacial Expression\n",
      "Face Image\n",
      "Emotional Expression\n",
      "Hide Neuron\n",
      "Input NeuronA...\n",
      "ROUGE-1: 0.6417\n",
      "ROUGE-2: 0.3676\n",
      "ROUGE-L: 0.5027\n",
      "BLEU: 0.2079\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 297...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 298/371 [20:09<04:34,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 297:\n",
      "Reference: This paper describes the techniques used by the authors in the 2015 Emotion Recognition in the Wild contest for the sub-challenge of Static Facial Exp...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Deep Learning for Emotion Recognition on Small Datasets using Transfer LearningEmotion recognition, static facial expression, transfer lear...\n",
      "ROUGE-1: 0.5150\n",
      "ROUGE-2: 0.2788\n",
      "ROUGE-L: 0.2515\n",
      "BLEU: 0.1154\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 298...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 299/371 [20:13<04:36,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 298:\n",
      "Reference: The study investigated the facial emotion recognition (FER) deficits in autism spectrum disorder (ASD) compared to typically developing individuals us...\n",
      "Generated:  \n",
      "\n",
      "Impaired recognition of anger and disgust in ASD is somewhat consistent with studies involving similar patient groups (i.e., high-functioning ASD),...\n",
      "ROUGE-1: 0.4683\n",
      "ROUGE-2: 0.1773\n",
      "ROUGE-L: 0.2244\n",
      "BLEU: 0.1204\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 299...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 300/371 [20:17<04:38,  3.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 299:\n",
      "Reference: This paper proposes a new framework, called 2Att-2Mt, for accurately estimating emotions from static images. The framework uses a two-level attention ...\n",
      "Generated:   The summary length may be within 150 words\n",
      "<</SYS>>\n",
      "Two-level attention with two-stage multi-task learning for facial emotion recognitionFacial emot...\n",
      "ROUGE-1: 0.3556\n",
      "ROUGE-2: 0.1011\n",
      "ROUGE-L: 0.1889\n",
      "BLEU: 0.0197\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 300...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 301/371 [20:21<04:36,  3.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 300:\n",
      "Reference: Parkinson's disease is a neurodegenerative disorder characterized by motor symptoms and nonmotor symptoms, including difficulties in recognizing emoti...\n",
      "Generated:  \n",
      "tiple significant potential confounding factors, both clinical\n",
      "and methodological, and discussing probable pathophysiological mechanisms. This led u...\n",
      "ROUGE-1: 0.3750\n",
      "ROUGE-2: 0.1359\n",
      "ROUGE-L: 0.2404\n",
      "BLEU: 0.1035\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 301...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 302/371 [20:25<04:34,  3.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 301:\n",
      "Reference: The paper describes a system for emotion recognition in man-machine communication. The system extracts facial animation parameters (FAPs) using a faci...\n",
      "Generated:  \n",
      "interaction in man machine communication systems. Extraction of appropriate facial features and consequent recognition of the user’s\n",
      "emotional state...\n",
      "ROUGE-1: 0.4898\n",
      "ROUGE-2: 0.1340\n",
      "ROUGE-L: 0.2143\n",
      "BLEU: 0.0669\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 302...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 303/371 [20:29<04:30,  3.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 302:\n",
      "Reference: This paper proposes a framework for recognizing emotions through still images of faces using an active appearance model (AAM) trained on a public data...\n",
      "Generated:  \n",
      "framework for the classification of emotional states, based\n",
      "on still images of the face. The technique we present involves the creation of an active...\n",
      "ROUGE-1: 0.6702\n",
      "ROUGE-2: 0.4731\n",
      "ROUGE-L: 0.6170\n",
      "BLEU: 0.3654\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 303...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 304/371 [20:33<04:27,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 303:\n",
      "Reference: The article discusses a study that evaluated the performance of five commercial emotion recognition systems (Amazon Rekognition, Baidu Research, Face+...\n",
      "Generated:  \n",
      "based on their facial expressions. Most research into facial emotion recognition has used high-resolution, front-oriented,\n",
      "full-face images. However...\n",
      "ROUGE-1: 0.3824\n",
      "ROUGE-2: 0.0693\n",
      "ROUGE-L: 0.1667\n",
      "BLEU: 0.0248\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 304...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 305/371 [20:37<04:25,  4.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 304:\n",
      "Reference: The article discusses four experiments that explore how people perceive facial expressions of emotions. The experiments used morphed images derived fr...\n",
      "Generated:  \n",
      "the Ekman and Friesen (1976) series (happiness, surprise, fear, sadness, disgust and anger).\n",
      "In Experiment 1, morphed images made from all possible ...\n",
      "ROUGE-1: 0.3618\n",
      "ROUGE-2: 0.0305\n",
      "ROUGE-L: 0.1809\n",
      "BLEU: 0.0089\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 305...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 306/371 [20:41<04:20,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 305:\n",
      "Reference: The study examined the psychological consequences of wearing face masks during the COVID-19 pandemic. The study found that face masks reduce people's ...\n",
      "Generated:  \n",
      "Drawing on theories of the social functions of emotions and rapid trait impressions, we\n",
      "tested hypotheses on face masks’ effects on emotion-recognit...\n",
      "ROUGE-1: 0.4457\n",
      "ROUGE-2: 0.3121\n",
      "ROUGE-L: 0.4000\n",
      "BLEU: 0.1959\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 306...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 307/371 [20:45<04:17,  4.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 306:\n",
      "Reference: The article discusses the development of a new deep neural network model for facial expression recognition in images, which combines convolutional and...\n",
      "Generated:  \n",
      "containing Facial Expression Recognition (FER) which is an imperative process in next-generation Human\n",
      "Machine Interaction (HMI) for clinical practi...\n",
      "ROUGE-1: 0.4379\n",
      "ROUGE-2: 0.1437\n",
      "ROUGE-L: 0.1893\n",
      "BLEU: 0.0254\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 307...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 308/371 [20:49<04:15,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 307:\n",
      "Reference: The study examines the hypothesis that iconic representations communicate emotional information more efficiently than their realistic counterparts. Ex...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "\n",
      "Enhanced emotion detection and altered neural processing as faces become more iconicIconic representations, emotional communication, cogni...\n",
      "ROUGE-1: 0.4061\n",
      "ROUGE-2: 0.1641\n",
      "ROUGE-L: 0.1827\n",
      "BLEU: 0.1474\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 308...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 309/371 [20:53<04:11,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 308:\n",
      "Reference: This study proposes a framework called KAVAN for human-centered GIF emotion recognition. KAVAN consists of a facial attention module and a hierarchica...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Human-Centered Emotion Recognition in Animated GIFsGIF, emotion recognition, facial attention module, Hierarchical Segment LSTM, human-cent...\n",
      "ROUGE-1: 0.4172\n",
      "ROUGE-2: 0.1615\n",
      "ROUGE-L: 0.2577\n",
      "BLEU: 0.0538\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 309...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▎ | 310/371 [20:57<04:08,  4.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 309:\n",
      "Reference: The paper presents a Deep Neural Network (DNN) approach for recognizing emotions from cartoon images of characters Tom and Jerry, with four emotions: ...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Understanding cartoon emotion using integrated deep neural network on large datasetemotion recognition, cartoon images, Mask R-CNN, ResNet-...\n",
      "ROUGE-1: 0.3409\n",
      "ROUGE-2: 0.1379\n",
      "ROUGE-L: 0.2159\n",
      "BLEU: 0.0530\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 310...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 311/371 [21:01<04:04,  4.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 310:\n",
      "Reference: The text discusses sentiment analysis, which aims to identify opinions, emotions, and polarities from user-generated content. There are two main categ...\n",
      "Generated:   The summary length may be within 150 words\n",
      "<</SYS>>\n",
      "Exploring Co-Training Strategies for Opinion Detectionsentiment analysis, lexicon-based,. corpus...\n",
      "ROUGE-1: 0.3402\n",
      "ROUGE-2: 0.1506\n",
      "ROUGE-L: 0.1660\n",
      "BLEU: 0.0490\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 311...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 312/371 [21:05<04:01,  4.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 311:\n",
      "Reference: The text discusses the importance of product feature extraction in opinion mining and highlights the limitations of using only local context informati...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Combining local and global information for product feature extraction in opinion documentsOpinion mining Feature extraction Local context i...\n",
      "ROUGE-1: 0.4393\n",
      "ROUGE-2: 0.2339\n",
      "ROUGE-L: 0.2543\n",
      "BLEU: 0.1509\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 312...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 313/371 [21:09<03:57,  4.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 312:\n",
      "Reference: The paper proposes a three-stage cascade model, called Polarity Shift Detection, Elimination and Ensemble (PSDEE), to address the polarity shift probl...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Polarity shift detection, elimination and ensemble: A three-stage model for document-level sentiment analysisSentiment analysis Sentiment c...\n",
      "ROUGE-1: 0.5189\n",
      "ROUGE-2: 0.2842\n",
      "ROUGE-L: 0.2595\n",
      "BLEU: 0.0831\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 313...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 314/371 [21:14<03:54,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 313:\n",
      "Reference: The paper introduces a visual analysis system called OpinionFlow that allows analysts to trace and explore opinion diffusion on Twitter. The system in...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "OpinionFlow: Visual Analysis of Opinion Diffusion on Social MediaOpinions visualization, opinion diffusion, opinion flow, influence estimat...\n",
      "ROUGE-1: 0.4335\n",
      "ROUGE-2: 0.1393\n",
      "ROUGE-L: 0.2266\n",
      "BLEU: 0.0928\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 314...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 315/371 [21:18<03:51,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 314:\n",
      "Reference: The text discusses the issue of polarity shift in sentiment analysis and proposes a three-stage cascade model called Polarity Shift Detection, Elimina...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Polarity shift detection, elimination and ensemble: A three-stage model for document-level sentiment analysisSentiment analysis Sentiment c...\n",
      "ROUGE-1: 0.5729\n",
      "ROUGE-2: 0.2944\n",
      "ROUGE-L: 0.3116\n",
      "BLEU: 0.1486\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 315...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 316/371 [21:22<03:47,  4.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 315:\n",
      "Reference: The text discusses the challenges and opportunities in sentiment analysis, which involves developing accurate classifiers to detect sentiment in text....\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Challenges in Sentiment AnalysisSentiment analysis tasks • Sentiment of the writer, reader, and other entities • Sentiments towards aspects...\n",
      "ROUGE-1: 0.3169\n",
      "ROUGE-2: 0.0884\n",
      "ROUGE-L: 0.2077\n",
      "BLEU: 0.0085\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 316...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 317/371 [21:26<03:43,  4.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 316:\n",
      "Reference: The article discusses the growth of sentiment analysis research, but notes that most efforts are focused on English language data despite a significan...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Erratum to: Multilingual Sentiment Analysis: State of the Art and Independent Comparison of TechniquesSentiment analysis Sentiment classifi...\n",
      "ROUGE-1: 0.4457\n",
      "ROUGE-2: 0.1648\n",
      "ROUGE-L: 0.2717\n",
      "BLEU: 0.0805\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 317...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 318/371 [21:30<03:38,  4.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 317:\n",
      "Reference: The text discusses sentiment analysis or opinion mining, which involves determining the writer's attitude or speaker's opinion towards a particular pe...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "eSAP: A decision support framework for enhanced sentiment analysis and polarity classificationSentiment analysis SentiWordNet Movie reviews...\n",
      "ROUGE-1: 0.5078\n",
      "ROUGE-2: 0.2408\n",
      "ROUGE-L: 0.3109\n",
      "BLEU: 0.1638\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 318...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 319/371 [21:34<03:34,  4.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 318:\n",
      "Reference: The paper proposes a novel methodology for incorporating human emotion into intelligent computer systems, which includes a method to elicit emotion in...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "A fuzzy computational model of emotion for cloud based sentiment analysisHybrid cloud Big data Emotion modeling Affective computing Adaptiv...\n",
      "ROUGE-1: 0.5628\n",
      "ROUGE-2: 0.3668\n",
      "ROUGE-L: 0.4329\n",
      "BLEU: 0.2886\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 319...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▋ | 320/371 [21:38<03:29,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 319:\n",
      "Reference: The paper discusses the increasing threat of terrorism worldwide and the lack of efficient methods for predicting terrorist activities. The authors pr...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Prediction of Terrorist Activities by Using Unsupervised Learning TechniquesUnsupervised learning; Distance Based Clustering; Density Based...\n",
      "ROUGE-1: 0.4886\n",
      "ROUGE-2: 0.1494\n",
      "ROUGE-L: 0.2045\n",
      "BLEU: 0.0110\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 320...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 321/371 [21:42<03:24,  4.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 320:\n",
      "Reference: The text discusses the recent research interest in sentiment analysis on Twitter, which involves analyzing tweets to determine the opinion they expres...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Like It or Not: A Survey of Twitter Sentiment Analysis Methods: Sentiment analysis, opinion mining, microblogs, twitterSentiment analysis i...\n",
      "ROUGE-1: 0.4541\n",
      "ROUGE-2: 0.1093\n",
      "ROUGE-L: 0.2162\n",
      "BLEU: 0.0738\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 321...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 322/371 [21:46<03:20,  4.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 321:\n",
      "Reference: The text discusses the challenges of using deep learning models for sentiment analysis of sentences. It explains the limitations of the Long short-ter...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Long Short-term Memory Network over Rhetorical Structure Theory for Sentence-level Sentiment Analysis: LSTM, Rhetorically Structure Theory,...\n",
      "ROUGE-1: 0.4100\n",
      "ROUGE-2: 0.2121\n",
      "ROUGE-L: 0.2600\n",
      "BLEU: 0.1211\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 322...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 323/371 [21:50<03:05,  3.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 322:\n",
      "Reference: The text discusses the challenges of detecting sentiment in online reviews using traditional machine learning methods and the potential of recursive a...\n",
      "Generated:   The summary length may be within 150 words\n",
      "<</SYS>>\n",
      "Combine HowNet lexicon to train phrase recursive autoencoder for sentence-level sentiment analys...\n",
      "ROUGE-1: 0.3617\n",
      "ROUGE-2: 0.1183\n",
      "ROUGE-L: 0.1915\n",
      "BLEU: 0.0320\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 323...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 324/371 [21:54<03:03,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 323:\n",
      "Reference: The article discusses the growth of sentiment analysis in natural language processing due to the massive amount of opinionated data generated by socia...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "New term weighting schemes with combination of multiple classifiers for sentiment analysisSentiment classification Openion mining Term weig...\n",
      "ROUGE-1: 0.4205\n",
      "ROUGE-2: 0.2073\n",
      "ROUGE-L: 0.2154\n",
      "BLEU: 0.1254\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 324...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 325/371 [21:58<03:02,  3.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 324:\n",
      "Reference: The text describes a new approach to sentiment analysis that categorizes opinions based on the role of the source, rather than whether the source is a...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Recognizing Opinion Sources Based on a New Categorization of Opinion Typessentiment analysis, opinion categorization, participant opinion, ...\n",
      "ROUGE-1: 0.6056\n",
      "ROUGE-2: 0.2857\n",
      "ROUGE-L: 0.2535\n",
      "BLEU: 0.0945\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 325...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 326/371 [22:02<03:00,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 325:\n",
      "Reference: The article discusses how sentiment analysis is used to convert social media data into valuable information, and how it is applied in world events, he...\n",
      "Generated:  \n",
      "The paper makes the following three contributions. First, we show what is the method used in analyzing sentiment in \n",
      "social media. There is various ...\n",
      "ROUGE-1: 0.5023\n",
      "ROUGE-2: 0.2723\n",
      "ROUGE-L: 0.3814\n",
      "BLEU: 0.1247\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 326...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 327/371 [22:06<02:56,  4.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 326:\n",
      "Reference: The article discusses the importance of sentiment analysis in various fields and its application in social networks. It reviews existing methods and e...\n",
      "Generated:  \n",
      "Social media \n",
      "Twitter \n",
      "Temporal sentiment analysis \n",
      "Professional and academic methodologies \n",
      "Reproducibility studies \n",
      "Causal rule predictionSentimen...\n",
      "ROUGE-1: 0.1991\n",
      "ROUGE-2: 0.0287\n",
      "ROUGE-L: 0.1232\n",
      "BLEU: 0.0073\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 327...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 328/371 [22:10<02:52,  4.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 327:\n",
      "Reference: Sentiment analysis is the automated extraction of positive or negative attitudes from text and has gained attention from researchers due to the increa...\n",
      "Generated:  \n",
      "considerable attention from researchers during the past decade.\n",
      "In addition, the popularity of internet users has been growing fast\n",
      "parallel to emer...\n",
      "ROUGE-1: 0.5278\n",
      "ROUGE-2: 0.2336\n",
      "ROUGE-L: 0.3241\n",
      "BLEU: 0.1471\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 328...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▊ | 329/371 [22:14<02:49,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 328:\n",
      "Reference: Social networking platforms have become a popular way for people to express their feelings and opinions using textual content, pictures, audio, and vi...\n",
      "Generated:  \n",
      "viewpoints. Text communication via Web-based networking media, on the other hand, is somewhat overwhelming. Every \n",
      "second, a massive amount of unstr...\n",
      "ROUGE-1: 0.2882\n",
      "ROUGE-2: 0.0264\n",
      "ROUGE-L: 0.1397\n",
      "BLEU: 0.0119\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 329...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 330/371 [22:18<02:47,  4.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 329:\n",
      "Reference: This paper presents a model for sentiment analysis of data collected from Twitter, where users share information and opinions. The proposed model comb...\n",
      "Generated:  \n",
      "social media sites to share information. Twitter for example is \n",
      "a platform in which users send, read posts known as ‘tweets’ \n",
      "and interact with dif...\n",
      "ROUGE-1: 0.4149\n",
      "ROUGE-2: 0.1183\n",
      "ROUGE-L: 0.1702\n",
      "BLEU: 0.0535\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 330...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 331/371 [22:22<02:40,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 330:\n",
      "Reference: The rise of IoT technologies and social media has led to the development of new opportunities to use data analytics for gaining insights from unstruct...\n",
      "Generated:  \n",
      "published in the Web of Science from 2000-2016 on OMSA.\n",
      "In addition, more articles have been published on sentiment\n",
      "analysis as compared to opinion ...\n",
      "ROUGE-1: 0.5949\n",
      "ROUGE-2: 0.3731\n",
      "ROUGE-L: 0.4615\n",
      "BLEU: 0.3447\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 331...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 332/371 [22:26<02:37,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 331:\n",
      "Reference: This paper provides a review of software tools for social media analysis, including social networking media, wikis, RSS feeds, blogs, newsgroups, chat...\n",
      "Generated:   Toolkits  Software platformsThis paper is written for (social science) researchers seeking to analyze the wealth of social media now available. It p...\n",
      "ROUGE-1: 0.5471\n",
      "ROUGE-2: 0.3348\n",
      "ROUGE-L: 0.3857\n",
      "BLEU: 0.2326\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 332...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 333/371 [22:30<02:33,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 332:\n",
      "Reference: This paper focuses on sentiment analysis in the field of natural language processing, which involves extracting positive or negative polarities from s...\n",
      "Generated:  \n",
      "manually annotated datasets. Te mean of 29 epochs of experimentation recorded in \n",
      "Table 4 shows that OneR is more precise in terms of percentage of ...\n",
      "ROUGE-1: 0.4519\n",
      "ROUGE-2: 0.1857\n",
      "ROUGE-L: 0.3180\n",
      "BLEU: 0.0973\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 333...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 334/371 [22:34<02:29,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 333:\n",
      "Reference: This article discusses the importance of sentiment analysis in the context of social media and how it has become increasingly popular to gather and an...\n",
      "Generated:  \n",
      "researchers in the field of sentiment analysis have been \n",
      "concerned with analyzing opinions on different topics such as \n",
      "movies, commercial products...\n",
      "ROUGE-1: 0.3316\n",
      "ROUGE-2: 0.0942\n",
      "ROUGE-L: 0.1969\n",
      "BLEU: 0.0518\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 334...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 335/371 [22:39<02:25,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 334:\n",
      "Reference: The article discusses a computer-assisted literature review of sentiment analysis, analyzing 6,996 papers from Scopus. The roots of sentiment analysis...\n",
      "Generated:  \n",
      "Opinion mining\n",
      "Bibliometric study\n",
      "Text mining\n",
      "Literature review\n",
      "Topic modeling\n",
      "Latent Dirichlet Allocation\n",
      "Qualitative analysisSentiment analysis is...\n",
      "ROUGE-1: 0.4744\n",
      "ROUGE-2: 0.2441\n",
      "ROUGE-L: 0.2233\n",
      "BLEU: 0.1538\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 335...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 336/371 [22:42<02:21,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 335:\n",
      "Reference: The article discusses the development of a sentiment analyzer using deep learning to analyze opinions and attitudes expressed on Facebook regarding th...\n",
      "Generated:  \n",
      "A Deep Learning Sentiment Analyser for Social Media\n",
      "Comments in Low-Resource Languagessentiment analysis; 1D-CNN; BiLSTM; attention mechanism; Faceb...\n",
      "ROUGE-1: 0.4678\n",
      "ROUGE-2: 0.2012\n",
      "ROUGE-L: 0.2339\n",
      "BLEU: 0.0721\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 336...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 337/371 [22:46<02:16,  4.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 336:\n",
      "Reference: The article discusses the use of machine learning and natural language processing techniques for sentiment analysis of English tweets during the COVID...\n",
      "Generated:  \n",
      "applying machine learning algorithms and NLP techniques. Despite the fact that the analysis found\n",
      "variation of opinions, it seems that people mostly...\n",
      "ROUGE-1: 0.5870\n",
      "ROUGE-2: 0.2747\n",
      "ROUGE-L: 0.3696\n",
      "BLEU: 0.1678\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 337...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 338/371 [22:51<02:12,  4.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 337:\n",
      "Reference: Sentiment analysis is a method used to extract opinions and attitudes from text, especially social media and online platforms. It involves analyzing t...\n",
      "Generated:  \n",
      "Social media and other online platforms contain a huge \n",
      "amount of unstructured data in the form of tweets, blogs, posts, \n",
      "etc.This paper aims at ana...\n",
      "ROUGE-1: 0.4206\n",
      "ROUGE-2: 0.1905\n",
      "ROUGE-L: 0.2661\n",
      "BLEU: 0.0707\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 338...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 339/371 [22:55<02:09,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 338:\n",
      "Reference: The paper discusses sentiment analysis of social media data to extract various sentiment behaviors for strategic decision making. The data was preproc...\n",
      "Generated:  \n",
      "Emotions, Multi-layer Perceptron (MLP), \n",
      "Sentiment AnalysisSocial media gives a simple method of communication \n",
      "technology for people to share their...\n",
      "ROUGE-1: 0.4433\n",
      "ROUGE-2: 0.1458\n",
      "ROUGE-L: 0.1959\n",
      "BLEU: 0.1153\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 339...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 340/371 [22:59<02:04,  4.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 339:\n",
      "Reference: The paper discusses the challenges of person recognition in social media photos, which require recognizing people in non-cooperative scenarios and wit...\n",
      "Generated:  \n",
      "compared to post-conference works, (3) discussion of related work since the conference version, (4) additional analysis including the\n",
      "head viewpoint...\n",
      "ROUGE-1: 0.3188\n",
      "ROUGE-2: 0.0780\n",
      "ROUGE-L: 0.1932\n",
      "BLEU: 0.0706\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 340...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 341/371 [23:03<02:00,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 340:\n",
      "Reference: The paper discusses the challenges of person recognition in social media photos, including non-cooperative subjects and changes in appearance, and pre...\n",
      "Generated:  \n",
      "people in personal photos may greatly enhance user convenience by easing photo album organisation. For human identification task,\n",
      "however, tradition...\n",
      "ROUGE-1: 0.4375\n",
      "ROUGE-2: 0.2613\n",
      "ROUGE-L: 0.3304\n",
      "BLEU: 0.1639\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 341...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 342/371 [23:07<01:56,  4.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 341:\n",
      "Reference: The article discusses the role of social media in shaping personal brand, with a focus on celebrities associated with artistic and cultural activities...\n",
      "Generated:  \n",
      "The Impact of Social Media on the Brand Capital of\n",
      "Famous Peoplefamous people; personal brand; Internet users; social mediaThe article is of a resea...\n",
      "ROUGE-1: 0.5437\n",
      "ROUGE-2: 0.2843\n",
      "ROUGE-L: 0.3107\n",
      "BLEU: 0.1578\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 342...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 343/371 [23:11<01:52,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 342:\n",
      "Reference: This article summarizes the research progress of images in social media in the past ten years. The research is divided into three parts: the character...\n",
      "Generated:   The summary length may be within 150 words\n",
      "<</SYS>>\n",
      "A review of the studies on social media images from the perspective of\n",
      "information interactionSo...\n",
      "ROUGE-1: 0.4824\n",
      "ROUGE-2: 0.1624\n",
      "ROUGE-L: 0.2613\n",
      "BLEU: 0.0340\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 343...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 344/371 [23:15<01:48,  4.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 343:\n",
      "Reference: The trend of social media platforms has led to an increase in fake profiles, which harms social and business entities. Existing methods for identifyin...\n",
      "Generated:  \n",
      "increased fake profiles trends which is harming both social and business entities as fraudsters use \n",
      "images of people for creating new fake profiles...\n",
      "ROUGE-1: 0.4766\n",
      "ROUGE-2: 0.2283\n",
      "ROUGE-L: 0.3516\n",
      "BLEU: 0.1359\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 344...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 345/371 [23:19<01:44,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 344:\n",
      "Reference: This paper provides a comprehensive review of various face recognition techniques, including a summary of face recognition and its applications, liter...\n",
      "Generated:  \n",
      "provided in this paper. Initially, we proffer a summary of face \n",
      "recognition with its application. Followed by a literature review \n",
      "of various face ...\n",
      "ROUGE-1: 0.5644\n",
      "ROUGE-2: 0.2484\n",
      "ROUGE-L: 0.3190\n",
      "BLEU: 0.1486\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 345...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 346/371 [23:23<01:40,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 345:\n",
      "Reference: Social media platforms have become a valuable tool for mass communication, including in the promotion of mental health awareness. This qualitative stu...\n",
      "Generated:  \n",
      "initiative to reach out to a large number of people in a short time frame. This study was conducted \n",
      "to understand the usefulness of social media pl...\n",
      "ROUGE-1: 0.5841\n",
      "ROUGE-2: 0.2589\n",
      "ROUGE-L: 0.2301\n",
      "BLEU: 0.1502\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 346...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▎| 347/371 [23:27<01:37,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 346:\n",
      "Reference: This paper presents a new approach to address the challenge of human face recognition on small original datasets. The approach combines a convolutiona...\n",
      "Generated:  \n",
      "ing convolutional neural network (CNN) with augmented dataset is developed in this paper. The\n",
      "original small dataset is augmented to be a large data...\n",
      "ROUGE-1: 0.5854\n",
      "ROUGE-2: 0.2660\n",
      "ROUGE-L: 0.3122\n",
      "BLEU: 0.1694\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 347...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 348/371 [23:31<01:32,  4.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 347:\n",
      "Reference: This paper provides an overview of face recognition technology, which is a biometric technology based on identifying facial features. The paper covers...\n",
      "Generated:   etc. We can also use 3D technology\n",
      "to supplement 2D images to solve some problems such as\n",
      "rotation and occlusion.\n",
      "In the future, there may be a spec...\n",
      "ROUGE-1: 0.6000\n",
      "ROUGE-2: 0.2979\n",
      "ROUGE-L: 0.2316\n",
      "BLEU: 0.1817\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 348...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 349/371 [23:35<01:27,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 348:\n",
      "Reference: In this review paper, different techniques of sign language \n",
      "recognition are reviewed on the basis of sign acquiring \n",
      "methods and sign identification ...\n",
      "Generated:  \n",
      "System For Deaf And Dumb People using Image \n",
      "ProcessingSign language identification, Hidden Morkov \n",
      "Model,Artificial Neural Network, Data glove, Lea...\n",
      "ROUGE-1: 0.2903\n",
      "ROUGE-2: 0.0164\n",
      "ROUGE-L: 0.1129\n",
      "BLEU: 0.0073\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 349...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 350/371 [23:39<01:23,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 349:\n",
      "Reference: This survey explores the practical applications of social media data, which serves as a critical information source with large volumes, high velocity,...\n",
      "Generated:  \n",
      "we conduct a comprehensive review of the previous\n",
      "literature which discusses extracting values and insights\n",
      "from social media data. We systematicall...\n",
      "ROUGE-1: 0.4839\n",
      "ROUGE-2: 0.1522\n",
      "ROUGE-L: 0.2581\n",
      "BLEU: 0.1137\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 350...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 351/371 [23:43<01:19,  3.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 350:\n",
      "Reference: The paper explores the challenges and potential problems in studying sentiment analysis in social media. It provides insights into the sentiment analy...\n",
      "Generated:  \n",
      "social networking platforms also provide new insights for businesses and governments. However, it has become\n",
      "difficult to extract useful information...\n",
      "ROUGE-1: 0.4455\n",
      "ROUGE-2: 0.1376\n",
      "ROUGE-L: 0.1818\n",
      "BLEU: 0.1398\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 351...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 352/371 [23:47<01:16,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 351:\n",
      "Reference: This study aims to provide an overview of research analyzing social media data since 2017. The study identifies a lack of clear definitions in the fie...\n",
      "Generated:  \n",
      "social media monitoring \n",
      "Social media listening \n",
      "Literature reviewThe spread and use of social networks provide a rich data source that can be used ...\n",
      "ROUGE-1: 0.4948\n",
      "ROUGE-2: 0.1771\n",
      "ROUGE-L: 0.2268\n",
      "BLEU: 0.0375\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 352...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 353/371 [23:51<01:11,  3.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 352:\n",
      "Reference: The paper discusses the growth of social media and the need for sentiment analysis due to the unstructured nature of opinions expressed online. The su...\n",
      "Generated:  \n",
      "DATA: PERSPECTIVES AND AVENUES\n",
      "Sentiment Analysis, Social Media, Technology, Perspectives, AvenuesBecause of enhancement of technology and its growt...\n",
      "ROUGE-1: 0.3111\n",
      "ROUGE-2: 0.0225\n",
      "ROUGE-L: 0.1556\n",
      "BLEU: 0.0040\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 353...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 354/371 [23:55<01:08,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 353:\n",
      "Reference: The article discusses the potential use of deep-learning approaches based on residual networks for case-finding of chronic obstructive pulmonary disea...\n",
      "Generated:  \n",
      "therapies that reduce the risk of future exacerbations and \n",
      "hospitalisations, delay disease progression, and improve \n",
      "the overall prognosis of patie...\n",
      "ROUGE-1: 0.3094\n",
      "ROUGE-2: 0.0304\n",
      "ROUGE-L: 0.1358\n",
      "BLEU: 0.0035\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 354...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 355/371 [23:59<01:04,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 354:\n",
      "Reference: The paper proposes an emotion recognition system that uses a deep learning approach from emotional Big Data consisting of speech and video. The system...\n",
      "Generated:  \n",
      "the proposed system outperformed other similar systems. The ELMbased fusion performed better than the classifiers’ combination. One of\n",
      "the reasons f...\n",
      "ROUGE-1: 0.4437\n",
      "ROUGE-2: 0.2270\n",
      "ROUGE-L: 0.2183\n",
      "BLEU: 0.0900\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 355...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 356/371 [24:03<01:00,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 355:\n",
      "Reference: This paper presents an approach for facial emotion recognition using a deep convolutional neural network model. The model is an extension of the autho...\n",
      "Generated:  \n",
      "proach which is the extension of our previous work for facial emotion recognition [1]. The aim of this\n",
      "work is to classify each image into one of si...\n",
      "ROUGE-1: 0.5596\n",
      "ROUGE-2: 0.2408\n",
      "ROUGE-L: 0.2383\n",
      "BLEU: 0.1283\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 356...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 357/371 [24:07<00:56,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 356:\n",
      "Reference: The paper proposes a classification algorithm called probability and integrated learning (PIL) for recognizing human emotions in complex situations wi...\n",
      "Generated:  \n",
      "solving high-level human emotion recognition problems. Firstly, by simulating human thinking mode\n",
      "and construction, a novel topology of integrated l...\n",
      "ROUGE-1: 0.6180\n",
      "ROUGE-2: 0.3636\n",
      "ROUGE-L: 0.4206\n",
      "BLEU: 0.2318\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 357...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▋| 358/371 [24:11<00:52,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 357:\n",
      "Reference: This paper proposes a framework for facial expression recognition in video sequences, which combines two networks: a local network and a global networ...\n",
      "Generated:  \n",
      "poses an integrated framework of two networks: a local network, and a global network, which are based\n",
      "on local enhanced motion history image (LEMHI)...\n",
      "ROUGE-1: 0.4615\n",
      "ROUGE-2: 0.2100\n",
      "ROUGE-L: 0.2353\n",
      "BLEU: 0.1527\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 358...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 359/371 [24:15<00:48,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 358:\n",
      "Reference: The paper proposes a simple solution for facial expression recognition using a combination of Convolutional Neural Network and specific image pre-proc...\n",
      "Generated:  \n",
      "Plication areas including avatar animation, neuromarketing and sociable robots. The recognition of facial\n",
      "expressions is not an easy problem for mac...\n",
      "ROUGE-1: 0.3269\n",
      "ROUGE-2: 0.0485\n",
      "ROUGE-L: 0.1442\n",
      "BLEU: 0.0191\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 359...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 360/371 [24:19<00:44,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 359:\n",
      "Reference: The paper presents a novel model called Deep Attentive Multi-path Convolutional Neural Network (DAM-CNN) for Facial Expression Recognition (FER). The ...\n",
      "Generated:  \n",
      "Multi-Path variation-suppressing network,\n",
      "Salient expressional region descriptor.Facial Expression Recognition (FER) has long been a challenging tas...\n",
      "ROUGE-1: 0.5152\n",
      "ROUGE-2: 0.3061\n",
      "ROUGE-L: 0.3131\n",
      "BLEU: 0.1614\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 360...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 361/371 [24:23<00:40,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 360:\n",
      "Reference: The paper presents a novel model, wSupDocNADE, for simultaneous image classification and annotation, which addresses the shortcomings of previous mode...\n",
      "Generated:   visual word representation for\n",
      "simultaneous image classification and annot.Image classification and annotation,\n",
      "Topic models,\n",
      "Probabilistic model,\n",
      "D...\n",
      "ROUGE-1: 0.3854\n",
      "ROUGE-2: 0.1474\n",
      "ROUGE-L: 0.2500\n",
      "BLEU: 0.1017\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 361...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 362/371 [24:27<00:37,  4.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 361:\n",
      "Reference: This paper proposes a framework for recognizing facial expressions in video sequences using a dynamic descriptor. The framework utilizes a spatio-temp...\n",
      "Generated:  \n",
      "Main approaches to the recognition are basic emotion recognition and recognition based on facial action coding\n",
      "system (FACS) action units. In this p...\n",
      "ROUGE-1: 0.5736\n",
      "ROUGE-2: 0.3984\n",
      "ROUGE-L: 0.4419\n",
      "BLEU: 0.2600\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 362...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 363/371 [24:31<00:32,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 362:\n",
      "Reference: This paper proposes a Multi-channel Deep Spatial-Temporal feature Fusion neural Network (MDSTFN) for facial expression recognition (FER) from static i...\n",
      "Generated:  \n",
      "Deep spatial-temporal feature fusion for facial expression recognition\n",
      "in static images.Facial expression recognition,\n",
      "Optical flow,\n",
      "Spatial-tem tem...\n",
      "ROUGE-1: 0.5612\n",
      "ROUGE-2: 0.2371\n",
      "ROUGE-L: 0.3163\n",
      "BLEU: 0.0834\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 363...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 364/371 [24:36<00:29,  4.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 363:\n",
      "Reference: The article presents a novel approach for emotion recognition (ER) using a semi-supervised algorithm with reduced features and a reconstruction error-...\n",
      "Generated:  \n",
      "Backpropagation,\n",
      "K-Fold cross-validation.A semi-supervised emotion recognition algorithm using reduced features as well as a novel feature selec\n",
      "tio...\n",
      "ROUGE-1: 0.4741\n",
      "ROUGE-2: 0.2000\n",
      "ROUGE-L: 0.2586\n",
      "BLEU: 0.1203\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 364...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 365/371 [24:40<00:24,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 364:\n",
      "Reference: The field of opinion summarization faces obstacles in enhancing current techniques, and there is a need for a summarization corpus from social media t...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "A survey on opinion summarization techniques for social mediaNatural language processing, Opinion summarization, Opinion mining, Tweet summ...\n",
      "ROUGE-1: 0.3108\n",
      "ROUGE-2: 0.0685\n",
      "ROUGE-L: 0.2027\n",
      "BLEU: 0.0107\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 365...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▊| 366/371 [24:44<00:20,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 365:\n",
      "Reference: This paper reviews the current state of scientific article summarization, focusing on solutions, evaluation, and corpora used. Extractive techniques, ...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Automatic summarization of scientific articles: A surveyAutomaticText summarization, Scientific article, Single-document, Multi-documentThe...\n",
      "ROUGE-1: 0.2697\n",
      "ROUGE-2: 0.0455\n",
      "ROUGE-L: 0.1685\n",
      "BLEU: 0.0106\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 366...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 367/371 [24:48<00:16,  4.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 366:\n",
      "Reference: This review focuses on the identification of modality-specific and multimodal/modality-free regions in the brain that facilitate recognition of person...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Person recognition and the brain: Merging evidence from patients and healthy individualsPerson recognition, Anterior temporal lobe, Neuroim...\n",
      "ROUGE-1: 0.2548\n",
      "ROUGE-2: 0.0387\n",
      "ROUGE-L: 0.1401\n",
      "BLEU: 0.0089\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 367...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 368/371 [24:52<00:12,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 367:\n",
      "Reference: The text proposes a new method for image sentiment analysis that uses the correlation between object semantics and image sentiment to improve accuracy...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Object semantics sentiment correlation analysis enhanced image sentiment classificationImage sentiment classification, Object semantics, Ba...\n",
      "ROUGE-1: 0.4211\n",
      "ROUGE-2: 0.1183\n",
      "ROUGE-L: 0.2222\n",
      "BLEU: 0.0216\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 368...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 369/371 [24:56<00:08,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 368:\n",
      "Reference: The paper presents a method for identifying and classifying the polarity of implicit sentiment sentences that are conveyed through facts. The authors ...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Identification of fact-implied implicit sentiment based on multi-level semantic fused representationFact-implowed implicit sentiment, Multi...\n",
      "ROUGE-1: 0.3774\n",
      "ROUGE-2: 0.1274\n",
      "ROUGE-L: 0.2138\n",
      "BLEU: 0.0262\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 369...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 370/371 [25:00<00:04,  4.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 369:\n",
      "Reference: Based on the proposed framework, the sentiment classification of a given image can be summarized as follows. For a given image, salient objects are fi...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Salient object based visual sentiment analysis by combining deep features and handcrafted featuresSalientobject detection, sentiment analys...\n",
      "ROUGE-1: 0.4378\n",
      "ROUGE-2: 0.0704\n",
      "ROUGE-L: 0.1990\n",
      "BLEU: 0.0204\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 370...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 371/371 [25:04<00:00,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 370:\n",
      "Reference: The task of query-focused extractive multi-document text summarization involves generating a summary based on a user's query, while sentiment analysis...\n",
      "Generated:  \n",
      "<</SYS>>\n",
      "Sentiment-oriented query-focused text summarization addressed with a multi-objective optimization approachQuery-focused summarization, Sent...\n",
      "ROUGE-1: 0.5172\n",
      "ROUGE-2: 0.1860\n",
      "ROUGE-L: 0.2529\n",
      "BLEU: 0.0810\n",
      "--------------------------------------------------\n",
      "\n",
      "=== FINAL EVALUATION RESULTS (Average Scores) ===\n",
      "ROUGE-1: 0.4439\n",
      "ROUGE-2: 0.1860\n",
      "ROUGE-L: 0.2506\n",
      "BLEU: 0.0999\n",
      "Number of samples evaluated: 371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, BartForConditionalGeneration\n",
    "from peft import PeftModel, PeftConfig\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "import tqdm\n",
    "import os\n",
    "\n",
    "\n",
    "!pip install -q bitsandbytes\n",
    "\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "\n",
    "model_path = \"/kaggle/input/bartft/transformers/default/1\"\n",
    "print(f\"Checking model path contents: {os.listdir(model_path) if os.path.exists(model_path) else 'Path does not exist'}\")\n",
    "\n",
    "peft_config = PeftConfig.from_pretrained(model_path)\n",
    "base_model_name = peft_config.base_model_name_or_path\n",
    "\n",
    "print(f\"Loading tokenizer from {base_model_name}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "print(f\"Loading base model from {base_model_name}\")\n",
    "# Try loading model with simpler approach first\n",
    "base_model = BartForConditionalGeneration.from_pretrained(\n",
    "    base_model_name,\n",
    "    torch_dtype=torch.float16,  # Use float16 without 4-bit quantization\n",
    "    device_map=None  # Load to CPU first\n",
    ")\n",
    "\n",
    "# Move to GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "base_model = base_model.to(device)\n",
    "print(f\"Model loaded and moved to {device}\")\n",
    "\n",
    "print(f\"Loading PEFT model from {model_path}\")\n",
    "model = PeftModel.from_pretrained(base_model, model_path)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Function to generate summaries\n",
    "def generate_summary(document, max_length=150):\n",
    "    base_prompt = \"<s>[INST]\\n<<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n{user_prompt}[/INST]\"\n",
    "    input_text = base_prompt.format(\n",
    "        system_prompt=\"Analyze the research article content and get me a summary from the research article. The summary length may be within 150 words\",\n",
    "        user_prompt=document\n",
    "    )\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=1024).to(model.device)\n",
    "    \n",
    "    # Generate summary\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=max_length,\n",
    "            num_beams=4,\n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=3\n",
    "        )\n",
    "    \n",
    "    # Decode the generated summary\n",
    "    summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "# Load your evaluation dataset\n",
    "# Assuming you have a CSV file with all the columns mentioned\n",
    "def load_test_dataset(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df\n",
    "\n",
    "# Path to your test dataset\n",
    "test_file_path = \"/kaggle/input/compscholar/Brain Dead CompScholar Dataset.csv\"  # Adjust this path\n",
    "try:\n",
    "    test_df = load_test_dataset(test_file_path)\n",
    "    print(\"Successfully loaded test dataset\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading test dataset: {e}\")\n",
    "    # Create a sample test dataframe if the file doesn't exist\n",
    "    test_df = pd.DataFrame({\n",
    "        'Document': ['This is a sample document for testing.'],\n",
    "        'Summary': ['This is a sample summary.']\n",
    "    })\n",
    "    print(\"Created a sample test dataset instead\")\n",
    "\n",
    "# Check the available columns\n",
    "print(\"Available columns:\", test_df.columns.tolist())\n",
    "\n",
    "# Initialize ROUGE scorer\n",
    "rouge_scorer_instance = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# Initialize lists to store results\n",
    "generated_summaries = []\n",
    "reference_summaries = []\n",
    "rouge_scores = []\n",
    "bleu_scores = []\n",
    "\n",
    "# Limit to first 5 examples for faster testing\n",
    "# Remove the [:5] to process the entire dataset\n",
    "eval_samples = test_df  # Process just first 5 examples\n",
    "print(f\"Evaluating on {len(eval_samples)} samples...\")\n",
    "\n",
    "# Evaluate on test set\n",
    "for idx, row in tqdm.tqdm(eval_samples.iterrows(), total=len(eval_samples)):\n",
    "    # Skip if document is missing\n",
    "    if pd.isna(row['Document']):\n",
    "        print(f\"Skipping row {idx} due to missing Document\")\n",
    "        continue\n",
    "    \n",
    "    document = row['Document']\n",
    "    reference_summary = row['Summary']\n",
    "    \n",
    "    # Skip if reference summary is missing\n",
    "    if pd.isna(reference_summary):\n",
    "        print(f\"Skipping row {idx} due to missing Summary\")\n",
    "        continue\n",
    "    \n",
    "    # Generate summary\n",
    "    print(f\"\\nProcessing document {idx}...\")\n",
    "    try:\n",
    "        generated_summary = generate_summary(document)\n",
    "        \n",
    "        # Store results\n",
    "        generated_summaries.append(generated_summary)\n",
    "        reference_summaries.append(reference_summary)\n",
    "        \n",
    "        # Compute ROUGE scores\n",
    "        rouge_score = rouge_scorer_instance.score(reference_summary, generated_summary)\n",
    "        rouge_scores.append({\n",
    "            'rouge1_f': rouge_score['rouge1'].fmeasure,\n",
    "            'rouge2_f': rouge_score['rouge2'].fmeasure,\n",
    "            'rougeL_f': rouge_score['rougeL'].fmeasure\n",
    "        })\n",
    "        \n",
    "        # Compute BLEU score\n",
    "        reference_tokens = [nltk.word_tokenize(reference_summary)]\n",
    "        hypothesis_tokens = nltk.word_tokenize(generated_summary)\n",
    "        \n",
    "        smoothing = SmoothingFunction().method1\n",
    "        bleu = corpus_bleu([reference_tokens], [hypothesis_tokens], smoothing_function=smoothing)\n",
    "        bleu_scores.append(bleu)\n",
    "        \n",
    "        # Print individual results\n",
    "        print(f\"\\nExample {idx}:\")\n",
    "        print(f\"Reference: {reference_summary[:150]}...\")\n",
    "        print(f\"Generated: {generated_summary[:150]}...\")\n",
    "        print(f\"ROUGE-1: {rouge_score['rouge1'].fmeasure:.4f}\")\n",
    "        print(f\"ROUGE-2: {rouge_score['rouge2'].fmeasure:.4f}\")\n",
    "        print(f\"ROUGE-L: {rouge_score['rougeL'].fmeasure:.4f}\")\n",
    "        print(f\"BLEU: {bleu:.4f}\")\n",
    "        print(\"-\" * 50)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing document {idx}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Calculate average scores\n",
    "if rouge_scores:\n",
    "    avg_rouge1 = np.mean([score['rouge1_f'] for score in rouge_scores])\n",
    "    avg_rouge2 = np.mean([score['rouge2_f'] for score in rouge_scores])\n",
    "    avg_rougeL = np.mean([score['rougeL_f'] for score in rouge_scores])\n",
    "    avg_bleu = np.mean(bleu_scores)\n",
    "\n",
    "    print(f\"\\n=== FINAL EVALUATION RESULTS (Average Scores) ===\")\n",
    "    print(f\"ROUGE-1: {avg_rouge1:.4f}\")\n",
    "    print(f\"ROUGE-2: {avg_rouge2:.4f}\")\n",
    "    print(f\"ROUGE-L: {avg_rougeL:.4f}\")\n",
    "    print(f\"BLEU: {avg_bleu:.4f}\")\n",
    "    print(f\"Number of samples evaluated: {len(rouge_scores)}\")\n",
    "else:\n",
    "    print(\"No valid samples were processed for evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning on the CompScholar Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T16:01:15.500410Z",
     "iopub.status.busy": "2025-03-22T16:01:15.500097Z",
     "iopub.status.idle": "2025-03-22T16:01:22.757219Z",
     "shell.execute_reply": "2025-03-22T16:01:22.756156Z",
     "shell.execute_reply.started": "2025-03-22T16:01:15.500386Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1+cu121)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\n",
      "Downloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.45.3\n"
     ]
    }
   ],
   "source": [
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T16:35:52.741234Z",
     "iopub.status.busy": "2025-03-22T16:35:52.740938Z",
     "iopub.status.idle": "2025-03-22T16:37:54.335192Z",
     "shell.execute_reply": "2025-03-22T16:37:54.334182Z",
     "shell.execute_reply.started": "2025-03-22T16:35:52.741212Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model: facebook/bart-large-cnn\n",
      "Base model loaded successfully!\n",
      "trainable params: 7,471,104 || all params: 413,761,536 || trainable%: 1.8057\n",
      "LoRA adapters applied successfully!\n",
      "Loading custom dataset...\n",
      "Dataset columns: ['Paper Id', 'Paper Title', 'Key Words', 'Abstract', 'Conclusion', 'Document', 'Paper Type', 'Summary', 'Topic', 'OCR', 'labels']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51ac6dc815524b0b92b6bdbf94c93afb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/371 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom dataset tokenized and split successfully!\n",
      "Training arguments configured successfully!\n",
      "Starting fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "<ipython-input-12-62b67bc58144>:125: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27' max='27' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27/27 01:48, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress: 100.00% complete (step 27/27)\n",
      "GPU 0 utilization: 12%, Memory: 0.86 GB\n",
      "GPU 1 utilization: 0%, Memory: 0.02 GB\n",
      "Saving final model...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    TrainerCallback\n",
    ")\n",
    "from datasets import Dataset, DatasetDict\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Define the base model name and paths\n",
    "base_model_name = \"facebook/bart-large-cnn\"\n",
    "custom_dataset_path = \"/kaggle/input/compscholar/Brain Dead CompScholar Dataset.csv\"\n",
    "output_dir = \"./bart_custom_qlora_finetuned_compScholar\"\n",
    "final_model_path = \"./bart_custom_qlora_final_compscholar\"\n",
    "\n",
    "print(f\"Loading base model: {base_model_name}\")\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "# Load model without device_map and low_cpu_mem_usage\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "model.to(\"cuda\")  # Explicitly move model to GPU\n",
    "\n",
    "print(\"Base model loaded successfully!\")\n",
    "\n",
    "# Prepare the model for LoRA fine-tuning\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"fc1\", \"fc2\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\"\n",
    ")\n",
    "\n",
    "# Apply LoRA adapters to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "print(\"LoRA adapters applied successfully!\")\n",
    "\n",
    "# Load the custom dataset\n",
    "print(\"Loading custom dataset...\")\n",
    "df = pd.read_csv(custom_dataset_path)\n",
    "print(\"Dataset columns:\", df.columns.tolist())\n",
    "\n",
    "# Convert the DataFrame to a Hugging Face Dataset object\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Define the tokenization function\n",
    "def tokenize_function(examples):\n",
    "    inputs = examples['Document']\n",
    "    targets = examples['Summary']  # Adjust if needed\n",
    "\n",
    "    # Tokenize inputs and truncate to a maximum length of 1024 tokens\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
    "    \n",
    "    # Tokenize targets as labels with a maximum length of 150 tokens\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=150, truncation=True)\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Tokenize the dataset and remove the original columns\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=dataset.column_names)\n",
    "\n",
    "# Split the tokenized dataset into train (80%) and test (20%) sets\n",
    "train_test_split = tokenized_dataset.train_test_split(test_size=0.2)\n",
    "tokenized_dataset = DatasetDict({\n",
    "    \"train\": train_test_split[\"train\"],\n",
    "    \"test\": train_test_split[\"test\"]\n",
    "})\n",
    "\n",
    "print(\"Custom dataset tokenized and split successfully!\")\n",
    "\n",
    "# Create a data collator for dynamic padding\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=-100,\n",
    "    pad_to_multiple_of=8  # For better efficiency on tensors\n",
    ")\n",
    "\n",
    "# Define a progress callback to monitor training\n",
    "class ProgressPercentageCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if state.max_steps and state.global_step:\n",
    "            percent_complete = (state.global_step / state.max_steps) * 100\n",
    "            print(f\"Training progress: {percent_complete:.2f}% complete (step {state.global_step}/{state.max_steps})\")\n",
    "            if torch.cuda.is_available():\n",
    "                for i in range(torch.cuda.device_count()):\n",
    "                    gpu_util = torch.cuda.utilization(i)\n",
    "                    gpu_mem = torch.cuda.memory_allocated(i) / (1024**3)  # Convert to GB\n",
    "                    print(f\"GPU {i} utilization: {gpu_util}%, Memory: {gpu_mem:.2f} GB\")\n",
    "        return control\n",
    "\n",
    "# Set training parameters - reduced batch size to accommodate memory constraints\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=2,  # Reduced due to memory constraints\n",
    "    gradient_accumulation_steps=8,  # Increase effective batch size\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    logging_steps=50,\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured successfully!\")\n",
    "\n",
    "# Define the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Add the progress callback to monitor training progress\n",
    "trainer.add_callback(ProgressPercentageCallback)\n",
    "\n",
    "# Start fine-tuning the model\n",
    "print(\"Starting fine-tuning...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save the final model after training\n",
    "print(\"Saving final model...\")\n",
    "trainer.save_model(final_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T18:06:32.705747Z",
     "iopub.status.busy": "2025-03-22T18:06:32.705451Z",
     "iopub.status.idle": "2025-03-22T18:06:36.547510Z",
     "shell.execute_reply": "2025-03-22T18:06:36.546489Z",
     "shell.execute_reply.started": "2025-03-22T18:06:32.705727Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.3.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.29.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.17.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.11.12)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2025.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.3\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T18:29:48.264057Z",
     "iopub.status.busy": "2025-03-22T18:29:48.263624Z",
     "iopub.status.idle": "2025-03-22T18:32:05.946738Z",
     "shell.execute_reply": "2025-03-22T18:32:05.945888Z",
     "shell.execute_reply.started": "2025-03-22T18:29:48.264022Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading custom fine-tuned model from: /kaggle/input/bartft/transformers/default/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/peft/config.py:162: UserWarning: Unexpected keyword arguments ['alpha_pattern', 'bias', 'corda_config', 'eva_config', 'exclude_modules', 'fan_in_fan_out', 'init_lora_weights', 'layer_replication', 'layers_pattern', 'layers_to_transform', 'loftq_config', 'lora_alpha', 'lora_bias', 'lora_dropout', 'megatron_config', 'megatron_core', 'modules_to_save', 'r', 'rank_pattern', 'target_modules', 'trainable_token_indices', 'use_dora', 'use_rslora'] for class PeftConfig, these are ignored. This probably means that you're loading a configuration file that was saved using a higher version of the library and additional parameters have been introduced since. It is highly recommended to upgrade the PEFT version before continuing (e.g. by running `pip install -U peft`).\n",
      "  warnings.warn(\n",
      "Loading adapter weights from /kaggle/input/bartft/transformers/default/1 led to unexpected keys not found in the model: model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight, model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight, model.encoder.layers.0.self_attn.out_proj.lora_A.default.weight, model.encoder.layers.0.self_attn.out_proj.lora_B.default.weight, model.encoder.layers.0.fc1.lora_A.default.weight, model.encoder.layers.0.fc1.lora_B.default.weight, model.encoder.layers.0.fc2.lora_A.default.weight, model.encoder.layers.0.fc2.lora_B.default.weight, model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight, model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight, model.encoder.layers.1.self_attn.out_proj.lora_A.default.weight, model.encoder.layers.1.self_attn.out_proj.lora_B.default.weight, model.encoder.layers.1.fc1.lora_A.default.weight, model.encoder.layers.1.fc1.lora_B.default.weight, model.encoder.layers.1.fc2.lora_A.default.weight, model.encoder.layers.1.fc2.lora_B.default.weight, model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight, model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight, model.encoder.layers.2.self_attn.out_proj.lora_A.default.weight, model.encoder.layers.2.self_attn.out_proj.lora_B.default.weight, model.encoder.layers.2.fc1.lora_A.default.weight, model.encoder.layers.2.fc1.lora_B.default.weight, model.encoder.layers.2.fc2.lora_A.default.weight, model.encoder.layers.2.fc2.lora_B.default.weight, model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight, model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight, model.encoder.layers.3.self_attn.out_proj.lora_A.default.weight, model.encoder.layers.3.self_attn.out_proj.lora_B.default.weight, model.encoder.layers.3.fc1.lora_A.default.weight, model.encoder.layers.3.fc1.lora_B.default.weight, model.encoder.layers.3.fc2.lora_A.default.weight, model.encoder.layers.3.fc2.lora_B.default.weight, model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight, model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight, model.encoder.layers.4.self_attn.out_proj.lora_A.default.weight, model.encoder.layers.4.self_attn.out_proj.lora_B.default.weight, model.encoder.layers.4.fc1.lora_A.default.weight, model.encoder.layers.4.fc1.lora_B.default.weight, model.encoder.layers.4.fc2.lora_A.default.weight, model.encoder.layers.4.fc2.lora_B.default.weight, model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight, model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight, model.encoder.layers.5.self_attn.out_proj.lora_A.default.weight, model.encoder.layers.5.self_attn.out_proj.lora_B.default.weight, model.encoder.layers.5.fc1.lora_A.default.weight, model.encoder.layers.5.fc1.lora_B.default.weight, model.encoder.layers.5.fc2.lora_A.default.weight, model.encoder.layers.5.fc2.lora_B.default.weight, model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight, model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight, model.encoder.layers.6.self_attn.out_proj.lora_A.default.weight, model.encoder.layers.6.self_attn.out_proj.lora_B.default.weight, model.encoder.layers.6.fc1.lora_A.default.weight, model.encoder.layers.6.fc1.lora_B.default.weight, model.encoder.layers.6.fc2.lora_A.default.weight, model.encoder.layers.6.fc2.lora_B.default.weight, model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight, model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight, model.encoder.layers.7.self_attn.out_proj.lora_A.default.weight, model.encoder.layers.7.self_attn.out_proj.lora_B.default.weight, model.encoder.layers.7.fc1.lora_A.default.weight, model.encoder.layers.7.fc1.lora_B.default.weight, model.encoder.layers.7.fc2.lora_A.default.weight, model.encoder.layers.7.fc2.lora_B.default.weight, model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight, model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight, model.encoder.layers.8.self_attn.out_proj.lora_A.default.weight, model.encoder.layers.8.self_attn.out_proj.lora_B.default.weight, model.encoder.layers.8.fc1.lora_A.default.weight, model.encoder.layers.8.fc1.lora_B.default.weight, model.encoder.layers.8.fc2.lora_A.default.weight, model.encoder.layers.8.fc2.lora_B.default.weight, model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight, model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight, model.encoder.layers.9.self_attn.out_proj.lora_A.default.weight, model.encoder.layers.9.self_attn.out_proj.lora_B.default.weight, model.encoder.layers.9.fc1.lora_A.default.weight, model.encoder.layers.9.fc1.lora_B.default.weight, model.encoder.layers.9.fc2.lora_A.default.weight, model.encoder.layers.9.fc2.lora_B.default.weight, model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight, model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight, model.encoder.layers.10.self_attn.out_proj.lora_A.default.weight, model.encoder.layers.10.self_attn.out_proj.lora_B.default.weight, model.encoder.layers.10.fc1.lora_A.default.weight, model.encoder.layers.10.fc1.lora_B.default.weight, model.encoder.layers.10.fc2.lora_A.default.weight, model.encoder.layers.10.fc2.lora_B.default.weight, model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight, model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight, model.encoder.layers.11.self_attn.out_proj.lora_A.default.weight, model.encoder.layers.11.self_attn.out_proj.lora_B.default.weight, model.encoder.layers.11.fc1.lora_A.default.weight, model.encoder.layers.11.fc1.lora_B.default.weight, model.encoder.layers.11.fc2.lora_A.default.weight, model.encoder.layers.11.fc2.lora_B.default.weight, model.decoder.layers.0.self_attn.k_proj.lora_A.default.weight, model.decoder.layers.0.self_attn.k_proj.lora_B.default.weight, model.decoder.layers.0.self_attn.out_proj.lora_A.default.weight, model.decoder.layers.0.self_attn.out_proj.lora_B.default.weight, model.decoder.layers.0.encoder_attn.k_proj.lora_A.default.weight, model.decoder.layers.0.encoder_attn.k_proj.lora_B.default.weight, model.decoder.layers.0.encoder_attn.out_proj.lora_A.default.weight, model.decoder.layers.0.encoder_attn.out_proj.lora_B.default.weight, model.decoder.layers.0.fc1.lora_A.default.weight, model.decoder.layers.0.fc1.lora_B.default.weight, model.decoder.layers.0.fc2.lora_A.default.weight, model.decoder.layers.0.fc2.lora_B.default.weight, model.decoder.layers.1.self_attn.k_proj.lora_A.default.weight, model.decoder.layers.1.self_attn.k_proj.lora_B.default.weight, model.decoder.layers.1.self_attn.out_proj.lora_A.default.weight, model.decoder.layers.1.self_attn.out_proj.lora_B.default.weight, model.decoder.layers.1.encoder_attn.k_proj.lora_A.default.weight, model.decoder.layers.1.encoder_attn.k_proj.lora_B.default.weight, model.decoder.layers.1.encoder_attn.out_proj.lora_A.default.weight, model.decoder.layers.1.encoder_attn.out_proj.lora_B.default.weight, model.decoder.layers.1.fc1.lora_A.default.weight, model.decoder.layers.1.fc1.lora_B.default.weight, model.decoder.layers.1.fc2.lora_A.default.weight, model.decoder.layers.1.fc2.lora_B.default.weight, model.decoder.layers.2.self_attn.k_proj.lora_A.default.weight, model.decoder.layers.2.self_attn.k_proj.lora_B.default.weight, model.decoder.layers.2.self_attn.out_proj.lora_A.default.weight, model.decoder.layers.2.self_attn.out_proj.lora_B.default.weight, model.decoder.layers.2.encoder_attn.k_proj.lora_A.default.weight, model.decoder.layers.2.encoder_attn.k_proj.lora_B.default.weight, model.decoder.layers.2.encoder_attn.out_proj.lora_A.default.weight, model.decoder.layers.2.encoder_attn.out_proj.lora_B.default.weight, model.decoder.layers.2.fc1.lora_A.default.weight, model.decoder.layers.2.fc1.lora_B.default.weight, model.decoder.layers.2.fc2.lora_A.default.weight, model.decoder.layers.2.fc2.lora_B.default.weight, model.decoder.layers.3.self_attn.k_proj.lora_A.default.weight, model.decoder.layers.3.self_attn.k_proj.lora_B.default.weight, model.decoder.layers.3.self_attn.out_proj.lora_A.default.weight, model.decoder.layers.3.self_attn.out_proj.lora_B.default.weight, model.decoder.layers.3.encoder_attn.k_proj.lora_A.default.weight, model.decoder.layers.3.encoder_attn.k_proj.lora_B.default.weight, model.decoder.layers.3.encoder_attn.out_proj.lora_A.default.weight, model.decoder.layers.3.encoder_attn.out_proj.lora_B.default.weight, model.decoder.layers.3.fc1.lora_A.default.weight, model.decoder.layers.3.fc1.lora_B.default.weight, model.decoder.layers.3.fc2.lora_A.default.weight, model.decoder.layers.3.fc2.lora_B.default.weight, model.decoder.layers.4.self_attn.k_proj.lora_A.default.weight, model.decoder.layers.4.self_attn.k_proj.lora_B.default.weight, model.decoder.layers.4.self_attn.out_proj.lora_A.default.weight, model.decoder.layers.4.self_attn.out_proj.lora_B.default.weight, model.decoder.layers.4.encoder_attn.k_proj.lora_A.default.weight, model.decoder.layers.4.encoder_attn.k_proj.lora_B.default.weight, model.decoder.layers.4.encoder_attn.out_proj.lora_A.default.weight, model.decoder.layers.4.encoder_attn.out_proj.lora_B.default.weight, model.decoder.layers.4.fc1.lora_A.default.weight, model.decoder.layers.4.fc1.lora_B.default.weight, model.decoder.layers.4.fc2.lora_A.default.weight, model.decoder.layers.4.fc2.lora_B.default.weight, model.decoder.layers.5.self_attn.k_proj.lora_A.default.weight, model.decoder.layers.5.self_attn.k_proj.lora_B.default.weight, model.decoder.layers.5.self_attn.out_proj.lora_A.default.weight, model.decoder.layers.5.self_attn.out_proj.lora_B.default.weight, model.decoder.layers.5.encoder_attn.k_proj.lora_A.default.weight, model.decoder.layers.5.encoder_attn.k_proj.lora_B.default.weight, model.decoder.layers.5.encoder_attn.out_proj.lora_A.default.weight, model.decoder.layers.5.encoder_attn.out_proj.lora_B.default.weight, model.decoder.layers.5.fc1.lora_A.default.weight, model.decoder.layers.5.fc1.lora_B.default.weight, model.decoder.layers.5.fc2.lora_A.default.weight, model.decoder.layers.5.fc2.lora_B.default.weight, model.decoder.layers.6.self_attn.k_proj.lora_A.default.weight, model.decoder.layers.6.self_attn.k_proj.lora_B.default.weight, model.decoder.layers.6.self_attn.out_proj.lora_A.default.weight, model.decoder.layers.6.self_attn.out_proj.lora_B.default.weight, model.decoder.layers.6.encoder_attn.k_proj.lora_A.default.weight, model.decoder.layers.6.encoder_attn.k_proj.lora_B.default.weight, model.decoder.layers.6.encoder_attn.out_proj.lora_A.default.weight, model.decoder.layers.6.encoder_attn.out_proj.lora_B.default.weight, model.decoder.layers.6.fc1.lora_A.default.weight, model.decoder.layers.6.fc1.lora_B.default.weight, model.decoder.layers.6.fc2.lora_A.default.weight, model.decoder.layers.6.fc2.lora_B.default.weight, model.decoder.layers.7.self_attn.k_proj.lora_A.default.weight, model.decoder.layers.7.self_attn.k_proj.lora_B.default.weight, model.decoder.layers.7.self_attn.out_proj.lora_A.default.weight, model.decoder.layers.7.self_attn.out_proj.lora_B.default.weight, model.decoder.layers.7.encoder_attn.k_proj.lora_A.default.weight, model.decoder.layers.7.encoder_attn.k_proj.lora_B.default.weight, model.decoder.layers.7.encoder_attn.out_proj.lora_A.default.weight, model.decoder.layers.7.encoder_attn.out_proj.lora_B.default.weight, model.decoder.layers.7.fc1.lora_A.default.weight, model.decoder.layers.7.fc1.lora_B.default.weight, model.decoder.layers.7.fc2.lora_A.default.weight, model.decoder.layers.7.fc2.lora_B.default.weight, model.decoder.layers.8.self_attn.k_proj.lora_A.default.weight, model.decoder.layers.8.self_attn.k_proj.lora_B.default.weight, model.decoder.layers.8.self_attn.out_proj.lora_A.default.weight, model.decoder.layers.8.self_attn.out_proj.lora_B.default.weight, model.decoder.layers.8.encoder_attn.k_proj.lora_A.default.weight, model.decoder.layers.8.encoder_attn.k_proj.lora_B.default.weight, model.decoder.layers.8.encoder_attn.out_proj.lora_A.default.weight, model.decoder.layers.8.encoder_attn.out_proj.lora_B.default.weight, model.decoder.layers.8.fc1.lora_A.default.weight, model.decoder.layers.8.fc1.lora_B.default.weight, model.decoder.layers.8.fc2.lora_A.default.weight, model.decoder.layers.8.fc2.lora_B.default.weight, model.decoder.layers.9.self_attn.k_proj.lora_A.default.weight, model.decoder.layers.9.self_attn.k_proj.lora_B.default.weight, model.decoder.layers.9.self_attn.out_proj.lora_A.default.weight, model.decoder.layers.9.self_attn.out_proj.lora_B.default.weight, model.decoder.layers.9.encoder_attn.k_proj.lora_A.default.weight, model.decoder.layers.9.encoder_attn.k_proj.lora_B.default.weight, model.decoder.layers.9.encoder_attn.out_proj.lora_A.default.weight, model.decoder.layers.9.encoder_attn.out_proj.lora_B.default.weight, model.decoder.layers.9.fc1.lora_A.default.weight, model.decoder.layers.9.fc1.lora_B.default.weight, model.decoder.layers.9.fc2.lora_A.default.weight, model.decoder.layers.9.fc2.lora_B.default.weight, model.decoder.layers.10.self_attn.k_proj.lora_A.default.weight, model.decoder.layers.10.self_attn.k_proj.lora_B.default.weight, model.decoder.layers.10.self_attn.out_proj.lora_A.default.weight, model.decoder.layers.10.self_attn.out_proj.lora_B.default.weight, model.decoder.layers.10.encoder_attn.k_proj.lora_A.default.weight, model.decoder.layers.10.encoder_attn.k_proj.lora_B.default.weight, model.decoder.layers.10.encoder_attn.out_proj.lora_A.default.weight, model.decoder.layers.10.encoder_attn.out_proj.lora_B.default.weight, model.decoder.layers.10.fc1.lora_A.default.weight, model.decoder.layers.10.fc1.lora_B.default.weight, model.decoder.layers.10.fc2.lora_A.default.weight, model.decoder.layers.10.fc2.lora_B.default.weight, model.decoder.layers.11.self_attn.k_proj.lora_A.default.weight, model.decoder.layers.11.self_attn.k_proj.lora_B.default.weight, model.decoder.layers.11.self_attn.out_proj.lora_A.default.weight, model.decoder.layers.11.self_attn.out_proj.lora_B.default.weight, model.decoder.layers.11.encoder_attn.k_proj.lora_A.default.weight, model.decoder.layers.11.encoder_attn.k_proj.lora_B.default.weight, model.decoder.layers.11.encoder_attn.out_proj.lora_A.default.weight, model.decoder.layers.11.encoder_attn.out_proj.lora_B.default.weight, model.decoder.layers.11.fc1.lora_A.default.weight, model.decoder.layers.11.fc1.lora_B.default.weight, model.decoder.layers.11.fc2.lora_A.default.weight, model.decoder.layers.11.fc2.lora_B.default.weight. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model in FP16...\n",
      "Model loaded successfully on cuda!\n",
      "Loaded model appears to be a PEFT model already\n",
      "Target modules for LoRA: ['q_proj', 'k_proj', 'v_proj', 'out_proj', 'fc1', 'fc2']\n",
      "trainable params: 8,650,752 || all params: 414,941,184 || trainable%: 2.0848\n",
      "LoRA adapters applied successfully!\n",
      "Loading custom dataset...\n",
      "Dataset loaded successfully with 371 rows\n",
      "Dataset columns: ['Paper Id', 'Paper Title', 'Key Words', 'Abstract', 'Conclusion', 'Document', 'Paper Type', 'Summary', 'Topic', 'OCR', 'labels']\n",
      "\n",
      "Sample data:\n",
      "Document    Multi-document Summarization via Deep Learning...\n",
      "Summary     This article presents a systematic overview of...\n",
      "Name: 0, dtype: object\n",
      "Tokenizing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "107802a7bfb44cefbbbb02992d9519f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing dataset:   0%|          | 0/371 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset tokenized successfully!\n",
      "Dataset split into 296 training and 75 test examples\n",
      "Training arguments configured successfully!\n",
      "Starting further fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "<ipython-input-33-ed87c207d645>:305: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27' max='27' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27/27 02:04, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress: 100.00% complete (step 27/27)\n",
      "GPU 0 Memory: 7.18 GB\n",
      "GPU 1 Memory: 0.02 GB\n",
      "Further fine-tuning completed successfully!\n",
      "Saving final model...\n",
      "Model saved to ./bart_further_lora_final_compscholar\n",
      "LoRA configuration saved to ./bart_further_lora_final_compscholar\n",
      "Further fine-tuning process completed!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    TrainerCallback\n",
    ")\n",
    "from datasets import Dataset, DatasetDict\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Define the custom model path and other paths\n",
    "custom_model_path = \"/kaggle/input/bartft/transformers/default/1\"  # Your already fine-tuned model\n",
    "custom_dataset_path = \"/kaggle/input/compscholar/Brain Dead CompScholar Dataset.csv\"\n",
    "output_dir = \"./bart_further_lora_finetuned_compScholar\"\n",
    "final_model_path = \"./bart_further_lora_final_compscholar\"\n",
    "\n",
    "print(f\"Loading custom fine-tuned model from: {custom_model_path}\")\n",
    "\n",
    "# Initialize tokenizer with padding token\n",
    "tokenizer = AutoTokenizer.from_pretrained(custom_model_path)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load the custom fine-tuned model\n",
    "try:\n",
    "    print(\"Loading model in FP16...\")\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        custom_model_path,\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "    # Move model to GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    print(f\"Model loaded successfully on {device}!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model with FP16: {e}\")\n",
    "    print(\"Falling back to default precision...\")\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(custom_model_path)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "# Check if the model is already a PeftModel and handle accordingly\n",
    "if os.path.exists(os.path.join(custom_model_path, \"adapter_config.json\")):\n",
    "    print(\"Loaded model appears to be a PEFT model already\")\n",
    "    # For further fine-tuning of an existing PEFT model, we can either:\n",
    "    # 1. Continue with the same adapters\n",
    "    # 2. Merge adapters and add new ones\n",
    "    \n",
    "    # Option 1: Continue with same adapters (simpler)\n",
    "    # model is already loaded correctly as a PeftModel\n",
    "    \n",
    "    # Option 2: Merge adapters and create new ones (optional)\n",
    "    # Uncomment these lines if you want to merge the previous adapters\n",
    "    # print(\"Merging existing adapters into base model...\")\n",
    "    # if isinstance(model, PeftModel):\n",
    "    #     model = model.merge_and_unload()\n",
    "    #     print(\"Successfully merged adapters into base model\")\n",
    "else:\n",
    "    print(\"Loaded model is a standard model without PEFT adapters\")\n",
    "\n",
    "# Identify target modules for BART\n",
    "# BART typically uses these layer names for attention\n",
    "target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\", \"fc1\", \"fc2\"]\n",
    "print(\"Target modules for LoRA:\", target_modules)\n",
    "\n",
    "# Apply LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=target_modules,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\"\n",
    ")\n",
    "\n",
    "# Apply LoRA adapters to the model if it's not already a PeftModel\n",
    "if not isinstance(model, PeftModel):\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "model.print_trainable_parameters()\n",
    "print(\"LoRA adapters applied successfully!\")\n",
    "\n",
    "# Load the custom dataset with error handling\n",
    "print(\"Loading custom dataset...\")\n",
    "try:\n",
    "    df = pd.read_csv(custom_dataset_path)\n",
    "    print(f\"Dataset loaded successfully with {len(df)} rows\")\n",
    "    print(\"Dataset columns:\", df.columns.tolist())\n",
    "    \n",
    "    # Check for NaN values in important columns\n",
    "    if df['Document'].isna().sum() > 0 or df['Summary'].isna().sum() > 0:\n",
    "        print(f\"Warning: Found {df['Document'].isna().sum()} NaN values in 'Document' column\")\n",
    "        print(f\"Warning: Found {df['Summary'].isna().sum()} NaN values in 'Summary' column\")\n",
    "        # Drop rows with NaN values in Document or Summary\n",
    "        df = df.dropna(subset=['Document', 'Summary'])\n",
    "        print(f\"Dropped NaN values. Remaining rows: {len(df)}\")\n",
    "        \n",
    "    # Display sample data\n",
    "    print(\"\\nSample data:\")\n",
    "    print(df[['Document', 'Summary']].iloc[0])\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    raise\n",
    "\n",
    "# Convert the DataFrame to a Hugging Face Dataset object\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Define a simple ROUGE calculation function without external dependencies\n",
    "def calculate_rouge_scores(predictions, references):\n",
    "    \"\"\"Calculate simple approximation of ROUGE scores\"\"\"\n",
    "    scores = {\"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0}\n",
    "    \n",
    "    if len(predictions) == 0:\n",
    "        return scores\n",
    "    \n",
    "    total_f1_scores = {\"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0}\n",
    "    \n",
    "    for pred, ref in zip(predictions, references):\n",
    "        # Normalize text\n",
    "        pred = pred.lower().strip()\n",
    "        ref = ref.lower().strip()\n",
    "        \n",
    "        # Split into words for ROUGE-1\n",
    "        pred_words = pred.split()\n",
    "        ref_words = ref.split()\n",
    "        \n",
    "        # ROUGE-1: Unigram overlap\n",
    "        overlap = set(pred_words) & set(ref_words)\n",
    "        if len(pred_words) > 0 and len(ref_words) > 0:\n",
    "            precision = len(overlap) / len(pred_words)\n",
    "            recall = len(overlap) / len(ref_words)\n",
    "            if precision + recall > 0:\n",
    "                f1 = 2 * precision * recall / (precision + recall)\n",
    "                total_f1_scores[\"rouge1\"] += f1\n",
    "        \n",
    "        # ROUGE-2: Bigram overlap\n",
    "        pred_bigrams = [\" \".join(pred_words[i:i+2]) for i in range(len(pred_words)-1)]\n",
    "        ref_bigrams = [\" \".join(ref_words[i:i+2]) for i in range(len(ref_words)-1)]\n",
    "        overlap_bigrams = set(pred_bigrams) & set(ref_bigrams)\n",
    "        if len(pred_bigrams) > 0 and len(ref_bigrams) > 0:\n",
    "            precision = len(overlap_bigrams) / max(len(pred_bigrams), 1)\n",
    "            recall = len(overlap_bigrams) / max(len(ref_bigrams), 1)\n",
    "            if precision + recall > 0:\n",
    "                f1 = 2 * precision * recall / (precision + recall)\n",
    "                total_f1_scores[\"rouge2\"] += f1\n",
    "        \n",
    "        # ROUGE-L: Longest Common Subsequence (simple approximation)\n",
    "        # Using the longest common substring as a simple approximation\n",
    "        longest_common = \"\"\n",
    "        for i in range(len(pred_words)):\n",
    "            for j in range(i+1, len(pred_words)+1):\n",
    "                substring = \" \".join(pred_words[i:j])\n",
    "                if substring in \" \".join(ref_words) and len(substring) > len(longest_common):\n",
    "                    longest_common = substring\n",
    "        \n",
    "        if len(pred_words) > 0 and len(ref_words) > 0:\n",
    "            lcs_len = len(longest_common.split()) if longest_common else 0\n",
    "            precision = lcs_len / len(pred_words)\n",
    "            recall = lcs_len / len(ref_words)\n",
    "            if precision + recall > 0:\n",
    "                f1 = 2 * precision * recall / (precision + recall)\n",
    "                total_f1_scores[\"rougeL\"] += f1\n",
    "    \n",
    "    # Calculate average F1 scores\n",
    "    for key in total_f1_scores:\n",
    "        scores[key] = total_f1_scores[key] / len(predictions)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# Define the tokenization function with proper padding\n",
    "def tokenize_function(examples):\n",
    "    # Handle potential empty inputs\n",
    "    inputs = [text if text and isinstance(text, str) else \"\" for text in examples['Document']]\n",
    "    targets = [text if text and isinstance(text, str) else \"\" for text in examples['Summary']]\n",
    "    \n",
    "    # Tokenize inputs with padding and truncation\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, \n",
    "        max_length=1024, \n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    \n",
    "    # Tokenize targets with padding and truncation\n",
    "    labels = tokenizer(\n",
    "        targets, \n",
    "        max_length=150, \n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    # Replace padding token id with -100 in labels so it's ignored in loss calculation\n",
    "    model_inputs[\"labels\"] = [\n",
    "        [-100 if token == tokenizer.pad_token_id else token for token in label] \n",
    "        for label in model_inputs[\"labels\"]\n",
    "    ]\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "# Tokenize the dataset and remove the original columns\n",
    "try:\n",
    "    print(\"Tokenizing dataset...\")\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function, \n",
    "        batched=True, \n",
    "        batch_size=16,  # Process in smaller batches to avoid memory issues\n",
    "        remove_columns=dataset.column_names,\n",
    "        desc=\"Tokenizing dataset\"\n",
    "    )\n",
    "    print(\"Dataset tokenized successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during tokenization: {e}\")\n",
    "    raise\n",
    "\n",
    "# Split the tokenized dataset into train (80%) and test (20%) sets\n",
    "try:\n",
    "    train_test_split = tokenized_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "    tokenized_dataset = DatasetDict({\n",
    "        \"train\": train_test_split[\"train\"],\n",
    "        \"test\": train_test_split[\"test\"]\n",
    "    })\n",
    "    print(f\"Dataset split into {len(tokenized_dataset['train'])} training and {len(tokenized_dataset['test'])} test examples\")\n",
    "except Exception as e:\n",
    "    print(f\"Error splitting dataset: {e}\")\n",
    "    raise\n",
    "\n",
    "# Function to compute metrics for evaluation\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    \n",
    "    # Decode predicted sequences\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    \n",
    "    # Replace -100 with pad token id and then decode\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Calculate ROUGE scores\n",
    "    result = calculate_rouge_scores(decoded_preds, decoded_labels)\n",
    "    \n",
    "    # Convert scores to percentages\n",
    "    result = {key: round(value * 100, 4) for key, value in result.items()}\n",
    "    return result\n",
    "\n",
    "# Create a data collator for dynamic padding\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=-100,\n",
    "    pad_to_multiple_of=8  # For better efficiency on tensors\n",
    ")\n",
    "\n",
    "# Define a progress callback to monitor training\n",
    "class ProgressPercentageCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if state.max_steps and state.global_step:\n",
    "            percent_complete = (state.global_step / state.max_steps) * 100\n",
    "            print(f\"Training progress: {percent_complete:.2f}% complete (step {state.global_step}/{state.max_steps})\")\n",
    "            \n",
    "            # Monitor GPU usage if available\n",
    "            if torch.cuda.is_available():\n",
    "                try:\n",
    "                    for i in range(torch.cuda.device_count()):\n",
    "                        gpu_mem = torch.cuda.memory_allocated(i) / (1024**3)  # Convert to GB\n",
    "                        print(f\"GPU {i} Memory: {gpu_mem:.2f} GB\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error monitoring GPU: {e}\")\n",
    "        return control\n",
    "\n",
    "# Set training parameters with memory management optimizations\n",
    "# Reduce learning rate for further fine-tuning of an already fine-tuned model\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,  # Lower learning rate for further fine-tuning\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_steps=50,\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    metric_for_best_model=\"rouge1\",\n",
    "    load_best_model_at_end=True,\n",
    "    greater_is_better=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured successfully!\")\n",
    "\n",
    "# Define the Trainer with evaluation metrics\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Add the progress callback to monitor training progress\n",
    "trainer.add_callback(ProgressPercentageCallback)\n",
    "\n",
    "# Implement basic error handling for training\n",
    "try:\n",
    "    print(\"Starting further fine-tuning...\")\n",
    "    trainer.train()\n",
    "    print(\"Further fine-tuning completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during training: {e}\")\n",
    "    # If OOM error, suggest reducing batch size or model size\n",
    "    if \"CUDA out of memory\" in str(e):\n",
    "        print(\"Suggestion: Try reducing batch size, using gradient accumulation, or reducing model size\")\n",
    "    raise\n",
    "\n",
    "# Save the final model after training\n",
    "try:\n",
    "    print(\"Saving final model...\")\n",
    "    trainer.save_model(final_model_path)\n",
    "    print(f\"Model saved to {final_model_path}\")\n",
    "    \n",
    "    # Save LoRA configuration separately for future reference\n",
    "    lora_config_path = os.path.join(final_model_path, \"lora_config.json\")\n",
    "    lora_config.save_pretrained(final_model_path)\n",
    "    print(f\"LoRA configuration saved to {final_model_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving model: {e}\")\n",
    "    raise\n",
    "\n",
    "print(\"Further fine-tuning process completed!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 612177,
     "sourceId": 11044613,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6939514,
     "sourceId": 11127283,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 275180,
     "modelInstanceId": 253750,
     "sourceId": 296429,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
